## 0101卢菁-ChatGLM-ChatGPT

今天我将阐述一个主题：ChatGLM。选择这一主题的原因在于我个人深信，在国内能与国际知名的 OpenAI ChatGPT 或 Google 这样的企业在人工智能领域一决高下的，仅有少数几家公司。在我看来，首当其冲的就是百度。百度在广告方面的效果固然显著，但更值得关注的是其扎实的技术实力。另一家值得一提的是专注于 XGM 技术的清华智普。智普在业界有着引人瞩目的特色：它所推出的产品几乎在各个方面都与 OpenAI 的产品形成了直接对标。这意味着，不管 OpenAI 推出了何种新技术，智普几乎都能够推出具有相似功能的产品。因此，智普或与之相关的 GLM 这类模型，在我看来，潜力巨大，并且非常值得我们持续关注。无论是研究大型模型还是其他相关技术，这都是非常值得我们投入时间去深入了解和学习的领域。

接着，我们来探讨当前主流的大型模型。它们主要分为三个流派：一是以 BERT 为代表的编码器（encoder）模型，二是以 GPT 系列为代表的解码器（decoder）模型，以及三是结合了编码器和解码器的模型。

BERT 以及与之类似的模型采取了编码器架构。我们看到，截至 2021 年，通过这一技术路线获得的成果已经不再增长，原因是这条路线可能已经接近尽头，或者至少在短期内难以作出突破。这是因为适用场景有限，技术改进空间受到限制。

而另一线 —— 以 GPT-4 为代表的仅包含解码器的模型，却显示出极强的活力。在这一路线上，不仅有 GPT-4，还包括了 XGPT 等多个模型。

今天，我希望重点探讨的是采用了编码器-解码器结构的 GLM 模型。我们将聚焦于它的构造和性能，以及与其他模型的比较分析。实际上，这不仅仅是两家公司产品的比较，更是两种不同技术线的较量。虽然二者在技术架构上有所差别，这种差异可能没有大家想象的那样巨大，但事实上，差异确实存在。以 OpenAI 为例，它发布了 GP3，也被称为「达芬奇」。之后则是迭代版本达芬奇 1、达芬奇 2，以及最新的 ChatGPT，在去年的十一月份又带来了新的视角和能力。

在这场技术的长跑中，每个参与者都不断地推动技术的极限，追求卓越的表现。各家公司的发展路径虽不尽相同，但他们之间的激烈竞争，无疑推动了整个人工智能领域的进步。因此，无论是行业内的专业人士还是其他领域的观察者，我们都应当持续关注这些前沿技术的发展和它们之间的竞争。这不仅能帮助我们把握未来技术的脉络，也可能为我们带来前所未有的机遇。

自去年以来，接近一年的时间，我们见证了 ChatGPT 的问世。今天，我想与大家分享一下智能生成模型（即智普）的发展时间线。智普团队从 2018 年开始研发自己的大型模型，虽然与 GPT-1 相比晚了两年，但从那时起，它便开始快速发展。不少人可能好奇，这家公司为何能在技术起步上有所迟缓，却能迅速成长。事实上，智普公司最初是在 2015 年或 2016 年成立（具体年份我记不太清楚），当时他们专注于制作智谱图。是的，他们的起点是知识图谱，正因为如此，公司积累了大量的数据。这是因为构建知识图谱需要爬取大批数据，因此，他们手头的数据量非常可观。有了这样的数据作为支撑，公司便迎来了飞速的成长。

去年 8 月，智普实现了令人印象深刻的突破，他们发布了参数达 130B 的应用，并且推出了一个专为 VS Code 设计的插件，这大大方便了程序员编写代码。事实上，这些成就的背后，是来自清华大学知识工程实验室的团队成员们的辛勤工作。

接着，我们注意到智普推出了 GLM-6B 模型。我想知道，有多少人已经体验过这个模型。不久前，他们刚刚发布了 3.0 版本。我亲自尝试后发现，这个新版本确实进行了一些改进，使用起来十分顺畅。这标志着 GLM 团队在整个大模型领域的一次显著进步。

今天，我们的主要议题是对比不同的大型模型。我们将具体分析 Decoder only 模型和 Encoder-Decoder 模型之间的区别。其中的一个模型可以进行量化，也引入了快速 Transformer 技术，加速了处理速度。关于横杠标记的部分，并不是说这个模型不能量化，而是因为 OpenAI 公司逐渐闭合其资源，变得不那么透明。他们很少发布论文，也许多技术不再开源，所以我们拿不到更多的技术细节。

值得一提的是，清华大学开发的模型是完全开源的，对实际工作来说支持帮助很大。与此同时，GLM 模型有一个显著优点，那就是它支持国产 GPU。这对我们来说意味着更大的便捷性和灵活性。

在目前的情形下，国有企业普遍倡导使用国产 GPU。实际上，尽管有人可能更倾向于使用进口产品，但如 A100 型号的显卡已经较难购买。我们现在能够买到的显卡型号大多是 A10、A6000 或者 V100。甚至抢手的 4090 型号也限制销售，这种情况有些让人无法接受。考虑到这一点，支持国产 GPU 无疑是一大亮点，并且这将成为未来的重要发展方向。

在训练方法方面，经典的训练方法，例如大规模 GPT，包括了四个阶段：预训练、微调、奖励模型与强化学习。GPT 能够顺畅地完成这四个步骤。相比之下，GLM 虽然宣称完成了所有步骤，但某些后续环节存在疑问。经过分析发现，它可能并没有完全达到预期。而且，后两个阶段 —— 奖励模型与强化学习 —— 目前存在质疑，有观点认为这些部分可能并非必要，只是过度充足而已，并且执行这两步骤需要耗费大量的人力成本。如果将其视为选项，那么我们或许可以认为这是一个变数。

接下来，我们再来看一下 GLM 的产品线。据我之前和大家的介绍，GLM 的产品线与 GPT 在很多方面是全面对标的。当前，我们将针对这些产品的特点，向大家详细解读 GLM 相关的内容，并进行整体概述。

关于我们的课程，它更注重理论方面，而不是只教授实际操作。市面上许多课程避开了理论教学，只是简单地指导如何使用特定的工具和技术。我认为，对于具备大学基础教育，英语水平达到四级的人来说，这些课程的价值并不大，因为他们可以通过自己的努力去理解和掌握这些知识。比起那些课程，我们的课程更注重深度和理论，可能自学起来比较困难，相关资料也不容易查找。但请放心，我们会继续探索这些难点。

我们继续关注 GLM 的发展。从 3 月 14 日 GLM-1.0 发布开始，接着在 6 月发布了 2.0 版本，然后到了 10 月 27 日 —— 也就是前两周 —— 发布了更新。在这两次发布中，GLM 推出了两个新模型。一个是多模态模型，另一个是专注于长篇文本与对话的智能体。其中，配备 6 位元和 17 位元的版本也随之推出。同时，引入的「agent」概念是 GLM 的一个亮点。我们首先需了解，从 1.0 到 2.0 的转变带来了哪些进化。首先，增强了对长序列支持的能力，这是一项重要的技术升级。

首先我们讨论了持有 8K 序列的主题，并且提到了快速的 transformer。这种快速的 Transformer 能够有效提升处理速度。我们通常认为，在大型模型领域中，Transformer 相对平静，它是一个较为平稳的技术点。因此，通过对其进行一定优化，我们便可以在一定程度上加快其运算速度。

我们还观察了其不同版本的模型，它们分别对应着不同的计算币数，包括 6B、12B、32B、66B 至 130B 不等。例如，一家名为 360 的公司，推出了 TALK 130B 模型。大家可能对 TALK 130B 这个名称比较熟悉，这是中国在大型模型领域内的一项杰作。

中国的大型模型可以被大致分为三大派系。首先是清华的 GLM（大模型）团队，其次是百度，第三则是套壳公司。在这些套壳公司中，llama 的套壳占据了极大部分。你们可能会提出一个问题，为什么国内的开发者较少使用这样的套壳方法。原因是很简单的，主要是因为他们担心法律诉讼的风险。以 llama 为例，简单来说，就是他们赌 Facebook 不会跨越国界来起诉他们。而 GLM3 模型，与 GLM2 相比，其测试分数在各个榜单上均显著提高。

至于阿里巴巴，我们可以认为它也采用了套壳的方法，虽然这只是我的个人推测。我记得我在 B 站上谈到过套壳，而我并未明说是谁。结果发现很多人猜测是训飞，并且有人甚至联系训飞的法务部门说要起诉我。这件事我觉得挺有意思的。我曾经说过训飞的法务部门给我发送了律师函，但最终，我等了两周也没有收到。

在选择大型模型时，我希望同学们不要过于迷信某些榜单。并不是所有的榜单都具备绝对的权威，有时候这些榜单与实际应用场景存在明显的差异。有时，一些机构会针对榜单的标准对自己的模型进行特定优化，这并不意味着该模型在所有情况下都同样优秀。以某川为例，他们就是采用了这样的优化策略。

对于这些大型模型的一些特点，我们可以概览如下：这些模型的优势之一是它们基本都带有免费商业授权。需要注意的是，目前 3B 和 1.5B 这两个版本尚未开源。我们也无法获得这两个版本，这主要是因为它们用途较为特殊。它们体积较小，主要用于在智能手机端进行部署。华为的模型则被称为盘古，具有其独特的应用场景和功能。

通过这次整理，我们对大型模型领域的纷繁格局有了清晰的了解，同时也提醒大家在实践中要注重模型的实际应用价值，而非仅仅追求榜单上的排名。

在讨论到盘古大模型的时候，我们不得不承认它的开源程度并不高，普通用户难以接触和使用。这是因为盘古更多地服务于内部的各个产品线，而其生态系统相对封闭。看似小巧的盘古，让人不易与之深入接触。我的亲身体验也是如此，我未能直接使用它，故不宜妄加评价。

为了技术的分享和交流，接下来讲述的是 1.5B 和 3B 模型。这两种模型在性能上接近 6B 的水准，意味着它们可以用更少的参数量，即一半甚至四分之一，达到相近的效果。特别值得一提的是，这些模型支持国产芯片，并可以应用于笔记本、手机、汽车等多种设备上，此外，还能在 CPU 上无损效果地进行推理。这显示出它们在计算效率上的优势。

在当前的发展态势中，如果依然只是单纯追求模型效果的提升似乎已经到了瓶颈期，不易有显著的进展。因此，现在越来越多的研究开始专注于模型在各种现实场景中的实际应用，以及垂直领域的具体效果。GLM3 在这方面做出了贡献，并进行了相关领域的研究。

说到 minimax，我必须坦白，我没有使用过这个模型，因此不便作出评价。但我可以分享我亲自参与过的工作。

继续讨论，GLM 模型因其庞大的训练数据、充分的训练步骤和合理的训练策略而显得非常强大。这三个因素，经常被人提及，似乎已经成为一种通用的表述方式。从携带码和知识角度来看，GLM 模型也有显著的提升。接下来，讲述的内容会包括 agent 功能，这将逐一展示。

另外，GLM 模型的另一个优势在于其全面的开源策略。它开放的内容更多，且基本上允许免费商业使用。这对于那些从事学术研究或者个人兴趣开发的人来说是十分有利的。我已下载了 3.0 版本，并会在之后的演示中向大家呈现。

3.0 版本的特色在于它不再是一个纯粹的语言模型。它通过调用各种工具，能够处理包括输入提示词在内的多种操作。然后，向模型提出问题，并得到模型的帮助和答复。

在探讨现代技术如何辅助我们获取信息的领域中，存在一些高效的工具，这些工具不仅能优化我们的信息检索过程，还能提高我们对信息的理解与应用能力。我有幸了解并使用过一些这样的工具，在此与大家分享。

首先，股票价格查询工具是我们常见的应用。它可以快速准确地提供股票市场的即时数据。例如，当我们想要知道特定股票在特定日期的收盘价格时，如 11 月 6 日某股票的收盘价是 23.16 元，这个工具能够迅速给出答案。

此外，文本转语音工具亦是一项重要的技术成果，尤其是应用了大模型的解决方案的这类工具。它不仅能将文本信息转化为语音，而且能理解并回应用户的查询。也就是说，在你向大模型提出需求后，例如查询股票价格，它能自动识别你的需求并调用外部的股票接口来返回查询结果。而这一过程并非单一的语言输出，它真正涵盖了与外部接口的实际通信和数据检索步骤。

来看一个场景应用，当我们对大模型说：「帮我查今天北京的天气怎么样」，并向它提示我们可用的工具 —— 一个能获取实时天气信息的程序时，大模型首先需要了解工具的特征。这就需要我们明确告诉模型，我们所用的工具不仅能获得当前位置的天气，还能提供气温单位，是摄氏度还是华氏度。在此基础上，模型运行后便能给出答案。

接下来，有必要解释一下「Agent」的概念。简而言之，「Agent」是在大模型外围附加了一系列工具，使得大模型与这些工具联动，共同形成了一个能进行更多复杂交互的系统。「大模型 + 工具 = Agent」—— 这个公式既简单又直观。当提及 Agent 时，可以想象大模型作为大脑，而工具则是它的手脚，共同作用以完成更复杂、具体的任务。

此外，还有一次我在 B 站上进行的有关 Agent 的直播，当时的内容覆盖了 Agent 的详细解释与应用。如果你有兴趣，可以通过加我微信的方式来获取这段直播的视频链接，以便更深入了解。

最后，我们不得不提的是 GLM 这套模型及其强大的功能 —— 借助搜索引擎来提供相关答案，并给出引用来源，使得信息查询不仅有据可查，还能确保信息的准确性和可靠性。这意味着即使我们询问大模型某个如「今天的新闻有哪些」这样的问题，它不仅能理解我们的查询意图，还能通过相关的数据源给出实质性答案。

总之，随着科技的进步，这些工具不断地提升我们与信息交互的效率，让我们迅速获取、理解以及应用知识。这些技术正以前所未有的速度改变我们获取和处理信息的方式。

在这次讲座中，我们探讨了一个实际案例，即某人在训练完自己的模型之后，对特定日期 11 月 16 日的具体事件一无所知。我们来看看，模型已经上线运行了一段时间，那么当问及它 11 月 16 号发生的事件，它将如何作答。这个过程其实涉及三个步骤，我会重点跟大家分享每一步的含义。

首先，第一步相对简单。当我们向模型提问时，它会首先调用搜索引擎，这里以谷歌搜索为例，去获得相关的信息。得到的结果是若干个网页。由于网页内容较长，模型进一步将这些内容切分成几个片段。

接着，在第二步中，模型运用一个称为「contraver」的模块计算问题内容与这些文本片段之间的相似度。它会对所有段落进行打分，并选出得分较高的，这些段落作为引用或参考内容。这里，如果搜索引擎足够精准强大，这一步骤即使省略也不会有太大问题。

然后，在第三步，即提示学习阶段，模型不仅简单地提取这些参考信息，还会在多个参考之间做比较和提炼。模型会将这些信息当作一个案例进行分析，反复学习，并结合实际问题进行模拟。比方说，如果我给出参考文献一和参考文献二，同时定义我的问题和预期答案，模型会在此基础上进行引用和整合，向大模型呈现需要解决的问题。此举其实是在训练模型，让它学会如何找到真实世界问题的准确答案。

最后，模型将所有这些参考资料整理、汇总，并根据模型对这些资料的处理和解析，给出一个最终答案。这个汇总步骤是必要的，因为面对一个问题，可能存在多角度的新闻报道或信息。模型需要将这些信息综合起来，以提供全面而准确的回答。

以上，就是整个过程的详细说明。我们通过这个案例，可以看到智能模型如何通过搜索、分析和学习，处理和回答现实世界中的问题。这一系列的步骤不仅展示了人工智能处理知识和信息的能力，也体现了我们如何能够让技术更好地服务于实际需求。

在讲述现代计算模型的构建与优化过程中，我认为各位应该注意到其复杂性与系统性。首先，我想强调的是，对计算模型的建构，既可以从多角度进行细节描述，也可以从一个整体角度来概括。在收集并整合了丰富的数据信息之后，我们得到的可能是一个非常大型且功能强大的模型。然而，这个模型未必能完美符合我们人类的需求和偏好。那么，面对这种情形，我们应当如何处理呢？

接下来，我会解释一个统计合理偏好的方法 —— 评分体系。简言之，这个评分模型首先会通过收集高质量的用户反馈来定义有效数据。例如，任何一个获得三次或以上点赞的回答，都将被认定为一个有效的反馈答案。借助这些筛选出来的、高度有效的答案，我们能够更好地理解用户喜好。再如，假设我们有三个不同的答案，分别是 A1、A2 和 A3。每个答案都会根据其获得的点赞数进行排名，从而形成一个相对的优先级顺序，比如 A1 大于 A2，A2 大于 A3，以及 A1 大于 A3。随后，我们利用这个打分模型来指导大型模型的学习，在模型中对各个答案打分，从而使其分数反映出我们人类的偏好。然而，这种打分操作可能会产生巨大差异。

也就是说，我们的目标是让大型模型在对 A1 的打分上尽可能高，而对于 A2 则尽可能低，这样模仿出人类评价的不同立场。这里的要点是，这种打分的依据来源于实际的人类标注。

很好，让我们把目光转向这个流程的总结。尽管整个过程看起来可能有些复杂，但其实其核心逻辑还是相当直接的。首先，它经历了网络搜索答案的过程，并通过筛选精炼了信息，这一步骤主要是提高搜寻答案的关联度。其次，运用大型模型将搜索结果整合汇总并进行输出。此处可能会有多个复合的结果。第三步则是运用评分模型计算每个结果的得分，并最终返回得分最高的答案。这样一来，我们的模型就能尽可能贴合人类的认知模式。

作为讲解的延伸，这个模型十分类似与一个我们所熟知的概念 —— Lanchain，但其实施要更为复杂。综述了整个 ChatGLM 的策略，我们可以看到其基于一个高效的思路引擎，从而能够补充和丰富知识。此外，当谈到 ChatGLM 与 ChatGPT 的对比时，我们了解到一个仅有 10B 参数的模型便能够达到另一个有 175B 参数模型的效果，这无疑展示了它的强大能力。虽然与人类的水平相比，还存在一定的差距，但是仅凭 10B 的参数就能接近或者超越 175B 参数的表现，确实令人印象深刻。

接下来，让我来给大家介绍第二个模型，它名为「核心视觉语言模型」（Core VLM）。这个模型主要对标的是 GPT-4。举个例子来说，当你向模型提示时，你可以这样描述：「下面这个菜是什么？」它会迅速响应：「这是麻婆豆腐。」顺带一提，麻婆豆腐是非常受欢迎的四川菜之一。此外，这个模型能够对图像进行深入理解。

该系统也推出了一个称为 COGVRM 的能力集，它以紫色边框表现出其能力的广泛性。这个集合可谓功不可没，可以形容为「八边形战士」，拥有多达 13 种强大的能力。例如，在一篇论文中，为了比较模型的性能，研究人员给出了同一张图片，并询问：「这张图上有多少棵树？有多少座房子？」然后 Core VLM 回答：「有三座。」而如果向它提问，它可能会回答：「有四座。」这是因为它注意到图片中的一角，实际上背后藏着一座房子。尽管 GPT-4 没有蒙对这一点，但这并不是重点。我的个人观点是，这个例子不需要太过纠结。毕竟，那个角可能并不是房子，也有可能是其他物体，比如人的帽子等等。因为这是一幅卡通图，细节并不是完全清晰的。尽管如此，这表明模型对图像的理解能力可能超过了其他同类产品。

不仅如此，Core VLM 还能解释为什么它会这样判断。如果只露出一角，它仍能判定出图片中有四座房子。此外，这个模型还能进行一些更为复杂的任务。比如，如果你给它一张图，并要求它描述场景，它能够准确地告诉你这是什么样的一个场景。基本上，它就像是在进行「看图说话」的活动，这项能力是非常强大的。

甚至，它能通过观察图像来完成作业任务。例如，如果你问它：「两块苹果加上一块苹果等于多少？」然后给它看一张图片，它能告诉你答案是三。对于养育过小孩的人来说，你会知道教小孩进行简单计算时，通常会用到这类有趣的图片。

Core VLM 的能力并非止步于此。它也能对图像进行一些数字化处理，并告诉你衣服的坐标位置。这相当于说，它具备目标检测的功能，可以在所需区域准确标注目标。总的来说，这个多模态的能力展现了它的强大潜能。

我们探讨了具有多模态能力的一种机制。简而言之，多模态能力是一种非常强大的能力。通过观察该机制的效果，我们可以获得对其原理的初步了解。虽然这个原理并不像我们想象中那样复杂，但也不是毫无难度。

以图文匹配为例，我们可以发现一张图片与其描述成对出现。每一张图片对应着它的描述，形成了图文对。在这个过程中，文本这一侧的每个标记（token）可以被转换为一个向量。这些文本向量序列，通过 VIT encoder —— 一种使用 Transformer 模型处理图像的专门模型来将图像也转换成向量序列。关于 VIT 模型的详细信息，我们之前在 B 站上有所讲解，如果大家感兴趣，可以联系助理老师通过微信获取视频链接。

在得到图像的向量序列后，我们通常会使用一个叫做 LMP 的处理步骤来实现向量序列的维度转换，从而使其与文本的向量序列保持一致。然后，将这两个向量序列拼接在一起，形成一个非常长的向量序列。

基于这个长序列，我们采用的处理方法非常巧妙。类似于 GPT，它通过输入图像来预测文本。例如，输入一张图片后，系统将预测出 A，随后连同图片和 A 一起用来预测下一个词 —— 如 Fox，然后再连同图片、A 和 Fox 一起来预测下一个字母或词。与 GPT 模型一样，这是一个逐词预测的过程。

总而言之，输入包括文本序列和图像序列，在将文本与图像的序列串联起来之后，这些序列分别作为 Q、K、V 参与到多头注意力（multi-head attention）机制中。这个过程中，注意力权重实际上是在图像信息和文本信息之间进行加权融合。因此，综合后得到的向量序列既包含了图像信息，也包含了文本信息，实现了两者的有效结合。

我们主要讨论了 Transformer 架构在处理多模态任务中的应用，尤其是在图像和文本信息融合方面的创新。首先，让我们来回顾基础的 Transformer 模型，它基于自注意力（self-attention）机制构建，通过全联接前馈网络（Feedforward Neural Networks, FFN）进行信息处理。Transformer 模型优秀的并行计算能力使其成为近年来自然语言处理领域的一个突破。

我们的研究中，通过对独立的文本和图像内容采用分开的 Transformer 模型进行处理，每个模型内部都包含了若干个自注意力层和 FFN 层。在此基础上，两者的输出通过特定的方式进行了融合。这种融合策略实际上是借鉴了 Transformer Decoder 的架构，并应用了一种技巧来合并不同源头的信息，即图像信息与文本信息的结合。

我们所讲的合并策略非常直观。简单来说，它通过将图像信息和文本信息的尾部进行串联，实现了信息的整合和预测。例如，当输入一张图片时，模型就会预测与之相应的文本描述。在准备训练样本时，我们采用了端到端的方法，即将一整串文字逐个进行预测。这样，输入模型的不仅是图像，模型还会逐字符地输出预测结果。

除了分析 Transformer 在图生文（从图像生成文本）任务中的应用，本次讲座还探讨了文生图（从文本生成图像）的模型。这类模型在艺术创作等领域应用广泛。例如，OpenAI 和智谱分别推出了各自的模型，其中包括 OpenAI 出品的类似 CoreWave 的模型。

我们还介绍了 OpenAI 的模型背后的原理。举例来说，当输入一段描述「一只可爱的小猫」的文本时，模型会将此文本转换成一系列的 token，进而变为向量序列。这与前面提到的 clip 模型有所不同。clip 模型是直接将文本或图像转换成相应的序列，而在我们讨论的这个模型中，并不是这样的处理方式。这里的模型具有特点：首先对图像进行编码处理，将图像离散化为 token。例如，它可以将图像分成 4x4 个区块，每个区块与一个 token 相对应。

首先，我们将图像分割为多个标记（token），并为每个标记寻找相应的向量，将其组合为一个系列的向量。接着，我们利用这些向量序列进行预测。例如，输入一张可爱小猫的头像，我们需要单独预测这幅图像中各个部分的标记。这个预测过程借鉴了 JPG 原理对图像标记进行推断。预测完毕之后，整个图像的所有部分都完成了预测，我们便可以通过解码器将这些图像标记重新组合，恢复出原始图像；这一步骤是通过 decode 的方式实现的。

这种方法选择不使用 CLIP 的原因在于，虽然 CLIP 能够产生高质量的向量序列，但是它并不适合逆向生成图像。而采用标记化的方法，我们可以较为容易地逆向重构出图像。这也是 OpenAI 采用此方案的理由。

智谱的方案其实与 OpenAI 的方案大致相似。具体来说，在处理一张图像时，我们首先也需要进行标记化，这里使用的是 BIT 模型。进行标记化之后，我们开始预测图像序列。不过，智谱的方法有所不同，它允许预测过程中能够「见到」某些先前的元素。例如，在预测 S5 标记时，它能够引入之前标记的信息，这使得预测的视角与前述方法不同。在这里，视线是完全线性的，我们只能参考前面的标记来进行预测。

至于 BID 模型，它需要离线训练。假设我们有一个图像，总共被编码为 812 个部分。我们用一个编码模型（例如，4x4 的区块）来识别图像中的每一块，并将这些区块与特定的编码对应起来。通过编码模型对图像进行编码之后，我们的目标是能够将这些编码恢复成原始图像。因此，我们会计算编码和原图之间的差异，即 loss，并在编码然后解码的过程中最小化这个差异。当编码器训练完毕后，我们可以用它来对图像进行编码。然后，我们希望能够使用解码器从这些编码中恢复图像。这就是 DIT 模型所要完成的任务。它所需要的编码器和解码器都是进行了离线训练的。

经过精心训练的模型均是通过离线训练达到其高效能的。在当前的应用中，模型只是执行已被训练的功能而已。关于「文生图」（text-to-image）和「图生文」（image-to-text）的转换，我们已经做了大体的讲解。接下来，另一个引人注目的功能是编程代码的生成，这是一个强烈的刚性需求。这方面的模型确实能写出长达 8192 个字符的代码序列，并且在各大主流编程语言中都有显著的性能提升。

让我们从编程语言的使用比例来细看。Python 大约占比 26%，C++ 则为 28%。目前，编写 C 和 C++ 的人可能比较少，而 Python 和 Java 的使用者则相对较多。模型的工作机制其实非常简单：比如，在 Python 中，你写了一个如「for i in range (……)」的循环语句，模型就会将这句话视为一个语言序列，进而直接预测下一个可能出现的 token。

这一机制其实是基于 GPT 架构的。换句话说，模型完全采用了 GPT 的结构。但在预测时，模型进行了一个创新 —— 在某一位置添加了一个「捷径」，将当前位置与历史信息 Dn 连接起来，预测下一位置的 N+1 的 token。此处我之前的表述有所纰漏：正确的是，模型在某一点引入一个变量，通过捷径连接，以预测随后的词。

这个新引入的自由变量是指，在一个序列中，比如 GPT 的序列结构中，token 1 预测 token 2，token 2 预测 token 3 以此类推，而在模型 code1 的结构中，每一个位置都有特定的编码来参与预测。在预测 token 2 的时候，一个特定的编码被放置在那个位置，然后用来预测 token 2。之后，位置编码会移动，为预测下一个 token 而放置。这样的结构意味着每个位置都有一个位置编码，这个编码帮助模型准确地进行预测。

当然，这涉及的是编程模型构建中非常精细的细节。简而言之，这类模型基本上采用的是 GPT 的架构，你可以把它看作是 GPT 的一个变种。这种模型还可以进行代码间的翻译，例如，你给它一个 Java 代码，模型就能够直接生成对应的其他语言版本。

总之，我们讨论了一个高效的模型在编程领域的应用，不仅仅覆盖了代码生成，在多种编程语言间的翻译上也展现了模型的强大能力。这个模型依据丰富的编码和创新的结构，提高了代码预测的准确性，极大地拓展了编程语言处理模型的应用范围。

在本次讲座中，我们讨论了如何将一种语言翻译成 Python 代码，这个过程实际上属于机器翻译。这与 GPT（生成预训练变换器）的工作过程类似，在这里，我们将一个输入转换成一种不同的语言代码。今晚，我们对各种模型做了一个概览性的介绍，主要聚焦在 GLM（通用语言模型）系列，概述了它们的功能和基本原理。由于详细解析每个模型需要大量时间，本次我们仅提供了粗略的轮廓。在后续的课程中，我们将逐一细致地探讨每个模型的详细信息。

我们讨论了 GBT（梯度提升树）的一些特别之处，指出 GBT 并非开源，同时存在安全问题。在这方面，我们可以比较国产技术与外来技术，如比亚迪与特斯拉。国产技术可能并不逊色于外来技术，在开源和生态支持方面更表现出色，特别是对国产 GPU 的支持，我认为这将成为未来在实际应用中的强大优势。随着算法性能和效果的逐步提高，即便存在微小的差距，也不会对实际工作产生太大影响。重要的是对国产 GPU 的支持，它将是成功应用落地的关键因素。

此外，我们也提到了对「小型化部署」的支持。拥有 1.5B 和 3B 模型的小型化部署在很多场景中至关重要。如果没有这类部署能力，很多端设备如安防设备将不能使用，例如在无法联网或在隐私需要保护的情况下。小型化部署可以在这些场景中扮演重要角色，特别是对于 1.5B 和 3B 这样的模型，它们的价值和应用潜力是巨大的。

在今天的课程中，我们还提及了自己正在开发的一些课程，包括多模态课程大纲。这个大纲将涵盖各种多模态主题，例如图生文、文生图，以及步态扩散学习等领域的内容。

本次的内容概述到此，我们期待在后续课程中为您呈现更多精彩的讲解和深入的模型分析。