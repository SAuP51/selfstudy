# 0307. 假设检验 2：为什么会导致错误结论？

刘嘉·概率论 22 讲

2020-07-31

上一讲，我们说了假设检验的基本逻辑。假设检验很强大，推动了好多学科的发展。但是，是不是只要正确使用了假设检验，而且 P 值还特别小，就一定能得到靠谱的结论呢？还真不一定。

1998 年，《柳叶刀》曾发表过一篇论文。在论文作者调查的 9 个儿童里，有 8 个都是接种了麻疹疫苗后出现了自闭症。正常儿童患自闭症的概率大概是 1% 左右，但现在，9 个孩子里竟然有 8 个。这时候，P 值小到几乎是零了。于是，经过一番假设检验，论文作者声称，「接种麻疹疫苗会增加孩子患自闭症的风险」。这可把家长们吓坏了。《柳叶刀》可是权威的医学杂志，这个结论怕是没错了。所以，很多家长就不给孩子打麻疹疫苗了，美国麻疹疫苗的接种率大幅下降。

但在 2010 年，事情发生了反转。《柳叶刀》撤销了这篇论文，说论文的结论完全是错的。但是，损失已经无法挽回。美国疾控中心的数据显示：从 2001 年到 2015 年，美国未接种麻疹疫苗的儿童数量翻了 4 倍，沉寂近 20 年的麻疹卷土重来，很多孩子付出了健康乃至生命的代价。而且「造谣一张嘴，辟谣跑断腿」，现在还有不少家长相信这个谣言。

奇怪！为什么好好的假设检验，却会导致错误甚至是荒谬的结论呢？其实，假设检验有很多坑，用不好就会掉到坑里。这一讲，咱们就详细说说这些坑。只有了解这些坑，尽量避开它们，我们才能更好地使用这个方法，让它发挥更大的作用。

## 7.1 忽视小概率事件

首先你得得明白一点，假设检验这个方法本身就是有瑕疵的。理由不难理解。我们的结论，不管是 H0 还是 H1，针对的都是全部、所有、每一个；但我们用来假设检验的，却只是一些个别的样本。就像女士品茶，说「女士能分辨出区别」，就代表她每一杯都能分辨。但实际上，我们只试了六杯。真要扛起来，六杯都说对了，只代表她这六杯说对了，不代表下六杯也能说对，更不代表她每一杯都能说对。对吧？

虽然我们中间用了小概率这个工具，认为如果概率特别特别小，就当它不会发生。但现实是这样吗？小概率事件就真的不会发生吗？当然会。

比如，有个叫亚当斯的美国人，短短四个月中了两次乐透彩票的头奖。天啊，中一次头奖的概率已经低到千万分之一了，中两次的概率几乎就是 0。但这件事，确确实实就发生了。彩票公司不能因为他连续中头奖的概率几乎为 0，就拒绝兑付奖金。所以你看，既然假设检验要从个别推导全部，就一定会忽视一些极端的小概率的情况。这是它从娘胎里就带着的基因缺陷，没法改变。

## 7.2 导致系统性偏差

假设检验的第二个问题是，它很容易导致系统性偏差，让人们更容易相信一些反常的结论。

什么叫系统性偏差呢？上一讲说过，P 值会直接影响我们最终的结论。但我们知道，P 值的大小是由样本决定的。选择的样本不同，就会得到不同的 P 值。也就是说，只要不断改变样本，就能不断改变 P 值，最终总能找到一个非常小的 P 值，也就能推翻原假设，得到一个自己想要的结论。但别人不知道他是怎么得到这个 P 值的，不知道他可能偷偷摸摸试几百次了，一看方法没问题，就相信了。这就叫系统性偏差。

1『确实，很多 Paper 里 P 值都是在 5% 低一点的区间，很可能就是专门找那些实验中的部分数据凑到 5% 以下的，样本的选区非常重要。（2021-02-14）』

拿著名的邮件骗局来说吧，你可能也听过 —— 你的朋友每天收到第二天股票涨跌的消息，连续 10 天、20 天都是正确的。于是他就推断说，这个发股票消息的人是股神，要找这个人理财。真的是这样吗？

我们知道，股票一般就只有涨和跌两种状态，如果概率一半一半，那我今天给 1000 个人发消息，500 个发涨，500 个发跌。如果明天涨了，再给收到涨的这 500 个人中的一半发涨，另一半发跌。长此以往，总会有几个人在好多天中，收到的都是完全正确的预测。

在这个骗局里，不管我们怎么用假设检验，一定会得到一个非常小的 P 值，从而推翻原假设，认为这人就是股神。为什么呢？因为我们只能看到那几个一直收到正确消息的人，而看不到还有几百、几千个收到过错误消息的人。就像一座冰山，只看到露出水面的一小部分，却看不到水下的大世界。科学研究也是一样。很多论文的结论特别颠覆，假设检验的推理过程也没问题，但后来却被证明是错的。这是怎么回事呢？

你可以想象这样一种情况：可能有 200 个团队都做了这个试验，但 199 个团队都没有得到什么反常的、有价值的结果。恰巧有一个团队，因为随机原因或者操作误差得到了一个颠覆的结果，这时候，他们赶紧发表论文，并且马上就会被媒体渲染成重大发现。但其实，这一个团队的结论只是偶然，只不过其他 199 个团队都没有发现反常的结论，没有发表论文罢了。我们只看到这个发表的论文，却没看到那 199 个没有发现异常、无法发表论文的结论，当然就会轻易相信它。本质上，和邮件推荐股票的骗局是一样的。

为什么对于很多号称有重大发现的单一试验的结论，科学家都会先存疑，然后再分头去重现、去验证？其实就是这个原因。孤证不立嘛，一次试验可能有很大的偶然性，只有很多试验都验证了某个结论，我们才能相信它。也因此，严谨的科学论文中一般不说「我们证明了啥」，而是说「我们认为什么和什么有统计的显著性」。

1『所以目前心理学的很多研究结论，没法复现，都应该持怀疑态度。（2021-02-14）』

## 7.3 显著性水平设置不好导致错误

上面两个，是假设检验这个方法本身的问题，但假设检验里的坑可不止这些。在使用假设检验的过程中，也有两个坑是要注意的。最常见的，就是显著性水平的设置要跟问题联动。

上一讲说了，显著性水平是约定俗成的，在不同的领域，需要选择不同的标准。举个例子 —— 传染病或者癌症的早期筛查，显著性水平的门槛能无限提高吗？显然不能。因为在这些时候，医生会先假设「这人没有患病」，然后通过各种检查，发现可疑的病症，把真正的病人找出来。如果显著性水平的门槛特别高，比如说 1%，甚至是千分之一，就会让推翻原来的假设变得特别困难。很多人明明患病了，但就是没法确诊，结果会漏掉大量的病例，不仅耽误病情，还可能导致传染病的传播。这危害可比误诊大多了。所以，对于这种情况，我们需要适当降低准入门槛，允许一些准确率只有 90%，甚至 80% 的检测仪器、试剂进入临床，通过多维度筛查，尽可能避免漏诊。

相反的，如果是物理学研究呢？显著性标准的门槛也应该降低吗？当然不行。

门槛降低，零假设就很容易被推翻，也就很容易得到各种牛鬼蛇神的结论。这时候，我们怎么确信一个科学发现是真的，而不是受到了设备误差的影响呢？经常有物理学发现被证明不靠谱，你能接受吗？所以这时候，我们的门槛要大大提高，让推翻零假设变得特别困难。虽然相应的，做出新发现也变得困难了，但起码保证每一个发现都确凿无疑。像大型强子对撞机发现希格斯玻色子这个实验，就需要百万分之一的显著性标准。

你看，显著性水平的设置要和问题联动，依领域而定。如果在该严的领域放宽了标准，或者在该松的领域设置了过严的标准，就可能导致一些错误。

## 7.4 用错分布导致错误结论

除此之外，还有一个小小的坑，虽然很明显，但也有人用错，所以咱们也说一下。什么坑呢？就是用错分布。比如，一般假设检验只用于正态分布，如果一个随机事件明明不是正态分布，却偏要用假设检验，当然会出错。

就像国家统计局说，2019 年，北京平均月工资是 7828.49 元。你想判断统计局公布的数据靠不靠谱，能用假设检验吗？可以随机选择 50 个人，看看他们的平均收入在不在 7800 元附近吗？当然不能。前面说过，人的收入不服从正态分布，而是服从幂率分布。而幂率分布，根本没有均值和标准差。这时候，再用收入的均值做假设检验就没有意义了。即使都是正态分布，用不对也一样会错。就像上一讲，明明是菲律宾人的身高问题，你却拿出亚洲人的身高分布做比较，或者拿出菲律宾人的智商分布做比较，分布都不是一个，结果当然也是错的。

你看，想要把假设检验用好，还得选对分布才行。用错了分布，结果必然是毫无意义的。

## 黑板墙

思考题：2019 年双十一，有人怀疑阿里的数据造假，因为它非常吻合二次或三次函数回归。结果，马云和张勇都出来辟谣。你是怎么看待这个问题的？

下节预告：从下一讲开始，我们就要进入课程的最后一个模块 —— 贝叶斯方法了。我们一起来看看，这个当今应用最广泛的概率论流派，到底是怎么回事。

### 01

之前网友是通过 2009-2018 年的实际数据，来推测 2019 年的销售数据，然后得到数据过度拟合，「疑似作假」的结论，这里的论证是不够严谨的。首先是系统性偏差的问题。选择不同的样本空间，就能得到不同的结论。双十一的数据有很多，总的销售数据只是诸多数据当中的一个，目前没有证据表明，其他的数据都有过度拟合的现象。换句话说，可能是网友在计算了所有种类的数据之后，发现只有总的销售数据有过度拟合的现象，然后就把它提出来，作为一种规律性的认识。

其次是小概率事件的问题。十年的数据，作为決策的参考是可以的，作为统计数据的规律性认识还远远不够。就好比你不能拿扔 10 次硬币的情况，来推断硬币正反面的分布概率。同样的情况在足球界普遍存在。比如，欧冠改制后，有很多年没有球队卫冕成功，成为媒体嘴里的魔咒，但是随着皇马实现三连冠，这个魔咒不攻自破。所以，小样本空间涌现的小概率事件不足为信。

作者回复：是的，二次函数三次函数在统计学中，能很好的拟合很多孤立的有限点，这不算小概率事件。你假设它没有造假，出现个小概率才能推翻零假设。但是这未必是小概率。

### 02

甲同学的回答：高拟合度说明不了任何问题，一个复杂的高阶多项式可以「过拟合」任意数据，达到一种完美拟合的效果。阿里双十一的数据应该也就是近 10 年的吧，想要找到完美拟合的函数还是很容易的。另外，阿里也是做了拟合、建了模型，有依据的提出业绩目标，员工会想尽一切办法完成业绩目标，这也会导致数据更像拟合的函数。

作者回复：非常好，这是这个问题最好的答案之一。被二次三次函数拟合本就不是小概率。所以不能推翻原假设。

乙同学的回答：回到质疑阿里数据造假的案例，其实那位质疑的博主是通过历年阿里双十一的数据，在坐标系中画出散点图，然后通过 Ex cel 的数据曲线，用一个函数来拟合，后来发现和二次函数，三次函数的拟合度很高，于是质疑造假。是如果在公司里做过预算，就发现其实可以从业务角度去寻找原因。通常起点是，老板们基于实际情况，以及各利益相关方的预期，给今年设定一个「跳一跳够得到」的 KP，然后各部门就开始行动，把这个 KP 分拆成流量，参与的商家数量，促销方案，定金预付比例，定金转換率等等等等。所有的部门在双十一的当天都会按照这个计划去执行，到最后实现这个目标，大家皆大欢喜拿奖金。其实大家不要忘记了，一开始老板那个设定的那个 KP，本身就可能是通过趋势函数计算出来的，实现这个目标，只能说明阿里团队执行力实在是太强大了。

1『这两人的回答提供了一个很新颖的视角，可能是阿里高层下指标的时候是根据函数拟合来的，而下面团队也完成了这个指标。（2021-02-14）』

### 03

学完今天的课，我想起了薛老师曾说的一句话：这个世界上每个人每分每秒都在创造浩瀚如海一般的数据和信息，只要你足够仔细，在浩瀚的信息中找到一点信息来佐证你的观点，还是不难的。以前有很多心理学家做出来的报告每隔一段时间都会被推翻，比如「只要一个人做出微笑的表情他的心情也会相应变得好起来」等，后来都被验证出不了重复性，原因就是样本的选择有问题，天知道他们是选了多少人、哪些人来做实验呢。这也是科学家「鄙视」心理学家的原因吧，因为「科学」讲的不仅是证明，而且是「不可证伪」。所以一看到「调查表明」等字眼，我都会本能的怀疑一下。

作者回复：孤证不立，但心理学社会学行为经济学确实有很多事，是反复做试验确认过的。

### 04

万维钢书中写到 p 值的动机性推理，就是先有结论，再在推理过程中刻意符合这个结论。学术圈，把结果显著，约定为 P 值小于 0.05。而很多科学家刻意选择美化数据，达到 P 值显著的目的。研究者把各领域历年论文放在一起，分析这些论文中 P 值的分布情况，P <0.05 是人为约定，理论上，论文中 P 值的分布应该是一条光滑的曲线，但研究发现 P 值的分布，在 0.05 处有明显凸起，这就是因为很多论文，通过美化数据，把 P 值「做」到了 0.05 以内。

作者回复：符合统计学本质的做法，P 值只做验证，你已经有了其他证据证明或者推演或者逻辑上的论证了一个结论，然后用 P 值验证一下，作为数据证据。现在很多学科，P 值是唯一的证据，结论没有逻辑、没有推演、没有其他证据。这是误用了假设检验和 P 值。越极端越引人瞩目吧。

### 05

一般假设检验只用于正态分布，如果一个随机事件明明不是正态分布，却偏要用假设检验，当然会出错。假设检验应用注意点：1）样本选择要多样行，样本要全覆盖，如同审计抽样的方式，既要抽发生额，也要抽余额，避免余额小发生额大的漏掉，导致审计结论错误；2）小概率事件的影响，就像审计过程中，既要重视金额大小，也要重视小金额事项性质，就像舞弊事件无论大小，如不做审计调整，都需要发表非标审计意见。严谨的科学论文中一般不说「我们证明了啥」，而是说「我们认为什么和什么有统计的显著性」。这句话马上联想到审计报告的阐述结论方式「按照 XXX 准则，重大方面公允反映 XXX」，异曲同工。「幸存者偏差」貌似是样本不全而导致结论错误的概念。

作者回复：审计这件事儿，你猜银行、公司等等，所有的工资、报销、业务收入，业务支出的那么多笔金额中，以 1 开头的占多少？是 1/9 吗？频率法还有个本福特定律，专门干这事儿。

### 06

加里·史密斯在《简单统计学》里面，就曾经多次提到过一种叫做「数据搜刮」的现象。简单地说，这就是利用了老师讲到的，假设检验「从个别推导全部」的固有缺陷，从样本数据中「挖」出一些惊世骇俗的「规律」来。解決这个问题最好的办法，就是将前一步假设检验得到的结论，作为一条新的假设，投入到新的数据样本中进行检验。科学研究中对实验的复现，就是这种再检验过程。而在生活中，当我们听说似乎有某种「理论」时，也要保留一些怀疑，把它放到新的「样本」里（不同的情境、条件下）验证下，或者在网络上搜索类似的验证信息。越是那些看上去很宏大，其实漏洞百出的所谓「规律」，就越容易露出破绽。这是避免被人忽悠，特别是避免被伪科学忽悠的好方法。

### 07

假设检验的坑：1）忽略小概率事件，从个别推导全部，一定会忽略某些极端的小概率事件；2）导致系统性偏差，样本決定 p 值，p 值影响结论，反过来可以指哪儿打哪儿，先有结论，得到 p 值，根据 ρ 值反选样本；3）显著性水平设置不好导致错误，显著性水平的设置与问题相关联，但有的场景需要各方权衡，人为主观调高或调低标准，标准差异会带来一些错误；4）用错分布导致错误结论，假设验证一般用户正态分布，如果幂律分布或其它分布或错配的正态分布，得出的结论是没有意义的。

1-3『

对于第二点的系统偏差印象深，之前没觉得什么，直到读了书籍「2021055简单统计学1101」，里面提到了「德克萨斯神枪手」谬误，先「射击」再「画靶子」。算是解答了自己之前迷惑很久的难题。为啥一些科学研究符合零假设检验（P 值小于 5%），还是不靠谱。那些 Paper 完全是先做大量实验（先射击），再从这些实验里挑选对自己有用的数据样本（再画靶子）。这里的信息补充进主题卡片「即使是随机数据，也可能存在聚集现象」。（2021-02-14）

总结：数据聚集现象无处不在，甚至存在于随机数据之中。想要寻找某种解释的人一定会找到一种解释。不过，某种理论与数据聚集现象相符并不是一种具有说服力的证据。人们发现的解释需要言之有理，而且需要得到新数据的检验。类似地，向足够多的目标发射足够多的子弹的人一定会击中某个目标。对数百种理论进行检验的人一定会发现至少支持一种理论的证据。这种证据不具有说服力，除非理论是合理的，而且能够得到新数据的证实。如果你听到某些数据支持某种理论，那么在你相信之前，请回答两个问题。首先，这种理论是否合理？如果不合理，不要轻易相信胡言乱语是合乎情理的。其次，房子里是否存在一位德克萨斯神枪手？宣传这种理论的人在提出理论之前是否查看了数据？他是否在选定他所宣传的理论之前对数百种理论进行了检验？如果你看到了冒烟的枪口，不要急于做出判断，应当等待这种理论接受其他数据的检验。

对于军团病，人们根据数据聚集得到一种理论，并用新数据对其进行检验。如果没有理论和新数据，我们所拥有的仅仅是数据聚集而已。韦特海默和利珀发现的癌症受害者聚集是「德克萨斯神枪手」谬误的一种形式。在最初的版本中，一个没有射击技能的人用一把枪向谷仓的一面墙射出大量子弹，然后在弹孔最多的位置画上靶心。即使癌症的地理分布是完全随机的，也会出现巧合性的聚集 —— 就像 100 次随机射击的弹孔会出现随机聚集一样。要想进行有效的统计检验，研究人员应当首先画出靶子，然后发射子弹 —— 首先论证输电线可能导致癌症的原因，然后比较有输电线和没有输电线的街区发生癌症的频率。

』

冯诺依曼说，用四个参数我可以拟合出一头大象，而用五个参数我可以让它的鼻子摇起来。所谓拟合，就是寻找一条曲线，使它尽可能地靠近已知的若干个点。从阿里来看，一定会制定销售目标的，而销售目标是根据以往的数据制定的，跳跳才够得着，于是拆解出很多手段来帮助实现目标，如各种优惠券、花呗临时提额等；从网友来看，假设验证的前三个坑他都踩了，拿十几个数据来推导全部、p 值和显著水平都能根据过往数据完美匹配出一条曲线，所以博眼球的成分更大

### 08

运用假设验证的方法，确实可以解决真实世界中的很多问题，特别是符合正态分布的事件。比如说各种保险产品，通常情况下，保险公司会根据以往统计数据，计算风险发生的概率，从而确定人身、财产、重疾等各类保费标准。这样的做法，确实可以在很大程度上应对很多随机性来的风险。但涉及到具体的个体，情況就有些不ー样了。有些风险一旦发生，产生的后果往往会超过大多数人的预期，对每个人生活的影响，也是干差万别。

比如最简单的生病来说，很多人都觉得，只要有钱，不就是花钱治病吗？其实还真不是，一旦真的得了像癌症这样的重大疾病，就很有可能意味着接下去好几年都没法正常工作，也就失去了最重要的经济来源。所以，很多时候，要考虑这样的高阶风险，我们就不能只买医疗险，还需要考虑重疾险；就是为了保障患重大疾病之后，不会因为没有收入，而造成生活质量的下降。

再比如说死亡，即使个人真的意外故去，父母、孩子今后的怎么办？他们以后的生活会不会出现什么重大风险？这个时候，寿险、年金险就是你需要应对各种意料之外情况的解決方案了。所以，假设验证的方法，并不能帮助我们解决所有问题；我们还需要掌握运用已知应对未知的方法，应对那些随机性所来的未知风险。这也是我们每个人都必须面对和学习的人生课题之一。

作者回复：假设检验本质是对一个结论的验证，而并非发现。