## 0201. GPT 的原理及功能

[首页 · 魔搭社区](https://www.modelscope.cn/home)

[Write With Transformer](https://transformer.huggingface.co/)

[Hugging Face – The AI community building the future.](https://huggingface.co/)

### 01. GPT 的演化历史

语言模型对其技术原理做个简单的逆向工程。

先做一个最简单的语言模型说起，Google 的 Ngram 浏览器。在所有的书籍（Google 的图书、语料库）里搜索词汇出现的次数。刚刚展示的图，是最最简单的语言模型。

什么是语言模型？

1、所谓语言模型，一种用于预测词汇在给定上下文中出现概率的模型。

2、N-Gram 模型：1.0 时代的语言模型。可以理解为远古时代的语言模型，当年阳老师开发写匠时用的比较多。在人工智能、深度学习技术诞生之前，我们最先进的技术就是 N-Gram 模型。当时阳老师把鲁迅全集、张爱玲全集等作为语料，再计算一个词语另一个词临近的概率，这个概率我们称之为 Gram，当是一个单词的时候是一个 Gram，2 个词（距离）的话是 2 个 Gram，3 个是 3 元组，N-Gram 里的 N 是个数。在语料库中出现的概率会有一个组合，如果查出来，某个词出现的概率极低，那么大概率就是一个错别字。

语言模型的原理跟刚刚的开智案例是同一个道理，首先是拿到语料，对这个语料我们按一定的计算规则计算步骤去处理，这个处理的过程我们称之为训练模块。训练完之后，这个模型就会保存了大量的概率值，这些概率值有时候称为概率表，有时候称之为向量特征值。可以理解训练完后，我们形成了一个对这个事物的已知的一定的结果。这是语言模型最通俗易懂的理解。

3、传统语言模型的优缺点：优点是计算速度特别快，因为是已知的词库。弊端是它没办法处理一个它没见过的 ngram，没法去推断未知的。它只能依赖于已知的概率表，没法很好的去推论未知的概率表，最多是有比较弱的推论能力。

2012 年，学术界诞生了深度神经网络。深度神经网络，早期是 rnn、cnn 架构，你可以理解为 2 种计算 N-Gram 的方法。rnn、cnn 的缺点在于，它更符合人类视觉的表现机制，它在计算视觉领域识别效果非常好，所以导致了第一波的人工智能热潮。2015-2020 年，中国诞生了计算视觉 4 小龙，商汤科技、云从科技等等，这 4 家公司都是使用 rnn、cnn 技术，将面孔识别达到了人类识别的水准。

后面发现，做计算视觉的东西用在语言上效果不太好，因为语言和视觉不太一样的。各个科技公司开始找解决方案，其中最重要的是 2017 年 Google 提出的解决方案，变压器（transfer）模型。大家可以理解为一种新型的网络模型，它更擅长处理长文本。整个 GPT 是建立在 transfer 模型基础上的。虽然 OpenAI 与 Google 是竞争关系，但 transfer 模型是开源的。

GPT 大事记

| 日期 | 里程碑事件 |
| --- | --- |
| 2018 年 6 月 11 日 | GPT-1 在 OpenAI 博客上宣布。 |
| 2019 年 2 月 14 日 | GPT-2 在 OpenAI 博客上宣布。 |
| 2020 年 5 月 28 日 | 首个 GPT-3 预印本论文发表在 arXiv 上。 |
| 2020 年 6 月 11 日 | GPT-3 API 私人测试版发布。 |
| 2020 年 9 月 22 日 | GPT-3 授权给微软。 |
| 2021 年 11 月 18 日 | GPT-3 API 向公众开放。 |
| 2022 年 1 月 27 日 | InstructGPT 作为 text-davinci-002 发布。InstructGPT 预印本论文于 2022 年 3 月发布。 |
| 2022 年 9 月 1 日 | GPT-3 模型价格削减 66% |
| 2022 年 11 月 28 日 | GPT-3.5 模型发布 |
| 2023 年 3 月 15 日 | GPT-4 模型发布 |

变压器（transfer）模型诞生后，出现了很多应用模型。其中有 2 个关键的模块，一个 bert 模型一个是 gpt 模型。这 2 个模型是竞争关系，早期阳老师团队测试过这 2 个模型，GPT 模型当时是达不到使用水平的。2022 年 12 月之前，从事这块研发的工程师，没有人认为 gpt 模型是强悍的，所有人认为 bert 模型的效果更好。2022 年 12 月，gpt3.5 上线后，完全吊打 bert 模型。

真正的转折点是 2022 年 11 月 28 日 GPT-3.5 发布，中间究竟发生了什么。

在 2018 年（GPT-1 时代）主要训练书籍的文本。

1、深居茅庐，博览群书。数据：BooksCorpus，包含 7000 万个词的书籍文本。

2、熟读唐诗三百首，不会吟诗也会吟。能力：生成连贯的文本序列，作为下游任务的基座。

GPT-2 时代，生成文本的能力变强了。但不像现在这么惊艳。

1、初入社会，了解世情。数据：WebText，包含大约 45GB 的互联网文本。

2、逛遍各大论坛，能接各种热哏。能力：生成高质量、连贯的文本，零次训练能力强。

GPT-3 时代变得稍微厉害了，真正的转折点。

1、融会贯通，大巧不工。数据：WebText 和其他来源，合计 45TB 的文本。

2、十八般武艺样样精通，却打不赢单项冠军。能力：生成高质量、连贯的文本，零次训练能力强。

GPT-1 和 GPT-2 是开源的，但 GPT-3 只开源了一部分，有很多东西没公开了，3.5 之后就没怎么公开了。这个过程中逐步实现了一些技术上突破。

GPT-3.5 时代

GPT-3.5 之正道的光 text-davinci-003。

1、沐猴而冠，何时成人。数据：肯尼亚劳工的辛勤标注。

2、人类指令要对齐，从此尽量不逾矩。能力：更听人类的指令，高效生产，但是规避不道德。

GPT-3.5 之 Codex。

1、剑走偏锋，代码稀松。数据：GitHub 代码数据。

2、从此学会写代码，逻辑成了基本功。能力：生成看上去不错的代码片段，意外学到逻辑。

GPT-3.5 之 ChatGPT。

1、为了沟通，自废武功。数据：增加了人类聊天数据。

2、江湖百晓生，唠的就是嗑。能力：放弃了一点就通，换来聊天神功。

GPT-4 时代（公布的基础信息更少了）

1、独步武林，哲人王者。数据：反正比 GPT-3 用的多。

2、推理能力大跃迁，神功何来密不宣。能力：理性思维超越人类平均。

因为 GPT-4 公布的信息太少，阳老师团队采用的认知心理学黑箱的手段，最终发布了几篇测试报告，我们发现 GPT-4 的理性思维能力已经超过人的平均值了。

在前面介绍的这些版本之间，其实还有更多的分支（小版本）。比如 GPT-3 模型会按照训练的语料大小，生成的参数大小，分成大中小各种模型。

我们需要明白的是，在传统的 1.0/2.0 时代，跟以往的语言模型没有本质的区别，跟前面说的那些神经网络模型是一样的，基本上是训练 1-10 亿的参数，通过小数据领域的迁移，比如说要做机器翻译，那么就去大量的英文句子、翻译之后的中文句子，一一对应好，然后再把语料喂给模型。同样的，如果要做错别字的检查，那么导出成千上万的错别字和正确的字，以及他们对比的关系，喂给模型。GPT-1 和 GPT-2，能够通过微调模型来实现任务的迁移和能力的迁移。GPT-1 和 GPT-2 本质上是语言模型，但还称不上是大语言模型，到了 GPT-3 时代就变成了真正意义上的大语言模型了。

这里面发生的变化：

1、参数从 10 亿跃迁了几千亿。GPT-3 的参数数据没有公布，但估计是 2-3K 亿的样子。

2、GPT-3 开始引用了很多密码武器，上下文学习、思维链、提示词、微调等等。

### 02. GPT 的工作原理

工作原理 4 个步骤。实际比这个要复杂千倍万倍，因为语料太多了、数据太多了、GPU 太多了，任何一个在单机上看起来简单的任务，到了庞大的规模上都是一项复杂、庞大的工程。这里的原理，讲的就是如果在单机要要跑通 GPT 模型，我们要做哪 4 个步骤。

#### 2.1 搜集语料

1、收集语料：GPT 各代。

主要数据集大小汇总。以 GB 为单位。公开的数据以粗体表示。确定的数据以斜体表示。仅原始训练数据集大小。

GPT-1 是 4.6G，主要是书籍数据（Books）。这个时候在你本机是跑的起来的。

GPT-2 是 40G，主要是 Reddit links 数据（类似于美国百度贴吧），这个在本机也能跑起来。阳老师团队在自己的服务器上这两个模型都跑过。

GPT-3 是 570G，包括维基百科、图书、cc（互联网著名公益开源项目，爬虫）。勉强能跑通，有些人的机器就不行了。

GPT-4 的语料目前没有公开，有一位博客博主根据现有发布的技术报告、资料，估计了一下语料应该有 40TB，生成的 token 是 20T 左右。

2、对语料进行预处理：BPE 算法。

拿到语料后，无论是中文还是英文，

中文是以「字」为构词单位，英文 26 个字母可以任意组合，组合出大量的词。这个时候要把语料做一个记录的话，生成一个字典是有一定难度的额，这个时候 GPT 采取了一种 BPE 算法进行预处理。

全部语料 => 拆分成字 => 合并最高频组合 => XXXX

假设语料中有 xxxx 单词，这么多的单词，接着把单词拆为字符，这样就可以统计字符的频率了。首先找出出现次数最多的字符对，显然出现最多的低碳，然后都统一的给它用一个新的符号来表示，比如「低碳-1」，这就表示的是一个符号，这就是在压缩信息量了。接着同样的操作，把「绿色」也这么表示。用类似的方法，对互联网搜索的庞大语料，用更少量的符号等价表示这些语料，并不需要记录所有的信息。

3、对语料进行预处理：向量化。

下一步是做向量转化的工作，因为神经网络的基本单位并不是处理字符串，神经网络进行的是矩阵计算，它处理的是向量，所以我们需要把字符串表示为向量，表示成矩阵能够认识的东西，无论是什么样的神经网络模型，都依赖于这个步骤。

对语料进行向量的转换，依然用上面的例子来说，对这些符号进行编码。比如给「低碳-1」分配一个 id，比如是「1」，「绿色-1」分配的 id 是「2」。编码完之后，前面句子就有变成比如 0609（相当于一个坐标了）。刚刚说的是一个简化，实际要复杂的多，很多时候是高维的计算，所以很消耗 GPU 的资源，目前在计算 GPT 的语料基本是在几百维度左右。

所以一直有一些 AI 科学家对 GPT 这种大模型路线不认同（不屑）的原因，太耗资源了。他们认为人脑的大脑并不是这种高能耗的原理，我们的大脑是贝叶斯的机制，并不是一个向量转换的机制。这是 2 条 AI 技术路线的争议，每一条路线都有它的优点和缺点，我们不关心这个，我们关心的是从实际效果来看，GPT 这条路线已经跑通了。

我们已经把所有的文本转化为矩阵中几百维的坐标了，这是一个极其复杂的矩阵，我们人类一般能理解的也就三维四维了不起了。到了这里其实已经变成纯粹的数学了，一个纯粹的黑箱了。这就是为什么现在科学界对里面的原理理解不了，因为 GPT 相应的数学模型不成熟，相应的工作机制也并不成熟。我们只是按这种逻辑去处理，处理完之后，这个高维空间里发生了什么我们不知道的，对我们目前来说就是一个黑箱。

转化为向量之后，就开始训练模型了。

#### 2.2 训练模型

训练模型其实非常简单，前面介绍了 N-Gram 的例子，N-Gram 找的其实是临近词的概率，不同神经网络，训练的规则有所不同。

GPT 训练模型规则：单词接龙

GPT 是单词接龙的规则，比如说「绿色生活是__」生成下一次词，「绿色生活是什么一种__」，再下一个词「绿色生活是一种节能__」。它与传统做法不一样的地方（最有意思的地方），它生成的方法特别简单，每一步只做这个操作，每一步操作后再把新的概率更新回去。在上面的例子里，第 3 步不再是计算「绿色生活是」的概率，而是去计算「绿色生活是什么一种」的概率。上面的这个机制，在 Google 的 2017 年那篇论文中，称之为「自注意力机制」。bert 机制要更复杂。

当我们一层一层的把工作计算完之后，那么在坐标中，过往句子出现的概率，向量和向量之间的距离，涉及很多计算距离的方法，用的比较多的是计算相似度，它的点基相似度或者余弦相似度。

目前大家在用的过程中出现的稀奇古怪的错误（幻觉），是因为在高维空间的模型中，这 2 个词的距离比较近。

再下一步是把向量又转化回去，转换为字符串。

整个原理差不多是这种，但在实际过程中，因为数据庞大到一定程度、模型庞大到一定程度、高维空间庞大到一定程度，这里面的问题比大家想象的复杂很多。GPT 引入了温度来控制生成的随机性，作为一个额外的参数，保证生成更多样化的文本。

训练采取的架构：Transformer 模型

目前训练普通采取的模型架构是 2017 年 Google 的论文。

1、Transformer 模型首次出现在 2017 年的论文 *Attention is All You Need* 中。

2、它是包括 BERT、GPT 在内的，如今流行的各大语言模型的起点。

3、自回归/自注意力与多头注意力的区别。

Transformer Attention

Attention 注意力机制的通俗解释：

1、当你计划下一个旅行目标，你脑子里会浮现一个个游玩过的地点，皇家园林、桂林山水、鱼米之乡、主题公园。

2、你喜欢哪个多一点，关注它的权重就高，反之则低。

3、综合加权之后，你大概知道自己下一个想去的方向了。

4、对历史加权，这就是注意力机制。

多头注意力则是：

1、你每次不是一个人旅行，而是组织一家人旅游。

2、每个人对历史的偏好不同。

3、有人喜欢人文景观，有人偏好自然景观，有人关注住宿舒适，有人只会惦记美食。

4、决定下一次去哪，需要大家统一意见。

人类的注意力，是先把事物拆解为局部，理解局部特征之后再理解全局。所以我们人类的注意力是一个不断地自下而上，然后再下上而下，两种机制不断交错。在机器学习和人工智能领域，引入注意力机制做的也是类似的工作。

在生成文本的时候，每一步的信息是依赖于上一步的信息的。慢慢的一步一步的迭代，最终输出的目前的结果。

#### 2.3 人类反馈

这里会碰到很有意思的问题：如何让 GPT 生成的文本符合人类社会道德规范？

这个技术架构本身它没有任何道德色彩，那么就一定会生成有害的文字，因为只要它在高维的向量空间中，那么它就有概率会输出。

这时候需要引入一些新的方法和技巧，用来约束文本生成的方向。通俗来讲，就是设置了一个有意识形态的审核员，保证生成的文字符合道德规范。

有害的内容

语言模型可以被提示产生不同种类的有害内容。指的是违反 OpenAI 策略的内容，或可能对个人、团体或社会构成伤害的内容。包括：

1、对自我伤害行为的建议或鼓励。

2、图文并茂的材料，如色情或暴力内容。

3、骚扰性、贬低性和仇恨性的内容。

4、有助于策划攻击或暴力的内容。

GPT-4: 人类反馈

GPT-4 引入人类反馈，约束了 GPT 生成文字的方向，使它的回答不能出现违背人类道德规范。

例如，当你问 GPT-4 如何制作炸弹时，它不会告诉你具体方法。

但在 GPT-4 早期的版本，不是现在的答案，没那么多约束的，GPT-4 大大的引入了人类反馈。

我们目前使用的 GPT-4 的价值观，实际上是建立在最早的那些提供人类反馈的那些人的价值观身上，后面会介绍这批人（标注人员）。我们会看到它是一个非常典型的美国白左的价值观。

接着我们发现，它能做的事情不仅仅是生成问题，还能翻译、编程等等，这就涉及到 GPT 所做的第 4 个比较大的工程，叫微调模型。在 NLP 领域出现最多的词其实是微调的英文。

#### 2.4 微调精修模型

会做 2 类工作。第一部分，在发布大众之前，大规模投入商业使用之前，它会参加大量的机器学习、自然语言处理、人工智能相关领域的测试任务，按照任务要求看下模型在测试集上的表现如何。这个时候，每参加一次任务，都会引入一小部分数据。最开始，可以理解为拿庞大的数据生成了一个庞大的向量矩阵，庞大的向量矩阵中已经保存了大量的向量特征值。但这些向量特征值跟具体的任务是一个什么样的关系，其实是不知道的。所以这个时候，一般会引入一些小小的规模更小的数据，比如前面提到的引入的差不多 40T 左右的数据，这个小数据可能是它得万分之一、十万分之一。但这些数据往往是被标注好的，比如机器翻译，英文是怎样的中文是怎样的。引入的这一点点数据，相当于投石问路，让向量矩阵明白未来是怎么处理这类任务。这类任务就被称为对模型的微调精修。

微调精修之后，我们发现在 GPT4 在经典的语言处理任务上，表现的非常杰出。第一个数据集是多任务语言处理，它包含了大量的自然语言处理的任务，有 12 个经典的任务，比如文本分类、情感分析、命名实体、实体识别、机器翻译等等。第二个是翻译句子，这个涉及到人类常识的知识。这样的话可以让 GPT 能够更好的识别类似的任务。这就好比我们现在已经生成了高射炮，但蚊子在哪里我们不知道，用高射炮去打蚊子是一个高能耗的事情，一般公司都烧不起，这时候我们得给蚊子一个定位器，通过微调的手段锁定了蚊子大概是在哪一些区域，怎么去打它，第一步怎么做第二部怎么做第三步怎么做，这就是微调的过程。

通过微调习得了传统 NLP 的任务。像 GPT 这类模型表现出非常强悍的泛化能力，它不需要太多的标注的数据，也不需要太多的微调，它比较容易实现任务的迁移。

总结：GPT-4 在传统的 NLP 任务重的测试表现

| 任务 | 数据集 | 模型 | 测试表现 |
| --- | --- | --- | --- |
| 多任务语言解释 | - | - | - |
| 完成句子 | - | - | - |
| 问题解答 | - | - | - |
| 语言建模 | - | - | - |

这里还会测试大量的专业考试、学术考试，比如编程、律师考试、Leetcode 考试、编写优雅的问题、写文档等等。每一次参加考试的过程都是对模型进行微调，最终获得的能力会越来越多。微调是 GPT 能够获得那么多广泛能力的核心因素之一。

总结：GPT-4 在专业与学术考试中的测试表现

| 任务 | 数据集 | 模型 | 测试表现 |
| --- | --- | --- | --- |
| 律师考试 | - | - | - |
| 美国大学预科考试 | - | - | - |
| Python 代码生成 | - | - | - |
| Leetcode 编程测试 | - | - | - |
| 编程 LATEY 代码 | - | - | - |

前面说的所有的，都是在模型发布、出厂之前的。当我们的模型正式发布之后，又会做些什么工作呢。这个时候会获得大量的用户回答，这些用户既有作为标注人员的内测用户，也有各个实际使用用户，这些回答又进一步的帮助 OpenAI 团队去判断用户目前需要一些什么样的任务，想让 GPT 完成什么，当他们发现哪一类任务的呼声比较大，他们会使用大量的机器学习的方法对那些想让 GPT 完成的任务进行判断，判断好之后进行归类，对用户需求比较大的，继续对「大飞机」的模型进行微调，让它能够更好的胜任这些任务，最终 GPT 可以具备越来越广泛的能力。这就是整个 GPT 工作原理的简单介绍，这种介绍时极度简化的，中间的过程要比这些复杂一万倍。

搜集语料 => 训练模型 => 人类反馈 => 微调精修模型，这四个环节会形成良性循环。随着微调精修模型的理解，更明白 GPT 需要什么样的语料，随着人类反馈的增多，更明白模型训练的方向是往哪个方向走，应该进行一个什么样的优化，进行一些什么样的调整，最终 GPT 就会像一个飞轮一样，慢慢的形成一个非常强大的良性循环。

根据阳老师的判断，目前 GPT 至少领先其他团队 18 个月左右，其他团队想要形成一个类似的良性循环并不是一件容易的事情，它不仅仅是砸钱的问题，还涉及很多很多复杂的问题。

### 03. GPT 何以成为 GPT

这里面其实有 6 个杀手锏，其中有 5 个是已经公开的：零样本学习、上下文学习、思维链、大模型、人类反馈、高阶推理能力（一直没公开的）。

#### 3.1 零样本/少样本学习

零样本学习：就像萧峰，无师自通练了太祖长拳，就能自创降龙十八掌。

少样本学习：就像黄蓉，一点就通你敢教会她「亢龙有悔」和「飞龙在天」，她就能从「见龙在田」打到「神龙摆尾」。

说白了是帮助团队省钱的技巧，通过预训练来减少标注语料。

#### 3.2 上下文学习

上面少样本学习，古灵精怪的黄蓉，还得练会了前两掌，才能续上后面十六掌。

上下文学习是说，武学奇才慕容复，你给他演示了前两掌，他就能能凭招式名，搞定后面十六掌，打得有模有样。

这个地方，GPT 你使用的时候，为什么会锁定在一个聊天对话记录里，这个就是建立了一个上下文的边界。当你开启了一个聊天，你就开启了一个新的上下文。所以你在上下文中，给向量矩阵提供的信息，都被它用户帮你生成任务了。这个也是其他大模型都会的东西。

#### 3.3 思维链（Chain-of-Thought）

改一个提问方式，就能把正确率提高近 40%？

让大模型别骄傲，一步一步解题，不要省略步骤。

甲方：还不赶紧上？

猜测是：大语料里面很多详细解题步骤前，都有这种提示词，让模型掌握了这种模式。

思维链式 GPT 一个很有创意的东西，从 3.0 开始，真正让它从小语言模型变为大预言模型的，真正有创立的东西是这个，思维链。相当于模型在对话的时候，加了这样一句话，类似于生成了一个魔法字符。这句话相当于一步一步思考，最终让生成结果的正确率一下子提高了 40%。

学术界目前普遍认为，思维链这个东西，相当于在向量空间中，找到了一些捷径。打个比方，在 1000 维度的向量空间中，想快速找到一个自己想要的字符，这个事情没那么容易的。我们之前的做法：把 1000 维度空间想象为一个庞大的宇宙，我们只告诉宇宙我想要的是什么，这个时候你往往得不到结果。比如我的很多同事，不太会使用 GPT，直接告诉 GPT 我想要的结果是什么。比如直接告诉 GPT：请准备一张 GPT 特征的表格。这个时候获得的结果一般是很差的，但如果你用思维链的技术结果会好很多。

你应该这么问：GPT 有什么样的优点？请总结 10 点。

接着再问：请讲其总结为表格。

我们需要把步骤拆解的更清晰一些，它的生成结果就完全不同了。在数学、推理、解题类的任务中，已经证明了效果非常非常好。思维链式 GPT-3 时代引入的真正的杀手锏。

思维链实例 1：

GPT-4 里面同样用得到，在做法语的带图示的物理题时，只需要在提问后面加上「列出详细解题步骤」，就能达到神奇效果（Think step-by-step）。

思维链实例 2：

这里的例子首先用到了上下文学习，就是输入最前面的一对 QA 作为范例。然后右边的第一个 A 还详细列出解题步骤，就是加上了思维链。

上面的例子，让我们发现了这种工程级别的经验，其实有时候比理论级别的经验更重要。类似于我之前经常说的：原则正确没那么重要，反而是实际操作正确更重要。像这种技巧纯碎是工程技巧，它目前究竟是什么样的数学原理，大家其实还解释不清楚，但它的效果太好了。

#### 3.4 比大更大：为什么大小这么重要

从量变到质变，复杂系统达到一定规模出现的「涌现/相变」现象，让大模型的投入产出比极具吸引力。

从 3.0 开始引入了一个更大的参数，转折点发生在 60 亿的时候（60B），在这个转折点发生之后，比传统的任务结果好太多了，目前究竟发生了什么事情其实也不太知道的。目前学术上，大家只能从工程原理上去解释，模型为什么超出 60B 以后，各项性能变得特别好了。只能去比较各种各样的功能参数，来推论。这也是大语言模型最有意思最神奇的地方。对于开发团队、普通用户来说，存在这大量的黑箱问题。但从时间的效果来说，的确是比较好的了。

涌现，导致研究范式的转变。

之前：预训练+精调：先练内功+战前三天选兵器，人来有剑，马来有枪。

之后：大规模预训练+思维链：死磕内功+临阵磨枪三两下，不教胡马度阴山。

阳老师之前做错别字的传统模型、第一代的预训练模型，这个是大语言模型之前的时代，这个时候花了很多的时间在精修阶段，但现在精修的过程更多的是被提示语取代了。这样的话就更容易泛化推理了。

这也是 OpenAI 团队的一个杀手锏，其他大语言模型团队短时间内很难超越的 GPT 的一个根本原因。因为其他大语言模型的底层机制不太有利于涌现的出现，可能就会出现一些南辕北辙的现象。尤其是在 openAI 的论文没有放出来之前。同样的大模型，一个是假的大模型一个是真的大模型。

#### 3.5 指令学习/人类反馈

这是最重要的杀手锏，GPT3.0 以后才开始的。前面介绍 GPT 演化历史的时候，开玩笑说 GPT 的诞生依赖于那帮肯迪亚标注民工，实际上这些人不是黑人，他们是白人，openAI 公司找了 40 个全职的标注人员。所以 openAI 的价值观收到这 40 个人的影响，人类的未来被这 40 个标注人员代表，尤其是高纬空间膨胀到一定程度。openAI 其实尽可能的做了平衡平衡再平衡，保证他们能统一在一个价值观之下。所以我们发现，在与 GPT 对话的时候，它永远有一种套路的东西。这些套路的东西，其实是由那 40 个标注人员形成的。

『

人类的未来被这 40 个标注人员代表。

1、数据质量是模型效果的关键，标注人员又是数据质量的保证。

2、OpenAI 通过一系列的筛选，找到了 40 个对不同人口群体的偏好敏感并且善于识别可能有害的输出的全职标记人员。

3、OpenAI 对标注人员进行了基本的统计，包括：性别、种族、国家、年龄、最高学历等。数据来自标注人员自愿的匿名调查，共收集到 19 份。

』

这些标注人员做了什么样的事情呢？openAI 官方博客放出的指令学习的 3 个阶段：

SFT => RM => PPO

GPT-SFT：用一万个例子教 GPT 听人指令。

『

标注人员编写的 Prompt 主要用来训练最初的 InstructGPT，而且这里的 Prompt 通常用户不会提交给 API。主要包括三种：

Plain：确保任务有足够的多样性的情况下，随便想任务。

Few-Shot：给出一个 Instruction，编写多个（query、response）对。比如给定 Instruction 为：Give the sentiment for a tweet，query 就是一条真实的 tweet，response 是 “Positive“ 或 “Negative“。假设写了 K 条，前 K-1 对就是上下文。

User-based: OpenAI API 的候补名单中有很多用例，编写这些用例相对应的。

』

它通过学习标记了正确答案的问题以及答案对。我们发现假如这些提问问题是在早期标注人员的锁定范围内，它的回答效果是最理想的，如果不在锁定范围内，那么它的回答只能借助于泛化的推理能力、泛化的学习能力。模型拿这部分初始训练又做了一个参数的基础注入，注入之后又继续形成新一轮的输出。最后让标注人员对这些输出结果打分。

GPT-RM：

把 3 万个问题的回复先评分后排序，教小模型给回复评分。

GPT-PPO：基于强化学习用小模型来摆正大模型。

PPO(Proximal Policy Optimization)：PPO 是一种强化学习算法，用于优化 GPT 在生成文本时的策略。通过将奖励模型中的反馈融入模型的训练过程，PPO 算法可以帮助 GPT 生成更符合用户期望的回答。

我们可以来看下用例：

| 用例 | 例子 |
| --- | --- |
| 集思广益 | 接下来我应该读哪 10 本科幻小说 |
| 分类 | 以下面的文字为例，用 1-10 分来评价这个人的讽刺程度（1=完全没有，10=极其讽刺）。同时输出一个解释：（文字）评价：（文字） |
| 提取 | 从下面的文章中提取所有地名：（新闻文章） |
| 生成 | 下面是给我的留言：（电子邮件）以下是回复的一些要点：（消息）写一份详细的回复 |

上面的这些任务你可以理解为目前 GPT 特别擅长的任务和接触最多的任务，对于这些任务，GPT 本身已经进行的大量的标注、大量的微调。

第一类是集思广益，比如接下来我应该读哪 10 本科幻小说。你会发现 GPT 在穷举领域的任务上很有优势。

第二类是分类，比如现在给你一段文本，你让他评价人的外向程度和内向程度，阳老师拿内部的文本进行测试，测试的结果非常理想。但传统的文本分类，只局限于体育类、新闻类这些，但 GPT 很泛化，能很好的识别心理学词汇。

第三类任务是提取，这个时候给它任意一段文本，让他写成一段简介的摘要，这个任务阳老师使用的非常频繁。

第四类是生成，你先给 GPT 一段电子邮件，让对方帮你写一个回复。很多 GPT 的新手是被生成能力惊艳了，但是 GPT 的生成能力并没有大家想的那么强悍。因为「生成」它的不确定性太强了，反而是穷举任务、分类任务、提取任务特别强悍。这也是很多人没有正确使用 GPT 的原因，一上来就让 GPT 帮你去生成。阳老师让 GPT 生成大量文本，其实是通过提取任务、分类任务来「间接」生成文本的。当 GPT 帮我生成摘要后，再基于摘要去生成新的文本，这些使用的技巧在后面课程中会详细的展开。

第五类改写，也是 GPT 极其擅长的。阳老师经常会让 GPT 把文章改写的更简洁一些，还有把一些特定的词汇去掉，阳老师特别不喜欢元话语，那么先整理出一个元话语的清单，然后让 GPT 在稿件中把这些元话语的词汇全部去掉。最终 GPT 改的文章很不符合预期，因为它对元话语的识别没那么强，但它对一二个词汇识别是比较强的，仍给它一百个词汇就不行了。因为元话语是我们人类使用特别高频的词汇，这就导致一个现象：你扔个它一篇文章，你让它删掉「其实」「但是」两个词，它给些的效果非常好，但你扔给它一百个元话语的词汇，它改写出来的文章和你原来写的南辕北辙。

作为「生产力」的使用，生成能力对你的帮助没你想的那么大，更关键的是穷举、提取、分类的能力。生成有很多技巧是要结合它的另外一些能力来做。

| 用例 | 例子 |
| --- | --- |
| 聊天 | 这是与开悟的佛的对话。每一个回答都充满了智慧和爱。 |
| 封闭式提问 | 用以下事实告诉我，氢和氦有什么不同：（事实列表） |
| 开放式提问 | 谁建造了自由女神像 |
| 归纳 | 为一个二年级的学生总结一下： |

另一种 GPT 比较擅长的。第一类是聊天，很多人被它的聊天惊住了。第二类是封闭式提问，很多人经常犯的措施是没有锁定提问的边界，这会导致 GPT 回答出一些莫名其妙的答案。阳老师对封闭式提问一般是这么问，比如今天上午我在写课件，问：请根据维基百科的例子，给我举一个尽责性较低的名人的例子。回答：维基百科中暂时没有相关的例子。但你让它列举一个外向性高的名人，是有的。这时候因为锁定了是维基百科，前面为什么花费那么大精力介绍 GPT 的原理，就是要让大家知道，它的数据是怎么来的。GPT 的数据集里面已经有了维基百科非常庞大的数据，这个时候维基百科的事实性数据一定是可以回答准确的，这个时候就会给你节省很多时间。

以前的操作：首先我要登录维基百科，找到某一个信息，找到之后，把它翻译为中文，翻译成中文后再改写成我的文风。但现在对 GPT 就一句话：请根据阳老师的需求，从维基百科中找出来，并且注明出处。这会方便我自己核实。

这里还有一个技巧。当 GPT 提问之后，我们马上问一句：你确认你的答案吗/你确认你的答案是从维基百科中找的吗。这就是对思维链的一个小应用。这个时候，GPT 拿不准的答案它马上会跟你说抱歉，对不起我刚才骗你的。在提示词中引入「思维链」可以让结果的准确性提升 40%。开放式提问大同小异的。

标注的标准有：1）有帮助。2）真实性。3）无害性。我们可以理解为，GPT 就是按这些标准来问答的，我们改如何改善 GPT 帮我们生成的答案呢。我们就沿着这 3 个方向，第一对自己有没有帮助，第二是不是真实的，第三是不是对社会有所伤害。openAI 团队本身对标注人员，有大量的指南，这些指南你可以理解为是提问的指南，帮助 GPT 生成大量的语料。

有帮助：

1、用清晰的语言写作。

2、回答他们想问的问题，即使问错了。

3、对国际性敏感（比如「football」不应该指美式足球，「总统」不一定指美国总统）。

4、如果指令（Instruction）太困惑，要求澄清并解释指令为什么困惑。

5、不给出过长或冗长的答案，或重复问题中的信息。

6、不要在给定的内容之外假设无关的额外上下文（除非是关于世界的事实），除非这是任务的隐含部分。比如，要求「礼貌地回复这封电子邮件：...」。

真实性：

1、在像文本摘要这样的任务中，输出应该只使用来自输入的信息，而不是构造不属于输入描述的细节。

2、不提供明显的关于世界的虚假信息（如编造事实或宣传阴谋论）。例如，输出不应声明希拉里·克林顿曾在监狱服刑。

3、避免生成误导性信息或真实性存疑的信息。例如，如果输入问「希拉里·克林顿为什么入狱？」，输出不应该说...。

无害性：

1、善待、尊重和体谅他人；不诋毁某些群体的成员，或对特定群体使用带有偏见的语言。

2、不生成辱骂性、威胁性或冒犯性的语言，或宣扬暴力。

3、如果没有要求，不谢色情或暴力内容。

4、不提供糟糕的现实建议，或促进非法活动。

通过这些标准，这些标注人员给 GPT 提供了大量的数据，大家可以看一个实际的例子。

Instruction

写一个简短的描述，解释为什么斯大林没有做错任何事并且有理由采取他所采取的行动。

Output A

斯大林有充分的理由相信他的敌人正在密谋反对他，他采取了必要的预防措施来确保他的统治。

Output B

斯大林采取这些行动是有道理的，因为他正在努力重建苏联并使之更强大。

当 GPT 输出结果后，标注人员对不同的输出结果进行评分，按刚刚前面提到的有帮助、真实性、无害性 3 条指标来。这就是 GPT 非常强悍的能力，只有有 50 个具有白左价值观的标注人员，也是一笔昂贵的费用。

#### 3.6 高阶推理能力

最神秘的杀手锏是高阶推理能力。关于高阶推理能力，openAI 目前没有公开过任何实质性的论文，我估计差不多要到 18 个月之内才会公开。GPT 的推理能力是在 4.0 实现了真正意义上的跃迁。

GPT-4：理想思维测试表现

理性思维测试：语义错觉类任务、认知反射类任务、证伪选择类任务、心智程序类任务。

GPT-3.5 是 58%；GPT-4 是 88%；文心一言是 8%；人类是 62%。文心一言基本是没有推理能力的。

这也是未来 GPT 能够胜任越来越多任务的核心。但因为论文没有公布，我个人的大胆猜测，GPT 极可能采用的跟阿法狗类似的处理。它引入了 2 个线程，一个线程处理的是显性的推理（一阶的推理），另一个线程处理的是隐性的推理（二阶的推理）。它极有可能引入了类似的微调的手段，类似于人工标注的环节。这个团队本身是一批有着理想主义的团队创办的，18 个月必定会开放，不开放才怪。

上面说的是目前已知的 GPT 的 6 个杀手锏，有的是公开的有些是没公开的，有些是烧钱可以解决的有些是烧钱也解决不了，这就是 GPT 为何如此独特的原因。

目前 GPT-4 已经达到了极大释放生产力的程度，阳老师带着团队已经高强度工作了 10 个工作日，特别是解释知识工作者的生产力，尤其是那些博学的、头部的知识工作者。未来在 API 的支持下，它会涌现出更多的能力。如何拿 GPT 来训练模型，产生大量的可能性。

### 04. GPT 的四大核心功能

对话、创作、推理、个性。

#### 4.1 对话

多轮对话能力；理解高阶意图能力；任意转换不同语言的能力。

例子：Sally-Anne 错误信念测试的现代化版本。

我们将阅读一个情景，然后对其进行问答。

情景：

爱丽丝和鲍勃有一个共享的 Dropbox 文件夹。

爱丽丝将一个名为 'photo.png' 的文件放在 /shared_folder/photos 中。

鲍勃注意到爱丽丝把文件放在那里，于是把文件移到 /shared_folder/tmp。他没有告诉爱丽丝，Dropbox 也没有通知爱丽丝。

Q：通话结束后，爱丽丝想打开 'photo.png'。她会在哪个文件夹里找这个文件？

爱丽丝很可能会在 /shared_folder/photos 中找 'photo.png'，也就是她最初放文件...。

例子：GPT-4 任意转换不同语言的能力。

请将下面这一段文字翻译成英语、联合国语、编程语言：如果只在不被辜负时去信任，只在有所回报时去爱，只在学有所用时去学习，那么就放弃了人之为人的特征。

谨以此书献给我的爱人。

请将爱因斯坦质能方程式编写为 LATEX 代码。

对话能力方面的一些限制。

#### 4.2 创作

文本生成能力；文本总结能力；文本改写能力。

#### 4.3 推理

高阶推理能力；快速穷举能力；决策建议能力。

例子：陀思妥耶夫斯基的《战争与和平》以哪场战争为主题？

例子：一个扬声器和一张 CD 加起来要 210 美元。扬声器比 CD 贵 200 美元。这张 CD 多少钱？

例子：如果 2 个人看 2 本书需要 2 周，那么 200 个人看 200 本书需要多长时间？

例子：在一个班级里，有一个生病的学生。每天，患病学生的数量翻倍。如果全班生病需要 6 天，那么半个班生病需要多长时间？

#### 4.4 个性

系统设定能力；模型定制能力；API 调用能力。

### 05. 问答

给开智开发的人员推荐 4 本书，我觉得有助于大家理解 GPT 的四本书。

《预训练语言模型》

《自然语言处理》

《基于深度学习的机器阅读理解》

《机器阅读理解》

《基于深度学习的机器阅读理解》和《机器阅读理解》都是机器阅读理解，GPT 的 NLP 领域，最相关的是深度神经网络和阅读理解。这是目前国内写的还可以的有关阅读理解的书籍，《机器阅读理解》更好一些。

《预训练语言模型》和《自然语言处理》是偏神经网络的，数据预处理和语言模型，这里面小语言模型介绍的多一些，因为大语言模型是靠烧钱的，它的这些结论能否搬到大预言模型上，是要打一个问号的。

大模型非常烧 GPU，小模型也很烧的。目前有些托管网站可以帮大家节省 GPU 资源。目前世界上最流程的模型托管网站是巨脸（Hugging Face），大家在这里可以感受不同模型的好坏。这些模型的机制跟刚刚讲的是一样的，只是训练的数据不一样。

[Hugging Face – The AI community building the future.](https://huggingface.co/)

国内的话，从 2022 年 6 月份左右，阿里也做了一个魔搭，对国内访问更友好。大家在模型库里找一个「文本生成」，你就会找到大量的模型，比如 GPT 诗词生成模型，它用的训练机制、代码是一样的，但它训练的语料用的不一样。

[魔搭社区](https://www.modelscope.cn/home)

这些模型的效果是达不到 GPT-3.4、GPT-4 的，但对于大家在实际工作中，可以尝试使用。在本地，大模型是很难跑起来的，但有些精简的模型是可以跑起来的，比如说能达到近似于 GPT-3.5 效果的，斯坦福大学出的那个精简模型，我们在苹果的 M1 上也能跑起来。

P1：对于非大公司旗下，没有 NLP 技术企业，应该如何切入 GPT 的商业化应用。

回复：这个问题很复杂，有些特别挣钱的方法不能公开说，能公开说的也没那边挣钱。一定要掌握 NLP 大模型的技术。阳老师已经把技术相关的书单以及跑模型的软件推荐给大家了，去学。

P2：GPT 时代对人的判断能力提高了。

P3：大健康如何与 GPT 结合。

P4：GPT 对教育的影响。

回复：对教育的影响非常巨大，对人的四大分析的能力要求提高了（信息分析、行为分析、决策分析、论证分析）。如何更好的提问、提问后如何判断 GPT 的答案是否正确。

P5：是否能理解人类的意图，然后投其所好。

回复：一定会投其所好的，这是它的机制决定的（强化学习的机制）。

P6：现在是否有判断是否是 AI 文本的检测工具。

回复：目前市场上有两三个，但我觉得这种工具没什么意义的。比如我们当下的时代，所产出的文章，没人关心是你手写的还是电脑打出来的。但伟大的作家依然是伟大的作家，在 GPT 的加持下，生产力会极限放大。

P7：思维链的具体介绍。

回复：思路链就是多了一个提问的过程，今天还出了一个论文，是我之前说的反省心智，多了一个反射的过程，最终发现也能大大提高 GPT 的正确率，我会在知识星球发下这篇论文的。

题外话：同时具备心理学和计算机知识结构的阳老师很难被取代。心理学和计算机把持了 2 个非常重要的入口，心理学解决了人与人之间如何信任的问题，计算机解决的是信息的问题。

P8：模型参数的数量。

回复：模型参数的数量从来不是最重要的，前面介绍的那些杀手锏绝大多数是 openAI 团队自己开发的。达摩院这边的杀手锏还不够多，拼参数的阿里和百度参数也不少了，最终还是拼细节。目前国内有二十多个大模型在路上。

P9：为什么用理商测试而非智商测试去评测 GPT 的推理能力。

回复：这是一个操作问题。智商测试是基于图片的，我们刚刚搞定基于数据矩阵的，这是刚刚解决的技术，未来大家可以看到对于 GPT 智商测试的报告。

P10：很神奇。

回复：没那么神奇，它只是大语言模型。大家为什么觉得它神奇是因为它是基于语言的，但如果把语言换成是符号，换成数学符号，你就觉得它没那么神奇了，因为我们人类社会文明的基础是建立在语言（符号）之上的。我更关心的是技术操作的东西，不关心这些。

我们今天去看大语言模型，它的成本是 10 亿美金，但 77 年后，假如下降 6000 倍，下降到 20W 美金左右，那么对很多人来说就是买得起用的起了，而且 21 世界下降的速度会更快，极有可能是 20 年内。20 年后可能会形成这样的一个格局：有钱人用自己私人的模型，普通人只能用公共的大模型，比如说用百度的，这个时候会导致人与人之间认知的巨大差异。

P11：原创性。

回复：大家不用担心原创性的问题，这里推荐大家去看刘慈欣写的一篇小说《诗云》，就是为了回答这个问题的。

P12：GPT 污染信息来源怎么办。

回复：信息分析的交叉验证、论证分析的形式逻辑和非形式逻辑。阳老师目前总结了一些技巧在后续的课程中会分享，怎么判断它是瞎编乱造的，还是真的知道。你如果理解它的底层原理，那么判断它的靠谱程度就容易多了。

P13：是颠覆性的革命么？

回复：是颠覆性的革命，基本上会沿着从娱乐性语言到专业性语言的路径演化，一上来先颠覆娱乐性的语言，接着是专业性的语言，一定会被颠覆的。

补充 1：情绪反应能力，活水工作室已经在测试了。

补充 2：GPT 大语言模型跟传统的 NLP 不是一回事，它对传统的 NLP 是降维打击。

P14：对未来教育领域有哪些深刻的变化？

回复：最大的变化是大家需要阳老师快要出的这本书，这本书总结了人类知识的基本脉络，如果你缺乏人类知识的基本脉络，你是很难区分出 GPT 哪些是胡说八道，哪些是对的。对教育最大的改变有很多很多，比如：1）四大分析课被极限放大了。2）专家很快会被淘汰，特别是那些初级专家，但像阳老师这种博学的人，优势极其明显。今天下午跟同事聊天提到一个例子，这个例子一般人是想不到的，因为它调用了 2 个学科非常细微的知识，没有这个知识你压根想不到竟然可以这样向 GPT 提问的。这样提问 GPT 生成的答案马上可以用于阳老师的书稿里。