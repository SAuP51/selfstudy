## 20231015. 阳志平AI闭门分享会

今天有个知识点忘记讲啦：https://scale.com/spellbook

主要用于处理大模型的微调的。openai 的官方微调功能与界面还不够成熟。

Github 上的这几个主题也是必须关注的：

1、https://github.com/topics/llms

2、https://github.com/topics/bot

3、https://github.com/topics/ai

4、https://github.com/topics/chatgpt

5、https://github.com/topics/gpt-4

6、https://github.com/topics/chatgpt-api

还有几个相关的主题：

1、https://github.com/topics/nlp

2、https://github.com/topics/deep-learning

大家加油！

Alex Edinburgh - LLM：

以及极其推荐的资源还有 OpenAI 的 Cookbook https://cookbook.openai.com/ 新手暂时不用去碰 llama 系的微调，构建好自己的数据集，Play with GPT-3.5 的微调，然后等待 GPT-4 支持微调才是上策。

如果是极其 FOMO 想不错过开源社区的进展，可以参考 OSS Insight 这个项目。它是国产开源数据库公司 PingCAP 维护的 GitHub 数据分析项目。可以理解为基于协议自动聚合的 Awesome List。

LLM Tools https://ossinsight.io/collections/llm-tools/
ChatGPT Apps https://ossinsight.io/collections/chat-gpt-apps/
Stable Diff 生态 https://ossinsight.io/collections/stable-diffusion-ecosystem/

### 音频整理

线下同学 A：

在现代生活中，我现在可能需要处理并行的多项任务，但在这个过程中，我可能遗漏了一些重要的信息。在这样的背景下，大语言模型就能对我的会议进行总结，明确列出其中我需要关注的事项，并把这些信息发送到我的消息流中，这就是我们当前正在探索的一种功能。

我还要格外留心两类信息：一个是被忽视的信息，另一个可能是垃圾。为此，大语言模型能在公司内部协助我筛选并减少那些我无需关注的信息，它还可以整理我的邮件，避免我处理大量的冗余邮件。我们公司正在朝这个方向发展。

不过，这种应用主要是针对大公司的情况。对于个人而言，我本人在做这项工作，并且更偏向于从产品经理的角度去思考这些问题。然而对于个人来说，如果没有技术只有场景，如何发挥这个模型的功能会是一个挑战。

阳志平：

对于在大规模公司工作的同事们，这里我特别想提醒你们：在大公司中，有许多新的机会正在出现，那就是大公司希望利用 AI 提高生产力。在这种情况下，我建议大家一定要坚定地抓住这些新机会，投身新的部门，尽管这些新环境可能相比之下薪水少一些，工作强度大一些。但却仿佛可以让我们回到二十年前的互联网时代，因为它会在提升你的认知的同时，提供了宝贵的锻炼机会。

我希望你们能理解我的意思。在新的机会面前，我们都要去掌握一种全新的理念和方式，这通常需要我们花费时间和金钱去实现。对于那些依然专注于副业的人来说，这是一场很难赢的比赛，因为他们无法与那些每天都在思考，每天都在接触和处理大模型的人竞争。所以这个时候，我们尽可能地都要转向相关的产品线，大量的机会将在未来等待我们。

最后，我想为一位名叫朱建的同学做一个简单的介绍。他今天可能由于交通原因稍微迟到了。朱同学是我们的老友，他目前在中国的一个大型银行工作，同时也参加着一项国家级的标准起草工作，该标准与大模型在金融领域的应用密切相关。这对未来的我们，以及广大想了解更多这方面的朱同学们、葛曼家属们确实具有非常深远的意义。

我们将会探讨一种随着 AI 技术发展而涌现的新的机遇，而那些具备专业技能并拥有大型公司的人脉资源，将有可能把握这个机会。举例来说，朱建，他是银行业内的一位同行，却因其在大模型应用上的积极态度和进取精神，有机会参与到 AI 金融大模型的标准制定中，这种机会往往意味着能够加入到某个重要的战略决定或项目中。

值得注意的是，前二十年可以说是划时代的，每个行业都在起草属于自己的标准。那时我们为制定这些标准做出的努力，既没有额外延长工时，也没有额外的工资奖金。我们还需要在周末抽出时间来研究国内外的最新动态和发展趋势。然而，二十年后的今天，形势已然有了翻天覆地的变化。你可以将现在的情况比作互联网早起的标准制定期，诸如 TCP 标准、互联网域名管理等，这个新的标准制定期同样充满了无数机会。

在这个新的标准制定时期，主要存在两种机会。首先，这类机会适合那些已经成为某一垂直行业头部公司的人，例如朱建是银行业的领导者，同时他也负责类似计算机中心的业务。这样的场合下，他必须积极主动地参与到行业的标准制定过程中，其中就包括金融大模型、教育大模型、能源大模型、农业大模型、工业大模型、交通大模型以及医药大模型等。

如果你所在的公司正好位于这些领域的前端，并且已经形成一定的领先优势，那么你理应发挥带头作用去制定这些新的标准。参与度越高，收获的好处就越大，尤其是在认识那些在未来二十年有话语权的行业领头人，这将推动你闯入并领导新的产业力量。

制定一个国家级的标准是一个共同的行动，往往需要多家银行共同起草。起草完成后，制造商和制造业的同行也将参与其中。这样你不仅能够扩大你的人脉，还能了解比如腾讯这样的大公司在大模型领域的实际应用和进展。这无疑能够让你在自己所在的公司或行业中，早早地拥有先进的生产力。

大家明白我的意思吗？所谓的第二类学员，主要是在科技公司工作的同仁。例如在腾讯或者美团工作的朋友们，你们需要积极地参与相关标准的制定，一个方面是为了配合垂直行业的管理机构。比如美团可能需要面对的是旅游教育、餐饮服务等领域的行业规定，以及与大模型的标准规范有关的制定。有一部分内容可能最典型的是类似于咨询服务这方面的工作，还有就是短视频制作方面的努力。这有需要各行各业也就是说，大家在面对各种具体的需求时，都需要积极贡献你们的经验和创新。所以，大家一定要积极参与进去。

除此之外，科技公司中还有一类非常特殊的员工，他们可能并非科技专业或计算机科学出身，但却是同属我们这个学习群体的一部分。他们可能已经是顶级的大模型领域的从业者，但也有大量的同学实际上并不是算法专业出身，例如产品经理，运营人员，管理人员等。这一部分人员虽然非科技专业，但他们在大模型的评测和标准制定中，同样能够从中获得相应的权益和利益。

我们所说的红利，是指参与评测工作所带来的经济和情感满足。这不仅需要算法专业的知识，更需要深厚的产品理解和行业经验。这一部分工作，往往是产品经理，运营和管理人员所擅长的。在这个过程中，可能会有大量的政治斗争，比如一个新的可行性标准最终会被行业内的多个竞标者胜出。

目前在国内，最主流的标准评价体系通常来自中国计算机学会，今年刚刚推出的新的评价标准正在被广泛执行。另外，在国际上，最主流的可能就是所说的 HUUM 标准。这两个都是当前相对主流的评价和标准制定机构。

但是大家会发现，这两个主流的标准评价体系，对于大量的垂直行业和科技公司来说，其实不尽是通用的，也就是说它并不完全适用于所有的环境和需求。所以，究竟如何能更好地服务于具体的业务领域，可能还需要我们各行各业的同仁们，共同在实践中探索和创新。

大家是否理解了呢？以金融领域为例，我们非常关心的是合规问题。那么如何确保我们的大型模型，无论是运用于智能客服，还是在其他方面，都能处理好这个合规问题呢？同样，对公安和法务领域而言，我们最关心的则是隐私问题。所有这些我们都是以大型模型为主进行处理。基本上，大多数大型模型是不允许接入外部网络的，军事安全领域也是如此。

因此，你会发现，不同行业的业务特点决定了其标准和方向有着显著的差异。这其中蕴含了一个很好的机会，即大型模型还很新，没有人敢称自己是专家。因为这个新生事物，即便从它诞生、发源到现在，也不过半年左右的时间。你只需要稍微花点时间，多写写、多做点研究，你就很容易成为这个领域的专家。这是我特别想要提醒大家的。

我看到显然有许多问题大家想要问我，但我刚才并没有提及，所以后面的问答环节我会统一回答。刚才看到有位线上的同学提出了一个问题，我们会优先回答，以确保在线或线下的同学都能获得统一的体验。由于时间关系，我现在无法阅读阳老师的信息，我们稍后再讨论。

那么，让我们正式开始吧。现在已经是十点二十一，我会给大家共享屏幕，看大家是否能看到？我现在正在分享的是一个偏实战的话题，并将进行大量的演示。

正文：

我们今天的主题是「知识工作者的生产工作流」。在我们这个论述中，「知识工作者」是指那些从事脑力劳动的职业，例如程序员、律师、医生、教师，和科研人员等，这些都是最典型的知识工作者。




那么，在这个话题中我们关注的重点是什么呢？在当前的 AI 时代，我观察到有两类极端情况。第一类情况是，许多同学在最初接触 AI 时特别兴奋，但到了现在，基本上已经落后了。他们在日常生活中，很多东西还是停留在 AI 出现之前的状态，这类同学占了大多数。而另一类同学则是投入了大量的时间在 AI 这部分知识的学习上。

在原来的讲座中，我表示许多人在重点事情上没有投入足够的时间，反而分散精力于许多并不那么重要的事情上。例如，使用大量并不重要的软件，或者沉浸在不重要的思考中。这个问题，我在今天的分享中，尝试去为这两类同学找到解决方式。

我将在今天的分享中从知识工作的角度，探讨我们在面对 AI 时能够怎么做。我们对 AI 的理解，既不能像小白那样只读表面，也不能像一些同学那样投入大量的时间却没抓住重点。我们需要清楚，AI 时代要提升我们的生产力，几件关键的事情就能够帮到我们。因此，今天的分享不会像以前那样涉及太多内容，更多的是深入一两个主题，和大家深度探讨。

对于这个主题，大家可以回头参考我们的 AI 思课，或者参加我们的 GPT 课程，获取更为完整的知识。有了这些背景，我想对大家解释一下当前的大语言模型的市场格局。从三月十五号以来，我们步入了一个百花齐放的大模型时代。至今，市场格局已经基本明朗，主要由三大公司的模型在市场上占据主导地位。

第一个值得关注的是 open AI 公司，它代表着平台 AI 的发展潮流。其次是以图片为主导的公司，这个公司目前年营收已经达到十亿美金以上。第三是一家主要做数据标注的公司，其营收也已经达到几个亿美金。

如果我们去评价一个大语言模型的价值，主要有两个标准。首先，它是否代表了业界最先进的技术，以及最顶尖的生产力。其次，从市场上看，这个模型是否真正实现了利润的收益。

面对这些大模型，许多人对其有误解，他们过于投入在研究多种不重要的模型上，却忽略了真正应该了解的重要模型。这里我要理清一点，AI 并非我们在互联网时代，移动互联网时代的熟悉体验，我们不能如同对待互联网那般对待 AI，我们需要有新的理解和应对方式。

在我们生活的这个移动互联网时代，使用 Google 或百度作为搜索引擎，或者是使用快手和抖音去观看短视频，这两者之间可能存在着较大的差距。然而这种差距并不足以形成对用户体验的决定性影响。当你使用百度搜索信息时，你依然可以找到需要的内容，只是可能在搜索速度上，无法与谷歌相比。同样，通过快手观看短视频，也许无法找到抖音上那样精致和符合一线城市口碑的作品，这些都不构成决定性因素。

然而，在人工智能中，一个突出的现象是，大模型的生产力差异，就如同两岁小孩和大学教授之间的生产力差异。所以，大量的模型其实都只停留在两三岁小孩到十八岁中小学生的水平。扪心自问，我们是否明白了这个道理？

因此，我们可能在花费大量时间研究那些比我们先进的模型方面，不论是注册账号或者研究模型，其实并没有什么实际意义。这是因为它们普遍还没有达到，并未足够发展到像大学毕业或者说教授水平。所以，你在他们身上投入的时间越多，实际上越是不值得。

在这里，还有一个普遍存在的误区。许多开发者会马上就拿起国内的大模型，或者借用王小圈的一些大模型去进行应用开发，甚至直接做成立项项目。这种逻辑错误你知道吗？其实你是让一个小学生去满足成年人的需求，自然效果不尽如人意。

所以，我们是不是应该耐心等待，千万别急躁。国内的公司迟早能追上的，但追赶的过程需要漫长的时间，没有像他们吹嘘的那样快速，可能是在十八个月，或者三年以后的事。

因此，我们现在所有的研究力量都应该集中在这三年的时间里。只需关注 AI、文本生产力、图像生产力、数据生产力这三个方面就够了。如果你这三个都无法弄明白，无法深入研究到优化的地步，那么去研究其他的大模型只是在浪费时间，没有丝毫意义。

接下来，让我们试着看一下，国内目前有哪些扮演着 AI、文本生产力、图像生产力、数据生产力这三个领域的替代品和对标产品。

在国内现在有三家知名的人工智能公司，其中一家独树一帜，特性独特，在图像制作方面的技术前沿超过了其他公司，即使与全球范围的同类公司对比，叫难找到与之匹敌的。遗憾的是，国内几乎没有公司能够与之对标，达到与其同等水平。因此，如果你关注的是制图能力，那么这家就是你首选的。国内其他从事制图的能力的公司相比之下，差距巨大，甚至可以说相距十万八千里。

然后还有两家公司，他们在研发领域中的水平，已经有一定实力，可以接近跟全球的前沿水平，它们最值得我们关注。这两家公司分别是上海的自动化公司和腾讯投资的一家公司，它们也是在估值上达到了与国际一线公司相当的地步，大约是其十分之一左右的水平。然而，它们的研发能力却并不逊色，甚至可以说是国内最强的。相比之下，国内的其他公司，倒没有太多关注的必要，由于它们的研发能力比较弱，关注它们也是在浪费时间。

这两家公司在大模型领域的研发已经有一段时间了，比如说智普公司，他们在前年 3 月 15 号之前就开始研发大模型了，而且他们一直坚持到现在。智普公司的创始人是清华大学的计算机学系教授团杰老师，早期他的主要研究方向是网络设备分析，正因为这一原因，他用这个角度来进行大模型的研发和建设，已经有接近 15 年的经验了。他所倡导的理念并非是随大流，而是有自己独特的思维方式。他们开发的大模型，不论是开源版本还是商业版本，都与 openAI 这样的机构有着独特的思路，这套模型已经全线开源。

总的来讲，智普公司是目前国内低调而神秘的一家高科技公司。他们并没有透露过太多的动态，因为他们的重心并不在于技术，他们更看重商务开发方面的突破。他们仿照「商汤」公司的成功模式，尽管他们在早期的商汤项目中吃了一些亏，但是他们在 GPU 资源上投入了巨大的资金，为了追赶世界潮流，他们在上海建了一座全国最大的 GPU 机房，从而在 GPU 训练方面取得了显著的成果。

这是关于一类结合视频与文本的技术，以及该领域中表现优秀的公司的讨论。商汤公司在这方面已经做得非常好，他们凭借过去多年的积累，且有幸得到了优质股东的支持，如网易游戏和腾讯等，所以在游戏行业的表现也非常优秀。因此，可以说商汤是目前这个领域中最低调、最高效、发展最快且估值最高的公司。

当然，这并不意味着其他公司不值得关注，但是通常情况下，媒体报道的越多的公司，往往会抢走大家的注意力。如果你能成功研究并理解这些核心公司的 API，你就已经理解了这个领域的核心了。

接下来，我想介绍一个人，他的名字叫梁斌。他经常在微博上分享他的工作和生活，我曾在信息分析课上提到过他。他为一个专注数据标注工程的项目工作，这个项目是他的致富之路。在大型语言模型兴起之前，他一直在做网络爬虫的工作。而这次大语言模型的兴起，相当于直接给了他财富。目前国内有一百多家大型语言模型公司，其中有五十到七十家公司从他那里购买数据。据我估计，最近应该有一个大厂，可能是字节跳动、百度、美团或是一家联合投资机构，准备投资一亿给他。因此，他终于实现了财富的突破，这也是他在微博上分享了十多年后的成果。

梁斌的成功法则很值得借鉴，他并不关注国内的数据，因为国内的数据存在数据版权的风险。他比较倾向于模仿 Open AI 的方式，全面抓取全世界的数据，尤其是美国的数据。无论是电子书还是网络政策文本，他几乎全都包含了。据我了解，他已经掌握了大约一千七百亿的电子书的数据，还有大量的其他材料。因此，他在数据领域的潜力很大，也具有很强的技术实力，并且与各大公司的 EO 保持着良好的关系。因此，如果有在南京工作的同学或者对爬虫技术感兴趣的同学，我推荐你们向他的公司投递简历。

最后，我想让大家简要感受一下各个公司在这个领域中的格局，Open AI 就不多说了。无论是商汤、梁斌的公司，还是其他的公司，他们都在努力推动这个领域的发展，同时也展现了自己独特的价值。

我们首先要关注的是一些可能还不太为人所知的公司，比如 Open AI 就已经被讨论的广泛了。这些公司的核心竞争力在于他们掌握的强大算力，比如有一家公司用的是上海的 GPU 数据中心支持，因此他们轻易的就拥有了上万卡的算力。在文本处理一块，这家公司已经做得非常出色，尽管在一些细节处理上可能还稍逊一筹。但是他们在未来的英语和视频处理上的竞争力将会非常强大，令人不容忽视。这要归功于他们强大的算力。这家公司的总部设在上海。

下一家我们要看的公司，他们提供了 API 调用的服务。我们需要投入时间去理解这些最有可能在未来崭露头角的公司，并深入研究他们的 API 等技术，而不是把时间浪费在一些无关紧要的事情上。

接下来我们要看的公司叫做质朴或智普，也被称为认知智能。这家公司的项目在目前可能可以说是最好的。我们的 AI 编程课本会以此公司的 API 为示例，因为如果要申请 Open AI 的 API，需要翻越的壁垒较大，因此，作为教学示例可能不太理想。因此，我们选择以认知智能的 API 作为教学示例。

特别需要提醒大家的是，彭杰老师开发的一个信息分析课程，这是我们的重点教学内容。因为黄杰老师是从事计算机科学的，他在课程中整理了大量的 AI 相关的关键词，你可以跟踪这些关键词。这个课程不仅包含了自动收集的信息，还有手工整理的部分。所以，这个部分强烈建议大家重点学习，比其他几个课程更有价值。

另一个值得一提的是李斌老师。李斌老师是一位来自贫困背景，却逐渐自我提升的励志故事的主角。他于 1997 年出生，一度在南京大学学习，最终成为了清华大学计算机专业的学生。

AI 并非易事，却有许多公司在其中展现出无与伦比的智慧和创新，举例来说，某种程度上我们可以将其视为一种 "闷声发财" 的模式。然而，最近就有一位名为梁斌的企业家过于高调，炫耀自己的成果，出现了疏忽，我有预感这可能会在今后给他带来不利影响。

这位梁斌所在的公司十分低调，甚至没有官方网站，我仅能找到他个人的网站。他的公司总部位于南京，然而在注册时选择的是北京。他曾在社交平台上公开了他在南京拥有一栋顶级写字楼，然而该楼内空无一人。他的公司主要从事网络爬虫相关业务。

事实上，网络爬虫业务涉及到法律风险，而梁斌则是如何规避这风险的呢？实际上，他运用了一种独特的策略，那就是参与了国家标准的起草。我们刚讲到，国内目前有一项标准涉及大型模型和数据抓取，而梁斌则成为了标准的起草人之一。这就是他如何利用政治资本来保护自己的方式，尤其是在涉及大模型的场景中，他们几乎成了所有大厂的数据供应商。

然而，梁斌在他的招聘信息中像是在秀自己的成绩，他详细列出了他的公司半年的营业额达到了两千万元，全年可能超过五千万元。据他所说，他的公司在中国大模型市场的份额超过了 50%，这无疑是很高的成绩。然而过于高调的展现可能会让人感觉他过于自大。

接下来，我们再来看一下他的公司对于新员工的需求。他们对于具有博士学位的算法工程师有很高的需求，硕士学历是他们的最低要求。对于那些大学生和家中有学者成员的人，这无疑是个很好的工作机会。对于有丰富经验的人，他们提供了技术总监这样的职位。对于那些能和知名高校联合培养的研究生，他们提供了博士后这样的职位。

首先，对于在场的所有人来说，大部分同学可能无法获得我们正在讨论的这种工作机会。那么，什么才是合适大家的工作机会呢？有一个决定性因素是工作氛围和角色 —— 工程师这样的角色。在这方面，那些在开智接受过训练的同学无疑已经处在领先的位置。因此，这种机会对大部分人来说其实是可以获取的。

那么，我们可以来看看这个职能的需求。首先，我们注意到一个叫做微调的工程师。然后，还有一种职位，或者我们可以叫它解决方案专家，这也可以看作是一种高级销售职位。在大模型时代，积极的销售活动是极其必要的。简单来说，这涉及到如何将你自己的大模型推销给一种叫做到企业（to b）的机构。实际上，这类职位的招聘门槛是相对较低的，大家是完全有可能转身驰骋在这个领域的。

此外，我们也看到了大模型工程师的需求。希望同学们都对这份信息有所理解和掌握。

进一步来看，让我们关注一下王小圈老师的情况。你们可以看一下王小圈的需求部分，目前看起来，他的需求相对稳定且已经成熟。你会发现他的招聘需求主要集中在哪儿呢？是数据质检。就是要求数据要有高质量。这一部分的问题尚未解决。而数据工程师的部分，关于数据资产的问题也尚未解决。尽管是这样，但是这种需求通常出现在 IP 领域，与数据仓库的管理能力有关。

通常，我们需要管理大量的数据，例如，1700 亿的电子数据，那么我们应该如何管理呢？在这个时刻，我们需要的其实是一种专用的技能。比如南孚这个数据库，目前是业界使用最多的数据库。我希望大家都能理解这个观点。

所以，这一部分才是真正适合大家转型发展的领域。这类岗位同样也是可以考虑的。但要注意，这类工作职位的要求也是相当高的。

南方设施，实际上王小圈老师招聘的一些职位，比如前面提到的高级销售，主要还是需要解决方案的能力。事实上，所有这类职位都要求高级的销售技能。

同时，举个例子，我们来看一下金融圈的同学们。他们更应该关注的是什么呢？让我们继续探讨。

是的，接下来，我要向大家介绍泛方科技的招聘需求。泛方科技是中国排名前四的量化基金之一。这意味着该公司的资金实力比起一般的 IT 公司将更为强大。泛方科技的资金来源是股市，他们甚至拥有价值一万块的 GPU 资源。

泛方科技有一个招聘岗位非常吸引人，尽管他们没有明确指出。这个职位非常适合我们这里的一部分同学，特别是那些对各个行业知识储备丰富、有阅读《阳光下的透视镜》等书的人。泛方科技希望这个职位的员工能为他们收集不同行业中的优质语料。

现在我要提醒大家的是，这个职位是在泛方科技的杭州总部招聘的，他们在北京还有一家分公司。很多人往往容易忽略这样的岗位，所以，我想告诉大家，千万不要低估你在 AI 时代的可能性，否则会错过大量机会。

接下来，我要讲的是竞品。竞品太多了，每天都有新的产品出现。所以我们要做的是跟踪最新的 AI 资讯。其中，国内的 AI 资讯，大家只需要关注「36 氪」和「虎嗅」就可以了。这两个平台都是国内做得最好的 IT 资讯平台。

然而，这两个平台之间有个很大的差别。「36 氪」比较注重信息的速度，而不太强调价值观。而「虎嗅」恰好相反，它非常注重价值观，以及与读者的共情，会讲许多深度的故事和行业八卦。不过，过多的共情有时会浪费我们的时间。

其中，「36 氪」的资讯源主要是采用大模型处理。我们一般会按照发布时间顺序去跟踪。至于国外的 IT 资讯，我们一般通过「GitHub」这个平台去跟踪。因为目前，世界上大部分的大语言模型都托管在 GitHub 上。

现在，我将演示如何在这些平台上跟踪最新资讯，并提供一些注意事项和技巧。「36 氪」的网站和 APP 都是好的选择，我们可以根据自己的喜好来选择使用。

首先，我要说明的是，定义一个关键词就能概括所谓的 "大模型"。你可以将其看作是具备了所有顶尖信息技术技能的巨星。然而，我要提醒大家，不要被一些吸引眼球的标题所诱惑，无谓地将注意力投向那些会极度浪费你时间的信息。实际上，你只需通过下载相关应用并设定关键词就能满足需求。

在我们的研究中，我们并不重视那些稍显滞后的国内资讯。我们真正应关注的重点在于如何获取帮助以及其他重要事务。那么该如何获取帮助呢？你们可以看一下。

这里有一个关键的小技巧。我们通过 GPP 搜索得出的信息量通常很大。例如，我记得在五月份的总结中，我似乎找到了两万条信息。但现在，如你们所见，我们有了十万个项目。目前的情况是，在这个领域，我们必须理清真正有价值的东西是什么。真正有价值的不是信息量，而是开发者的品位，他们的品位远胜于我们。因为他们具备识别那些无关紧要的信息的能力，这就像识别出了某些人像王小轩这样，可能只是购买了一些马甲账号而已，这对我们毫无意义。唯有开发者的眼光才能说明实质性问题。

举个例子，我们在评价一本书时，它的销量对我们毫无意义。但是，如果一本书被列为学术界核心青年参考文献，那就十分有意义。在程序开发领域，同样的参考文献就是这些项目，大家明白了吗？因此，我们可以尝试使用这个小技巧：选择一下某个参数，例如，50 到 100 个 focus，或者大于 500 个 focal。大于 500 个 focal 的项目，基本都是顶级项目。你们看到了吗？这样一来，项目数量就直接减少到了 107 个。因此，你只需要深入理解这 107 个项目就足够了，不要被那十万个项目误导。

其中的十万个项目，有很多是垃圾信息，是营销手段，是忽悠人的。甚至很多还是一些大公司的工程师编造出来，以忽悠老板，作为绩效考核的项目，我们将其称为「绩效评估项目」。

过一段时间后，这些项目总有一天无人维护，理解我的意思吗？只有那些顶级开发者们围绕着风口关注的项目，比如项目规模在五百以上的这种，我们才会去积极引用和关注。当我看到一个项目可以有效提升我的工作效率时，我便会选择调用它。这种有益的共享项目，就具有了极高的意义。你们都明白了吗？

好的话，那我推荐大家可以稍微了解一下，也就是对这些项目产生兴趣，并将其标记下来。标记下来之后，就可以明确我们关注的重点了。此外，还有一种稍微重要一点的方法，就是咨询师的建议和指导。

另外要强调的是，我们还可以通过甄选，比如借助左侧一些工具，像是角辩语言等等，来筛选更多的有用项目。一大部分项目是针对 GS 和 TS 这样的前端项目。例如，如果你想要改善自己使用 GPT 等这类人机交互区的效率。只需要单独研究这个领域就够了。我们看一下，所有这些趣味项目，都是以人机交互为基础的。你们理解了吗？

但在实际情况下，我们提及我们的生产效率，仅仅依靠人机交互是远远不够的。我们需要深入到一些底层问题上去。这些底层问题，往往是和沟语言和 Rush 语言等有关系。比如，我们可以运用沟语言进行筛选。

大家可以多关注一下这类项目，尤其是我强烈推荐的一些项目。例如这一个，由 Open i 事务所发布的项目。虽然该项目一般，但曾经也产生过一定的影响。其主要是集结了当前十几个大型模型 API，并将它们打包到一个地方，容易调用和使用。

这个项目的主要思路是什么呢？它的思路是一种脱离我们常规思维的方式。有的学生看了这个项目后，往往会立刻想去实行。然而，这个项目本质上只解决了一个问题，即将十几个大型模型集成在一起。

这时候，大家要对开源界的项目有一个清晰的认识：并不是所有的项目都完美地提供应用方案，而是专注于某个小领域，并在这个领域中做到最极致。

本篇的核心话题围绕着大模型、人机加速、开源项目以及运用开源项目的相关问题展开。通过对所谓「一锅炖」的解决方案进行探索，我们寻找由多个大型模型联手解决问题的可能性。在这个过程中，一个重要的考虑因素是如何以更用户友好的方式利用人机协作以进行加速。

关于开源的讨论，在目前的计算机科技界已经非常丰富了。开源的理念并不仅仅是在商业软件的购买和使用上提供替代方案。反而，开源方案旨在通过解决特定的需求和问题。因此，开源项目不是全能解决方案，而是为了解决某个特定需求而生的工具。当一个开源项目美妙地解决一个问题，然后另一个开源项目又美妙地解决另一个问题时，这两个开源项目就会在相互影响之间变得更有影响力，并可能进行一些协作，这就是开源世界的生态。

因此，我们需要清楚明白，在使用开源项目时，我们往往需要多个项目的联合作用，而非仅依赖一个项目来解决问题。例如，如果你查看某项技术的相关项目，你就会发现这个技术目前支持的项目非常多，包括国内外的开源项目，甚至还包括第三方代理等等，而且部署也特别简单，能支持各种各样的方式。

这就导致我们可以看到这些项目的互相关联。三个具有特定功能的项目可以形成一个独特，新颖的群体。理论上讲，在这个过程中，所有问题都能得以解决。归根结底，通过大量密集的努力，开源界已经基本解决了我们能想到的所有需求。

再次强调，使用开源项目不仅仅是使用单个项目，而是需要利用一系列的项目进行组合，才能解决我们的问题。比如部署到第三方平台，最推荐的是第一个平台，这个平台是目前做的最好的，服务器放在新加坡，使用一种我们不怎么说的科学算法。

随后的内容会更多的涉及到这个大的思路，以及第三方平台的利用等题目。同时，对于某个国外电视节目的探究，其中一个重点是看电视哪几个栏目，趋势这个栏目吸引了很多的注意力。此外，还会发现它分为三个核心的表格：模型、数据企业和空间。在这三个表格中，模型这个表格是最重要的。

总的来说，本讲座致力于揭示大模型和开源项目在解决问题方面的无限可能性，以及如何有效地利用这些工具以满足我们的需求。只有正确理解和应用这些开源工具，我们才能完全利用这些工具的潜力，以实现我们的目标。

在这个表格中，我们会发现，你所看到的都是最新的报道。例如，最近跃居首位的 GPT 模型被誉为 "强大"，所有这些最新的模型和发展趋势都在这里一一呈现，可以清晰地在这个清单上看到，因此你没有必要花费大量时间去查询。通常来说，模型的参数越大，得到更多关注，被标注和下载的人数也越多的模型，其效果往往更好。这个模型现在有一个特殊的功能，即可以在线测试，直接测试这个模型。大家可以直接在这个界面进行测试，点击测试按钮即可。这就相当于在我们判断一个模型是否更能满足自己的需求时，我们可以先在这里进行测试。这是模型社区的优秀之处。在国内，阿里巴巴的模型社区被模仿，但因为国内的竞争非常激烈，阿里的模型社区没有得到其他大厂的认同。因此，国内的模型社区对我们来说意义不大，阿里的模型社区基本上都是自家的大模型，很少有新的模型，比如来自百度的大模型。所以我们只需要关注这个模型社区就足够了。国内的小公司也将模型授权在这里，因此，只需要看这一个模型社区就够了。

但是，对我们来说，除了那些专注于算法的同学关心模型之外，我们绝大多数的同学其实更关心的是数据集。杨教授基本上将大量的数据集都下载了下来。数据集解决了什么问题？例如，在数学中，我们需要看一下这个数据集解决了哪些问题？有了提示词之后，我们就能想到，大模型最能提高我们的生产力就在于数据集的部分。我会在后面讲述为什么数据集如此重要。因为作为知识的生产者，我们自己都积累了大量的领域数据。但是如果这些数据按照什么样的格式，什么类型的级别进行整理，你可能并不知道。但是当我们下载这些数据，并按照预定的格式，将其整合在一起，我们就可以迅速掌握其用法。

例如，在微调大模型的时候，我们需要准备大量的输入数据，但是这些输入数据如何编写才是最合理的呢？这些细节你可能并不清楚。但是只要我们有了有效的数据集，我们就可以根据这些数据来微调我们的大模型，从而提高我们的生产力。

然而，当你浏览这种大量人关注的数据集，你会迅速理解他们的编写方式。我希望大家能明白我的意思。例如，我主要下载了几个数据集，我是否之前跟大家提过这一点？我当时把这个网站的所有数据集全都下载，随后还有其他关联数据也一并下载，最后将这些数据集合并在一起，形成了一个包含二百五十万条提示词的大数据集。在去重后，这个数据集依然拥有二百五十万条数据，这就是我们最终构建了 Outapp 框架基础。我记得是在三月份，由于主要是在观看电视，现在的数据集比这个更多，我还没有完全看完这些数据集。比如某个数据集，我还没来得及看，感觉好像是刚上传的。看看这类数据集，你就知道这个提示词就是它了。所以这对我们的工作是非常有帮助的。你需要能够判断哪些提示写得好，哪些提示写得不好，这是我们每个人都需要拥有的判断能力。

好的，大家理解我现在说的吗？那时我还讲说，下载这个数据集其实非常简单，非常简常，就像下面这样。你看，下载数据集和使用 get help 的方法完全一样，我来给大家演示一下怎么下载这份数据的，它和 get help 的使用方法是一模一样的。你看这个网址，其中一个就是 get，您只需点击这个按钮，就能将这个数据集下载下来，所以这就是它的逻辑，这个逻辑其实是完全一样的。好了，很简单，我们就不等它慢慢下载了，因为可能需要相应的权限。所以我们大概已经明白了。

好，那么我们接着讨论回到这个主题，返回到这个领域，其实这部分相对简单，你可以将其理解为在大模型领域，最好的开源组织就可以了，它比我们臆测的要好得多。可以说全球最好的公司，最好的组织，基本上都在这个领域。你会看到，这个就是我们刚刚讨论过的，做的最好的，就是做图片的这几个。我们再来看看这几个，这个是当前信息的主流。所以我们一查看就清楚了，大家就会明白，这是做的最好的 AI 公司，最好的商业实践。其实没有必要耗费太多时间，因为他们的项目就都集中在这个领域。好了，我们继续向前发展。

首先，这个网站具有一项核心功能：它的博客。至于我的头像，由于某种原因，当我在十个月前的 19 点尝试更改时却无法实现。在此，我特别提醒大家注意这一点，那些大型公司，如这三家巨头，他们的博客写得都非常棒。他们已经为我们解决了大量技术问题，我们一定要把这些技术文档仔细查阅。这些公司解决了我们的大量问题，并提供了更加简洁的实现某些任务的方法，例如如何做脱敏处理，如何进行模型评估等问题，他们实际上已经帮助我们解决了许多问题，我们没必要从零开始。这是我特别想提醒大家的。

现在，接下来我们再看看「最佳生态」的问题。当前，Open AI 的发展速度非常快。除了 Open AI，他们还在努力建设自己的生态系统，但是他们的生态系统建设得并不是很好，可以说他们一直在追赶竞争对手。我预计再过三个月他们在生态构建方面才会有所起色。现在，大量的开发者并没有使用 Open AI 提供的服务。在生态搭建方面，开源社区做得更好。

当前，生态态势的变化有三个典型趋势：一个是我们刚才讲的可能是围绕应用的相关领域；第二个是把 Open AI 的 API 能力迁移到本地；第三个是在军事、数据安全这些重要领域出现了完全本地化、保障隐私的大型模型。这三个项目可以提高应用性，我们后续会反复讨论。针对前两个趋势的讨论，即 Open AI 的 API 能力，其中最核心的就是代码解释器。代码解释器可以处理比较长的文本和更多的数据。然而，Open AI 的代码解释器存在一个问题，就是无法和本地的工作流程，以及我们的项目相应地进行配合。

阳老师极度推荐的是这个项目，它近期是最受瞩目的一个项目。仔细看看这个项目，你会发现其涨势迅猛，近两三个月更是崭露头角。这个项目实则相对简单，一旦我们安装好之后，就获得了一种本地化的能力。你可以清晰地看到，它具备这种本地能力。

项目默认使用的是 GPT-4 模型，当然，我们也可以选择使用其他大型模型，本地化的能力依然保持。项目的应用方式还有许多，如果你对此感兴趣，可以继续进行深度研究。我强烈推荐这个项目，因为它的理念甚至比许多代理项目还要正确。

在这里，我要指出一个问题，就是过多的代理项目实际上将大量能力封装进了黑箱项目，这类操作对于我们人类来说相当无法控制。我们不知道这个代理最终会带来何种后果，对于我们知识工作者的生产习惯来说，其关联关系并不大。

然而，就像代码解释器，这种能力可能是最强大的，没有任何能力可以超越代码解释器的这种能力。它能够将大量本地工作流整合到一起。所以，这个项目受到广泛关注的原因也在这里，对此大家应该已经非常清楚了。

好的，接下来，我要为大家介绍第二个项目。像我刚才提到的，这两个项目都需要满足本地隐私法或者军事安全法律，所以是没有办法将大量的文本上传到 GPT 的服务器的。这两个项目都是在解决这个问题。

第二个项目是以 facebook 的开源项目为基础的，是第二代开源项目。这个项目的支持度要更高，其发展速度也更快。我在这里要特别提醒大家，这些项目其实都在解决同一个问题：如何在本地，如苹果笔记本上，运行一个本地化的模型。

在这次讲座中，我们主要讨论了在进行机器学习和深度学习开发时，如何利用开源项目和工具，通过充分发挥其现有功能，以创新的方式解决实际问题。

假设我们正在利用本地数据资源，例如自己的博客或自己创建的语料库，进行一次自我表达的实践。在这个环境中，我们有多种方式来完成前端的展示。对此，我们可以与之前讲述的几个项目相结合。我们这次的开源界之旅，实际上是一个创新的过程。我们所做的，并非从零开始，也并非全然开发新的项目，而是利用现有的开源项目，通过巧妙地组合与搭配，解决各种不同的问题。

例如，我们可以将四散在各处的开源项目汇集起来，通过前端技术进行有机的组合。其中一个项目，是一个开源电视中的模块，这个模块的主要功能是进一步增强本地 AI 的效果。此外，还有类似 Facebook 的项目，也可以借助其功能，实现更为逼真的接口效果。还有一个非常关键的项目，其主要功能是解决授权问题。

授权问题是很重要的一类问题。在 AP 的功能介绍中，我已经向大家提到过。其主要是为了确保我们的工作流程更为安全和有效。这就好像是我们所构建的一个最佳工作模型。在这个模型中，我们将一系列的项目和技术有机地组合在一起，形成了一套功能齐全，重视用户隐私的开发模型。

在这套模型中，有些项目的功能是解决如军事法务、政府工作等需遵守严格保密政策的文档不允许上传到互联网的问题。因此，这套模型能够在不需要与互联网进行任何交互的情况下，在局域网环境中完整地运行一整套的工作流，其中包括 AI 处理、代理服务器、授权管理、前端展示等等。

你们应该已经听懂了，这就是我们所推荐的，最佳的工作流程和生态模型。其实，与其在其它地方搜寻解决方案，不如将精力集中在这套工作流程上，因为这才是最重要的部分。

现在的程序开发界被大量新项目所涌现，这些项目或许充满了活力，但也充满了挑战。因此，经过大约 6 个月的摸索与实践，我们已经对开源界有了更为深入的了解。在过程中，有许多项目因为各种原因而被此次潮流所淘汰。或许，最初的开发者将项目遗弃；或者，项目本身设计不够成熟，无法满足市场需求；而有些项目，则由于它们逻辑简单明了，代码质量高，用户界面精致，以及配备了完整的技术文档，因此能够在竞争中脱颖而出。

对此，我想强调的一点是，成功的项目并不一定来自于知名公司或著名人士，而更多的是依赖于实际效果和应用价值。希望大家明白这一点。

在前几个月，我们一直致力于某个特定项目，如今，其发展格局几乎已经明朗化，从三月十五日以来，已经过去了半年的时间。我们对这些项目的理解和认识已经相当深刻，至少对于最理想的项目组合已经有了明确的把握。

要谈论这些项目，不得不提到 "Lock AI"，这并不只是一个特定的大语言模型，而是一个平台，它可以对接各种不同的大语言模型，甚至包括那些由中国提供的如 "汉字谱" 等模型，或者是由斯坦福调适的模型等等。不过要注意，这种模型的运行必须依赖于本地设施，因此不同的设置可能会带来不同的效果。

在这段时间，我想和大家分享的内容，显然会与六个月之前有所不同，可能会更详尽，也可能观点会有所不同，假如我讲话速度过快，那么请随时提醒，我可适当放慢。

下面，我想对一个关键问题进行探讨，那就是关于计价工作流的问题，怎样才能构建出一个最佳的工作流程呢？

首先，我想强调的是 "提示词工程"。我们必须明确，无论是 Open AI 的官方团队，还是我们的团队，对大语言模型的潜力挖掘都还不够充分。不同的提示词，可能会引导出完全不同的结果。所以，我们第一步，就应该着手解决明白提示词工程，为特定的需求进行充分的测试。

我们不能一开始就困扰于如何选择最强的 GP4 模型，或者是乱七八糟的大模型。相反，我们的出发点应该是，如何根据特定需求调试出最理想的提示词。然后再考虑如何在其他大模型上产生优异的效果。

因此，我们一定要把重点放在 "提示词工程" 上，并要明白，即使是 Open AI 的开发者，也没有唯一正确的写作方式。实际上，提示词的拟定，甚至可以看作一种随机组合。同样的一段表达，稍作修改，可能就会得到完全不同的结果和质量。这便是我在这部分特别要强调对大家的提醒。

尽管如此，今天的分享，我并不打算过多的讲述关于 "提示词工程" 的内容。

讲座中，主题围绕了 GPT 和我们的 AI 系统的提示词的相关影响，然而这并非是本次讲座的主旨。我们今天的焦点更多地放在如何将各个流程进行有效的组合。关于提示词，比如我们所使用的四下掐提示词，我可以在这里可以为大家做个演示。这个提示词是我们团队的成员在使用的，是四赛香火血型的分享。好的，接下来，当提示词满足了我们对某个设备的需求后，演示得十分精彩，我们会进行下一步操作。

那么接下来我们要做什么呢？在 GPT 的协助下，我们的第二步将会尝试去编写批量化的脚本。这个批量化脚本的作用就是去突破人机交互质量的局限。举例来说，我们知道 ONI 系统在处理庞大的电子书文件时，比如说一本十万字的书，会遇到一些问题。这种系统设定的局限，如果我们使用批量化脚本来处理，那么这将会非常合理且高效。

同样的情况也适用于大量的文档处理。假设我们有三百个文件，每个文件你都要上传给 ONI 代码提示器，然后让它帮你查找数据错误，这将会是非常耗时的过程。因此，使用这种批量化脚本进行处理会节省大量的时间。这点对于任何数据分析人员来说都非常清楚。

在这里，我想和大家探讨一个改进的思路。在我们的第一步中，比如说，我们参考彭建怡他们的工作，做了数据分析，找到了最佳提示词，并立即将其写入了脚本。是的，这个脚本是批量化的，意味着它可以自动地生成相应的统计结果，比如统计出过百个项。这个过程就是像处理一个 Excel 文件一样，大家都理解这个逻辑吗？这个逻辑是非常关键的。许多人在实践中犯错误，偏要一次性完成所有的工作，却往往达不到理想效果。也许他们可以编写批量脚本，但最后生成的效果并不好。

举一个例子，比如静怡需要处理大量的数据，可能有三百个 Excel 文件需要处理。传统的做法是检查这三百个文件中的错误，这无疑是非常浪费时间的。但是，如果你现在调用我之前强调的代码提示器，是不是问题就得到了解决？所以说，这一套逻辑是非常重要的。这次讲座，我希望你们能理解 API 和大型的代码提示器的重要性。

现阶段，我们的欧洲 AI 团队正在承受巨大压力，因为广大用户对他们的工作效率并不满意。从这周开始，他们会将大量的资源，包括人力、实践经验，都投入到提高社区的服务能力中。所以，我们可能会有更多的能力，例如我们可以直接使用命令提示行，API 调用，甚至是相关插件。

目前，我们已经解决了最优提示词的问题，编写了批量化脚本，以提高我们的工作效率。接下来的第三步是什么呢？ 当我们发现我们的提示词非常出色，而且我们的批量化脚本也写得很好，接下来，我们应该与这个活跃的团队合作，将其转化为一个小产品。现在，我们已经购买了一百到三百个域名，这些域名的特点都是以 432 开头。我们有一个域名是 4384.com，准备为大家提供一百到三百个可用的域名。因此，最好的提示词解决了某个领域的具体问题，最好的脚本以及方便使用的应用程序或者桌面程序，可以被封装起来，最终形成一个非常完美的产品。

所以说，这其实是一种最佳工作流程，大家可以再思考一下，在这三种不同的常见情况中，如何应用这个工作流程。

本次讲座主要解答了以下问题：知识工作者最核心的问题在于怎样解决不同工作场景下的具体问题。我们根据具体工作场景和团队规模，可以将它们划分为以下三种类型：

第一类是针对小团队，尤其是以个体工作为主的场景。例如，如果有一个项目需要由一个主导者与十人左右的团队完成，我们需要解决的问题就是如何有效地将工作任务和方式传达给每一个团队成员。这种情况下，分享工作流程，或者提供最优的操作提示词是一个很合理的做法。但是在更大的团队，例如几万人的大组织里，这种方式就不太合适了。这一类场景的技术和产品能力要求相对较低，用户只需要理解基本操作步骤，如复制粘贴或者查看账单等。因此对非技术人员，如客服等，更适合提供这样类型的界面。

第二类则是针对技术能力较高的团队或个体。这一类场景下，主要通过量化脚本来提升工作效率，因此我们需要向研发团队这样的技术人员提供更高级的操作界面。

第三类则是针对的不仅是内部团队，还有可能包括外部的收费用户。在这类场景下，我们可能需要向外部用户提供某些服务并进行小额付费。

因此，总结起来，这三个类型的场景是我们在 AI 时代最应该注意的不同工作模式。

接下来我将以一个具体的前端界面为例，给大家演示一些实际操作技巧，这些技巧会帮助大家更好地理解和适应上述不同的工作场景。在这一部分，我注意到有些同学可能对某些专业术语不太熟悉，我会尽量给出详细解释。

让我们以 LO 平台为例，第一步，我们需要登录进去。登录之后，在界面左上角，我们可以做一些个性化的设置。第一个就是语言设置，我们可以将其改为简体中文，适应大部分用户的习惯。第二个设置是自动生成标题。我们还可以选择去掉预览气泡，这样可以使界面更为简洁。最后一个重要的功能是云端数据同步，通过开启这个功能，我们可以轻松实现多设备之间的数据同步。以上这些都是 LO 平台的一些基本操作，相信通过这些设置，大家对如何适应不同工作场景有了更清晰的理解。

这是一项大数据应用，利用云端保存我们的一键使用记录。我在这里最推荐的是一个由外国开发者开发的服务。我需要特意提醒大家，这一切并非由杨总团队开发，所有这些产品都是国外开发者贡献的，与我们团队并无任何关系。我们只是使用者而已，你们需要明确这一点，确保不会产生任何误会。

这个产品功能强大，下面我会详细讲解如何进行配置。首先打开指定的网站，你会看到一篇测试文章，这是 losh car 云端数据库的一个产品，它在大模型时代中是一项主流服务。看到网址后，请务必记录下来，它是接下来配置过程的关键。

这个数据库提供了三个主要服务：云端支持、losh core 数据库以及其他尚未明确的功能。

接下来是注册步骤，点击指定位置进行注册，然后登录系统。登录完毕后，你会看到一个新界面，我在这里已经为大家创建了一个演示用的数据库。

创建完数据库后，最为关键的是两个密钥，这是配置数据库连接的必须条件。你需要将这两个密钥填入指定位置，然后点击检查可用性并确定。

最后，我要重申一点，这个云端信息同步的能力，是这项服务最值得推荐的一部分。我们可以实时将使用的数据同步到云端数据库，这样无论何时何地我们都可以方便地获取自己需要的信息。

我们现在就来测试一下，首先我会创建一条新的记录，然后在数据库中搜索该记录，以此来演示云端数据库的实时信息同步功能。

在这次讲座中，我想让大家了解到如何在某个特定的软件中进行数据同步。首先，你需要找到并点击这个同步按钮。当你完成这个操作之后，你会发现时间已经变成了十一点二十五分二十七秒，这表示数据同步成功。

接着，我将在下文中介绍两种可以查看同步数据的方法。第一种方式是进行在线查看，你只需要点一下数据浏览器就可以了。第二种方式则需要用到命令提示行，这一般是程序开发者使用的方法。如果你并不擅长编程，那么你可以将数据保存到本地。这个过程其实非常简单，你只需要找到一个本地数据库的客户端，就可以将这些数据保存在自己的电脑上。我之前常用的是一个叫 NAV 的客户端，你也可以去找一下。当你找到一个合适的客户端后，将网址输入其中，运行程序，程序便会自动将这些数据同步到你的电脑本地。

在这里，我只是简单的介绍了一些基础功能，还有一些更高级的功能这次就不多提了。接下来，我想告诉大家云端的一种配置方法。这种同步数据的方式你们应该都很熟悉了，这个内容在 "肉腿的一个课程" 中我曾反复讲解过。以前的所学内容小节现在就能发挥作用了。一般在国内，大家常用的同步工具是坚果云，只需要在坚果云上进行简单的配置就可以了。关于这个内容我讲解过很多次，本次我就不再展开了。

每种配置格式都有其自己的优缺点。比如数据库模式的优点在于它可以使用数据库查询语言，使得查询变得更为方便。而如坚果云这样的微部署或 VDIV 配置的格式，其优点在于它可以将数据直接生成为一份交程文件，这对于非编程者来说，则更像是一份程序文件，但如果你没有编程技能，查询数据可能会变得复杂。大家理解我所说的意思就好。

我刚刚介绍了数据同步，这是非常核心的一个功能。下面我想介绍另一个同样重要的功能，那就是代理。当你在某个特定的场景中需要用到这个功能时，它的作用可以发挥得相当显著。

设想你在一个无法使用网络代理（翻墙）的环境中，同时，你的团队成员们也无法翻墙；你需要向他们介绍某个功能。此时，我们需要搭建一个代理服务器，大家通过代理服务器来使用这个功能。你只需要启用代理（代理服务器），并根据具体的情况，填入相应的选项即可。但是，假设你是在像阳老师这样的团队环境中，大家都会科学上网，这时代理服务器的功能就显得多余了；它通常只在为学生提供服务的时候使用，所以一般情况下，用得并不多。

接下来，我们谈谈文件相关的功能。数据的导入导出功能是非常重要的，该功能反映了保护用户隐私的需求。由于我们所处理的项目往往不能联网，所以假如你在一台电脑上使用浏览器存储了一些数据，但是如果你更换电脑，之前的数据就会丢失。但如果你使用了数据导出功能，将数据储存下来，那么在新电脑上只需要再导入数据即可。然而，有些功能一般需要被禁用，比如自定义提示词功能。

自定义提示词列表是一个非常关键的功能，对于用户来说，非常重要。这个功能我将稍后详细讲述。其次，我们要讨论访问密码。比如 LOA PM 密钥和访问密码，这是两种不同的访问方式。一般来说，我们更多地使用 API 密钥，它的使用限制较少，并无像其他方法那样每三小时只能使用三十条之类的限制，但费用稍高。例如，我在上个月就花费了约 500 多元人民币，然而，在八月份我却花费了 600 多元人民币。不过，其成本会逐渐降低，比如我在九月份其实使用的更多，但花费的并不多。

接下来，我们讨论自定义模型名。这部分内容是与前面讲述过的产品密切相关的，比如 Local AI 等产品。假设我们本地有一些高隐私的信息或文档，这一部分就填入到模型名里。而我建议大家选择使用 GPT-4 模型。我们在给出提示时，一定要选择最优的模型，以及最佳的提示词语。千万不要因小失大，选择太便宜的模型，你可能只是节省了一点成本，但可能导致你的工作效率大打折扣。

就像你请一个小学生帮忙干活，虽然不用花钱，但他完成活动后你可能需要花费大量时间来修正他的工作。因此，我主张使用最优质的模型来完成任务。大体上讲，我们通常会选择 GPT-4 作为首选模型。

在之前的完整的 GPT 讲解课程中，我已经对所有相关的参数进行了详细解析，今天我不再重复。但是，我还是要强调一下，这其中有一个被称为 "话题新鲜度" 的参数对你的模型选择极为重要。以前在完整的 GPT 课程中我曾提到过这一点。

例如，如果你主要工作内容是处理严肃的、例如法律文本之类的内容，那么你可能需要把话题新鲜度这个指标稍微调低一些，但是如果你的工作内容是写小说、笑话或者诗歌这样更具创新性的文本，那么你则需要将这个指标调高。我在三月份的课程中已经详细解释过这些。

我想再提醒大家一个常被忽略的功能，就是 "历史摘要"。如果你主要工作内容是翻译，那么我建议你最好不要使用这个功能，因为在进行翻译工作时，历史摘要的效果并不理想。

下面我想讲的是另外两个核心概念，它们分别是 "面具" 和 "提示词"。让我们先来讲解一下 "提示词"，这个更容易理解。

提示词分为内置的和自定义的两种类型，未来我们可能会将 GPT-4 中的数千个提示词全部内置于系统之中。

假如我们要添加一个新的提示词，我将给你展示一个如何添加的指导示例。

首先，你需要给这个提示词起一个方便自己记忆的名字，比如我们可以叫它 "中文写作大师"。之后，你需要为其定一个最优化的提示词模型。

对于每一个提示词，我们都有约定的输入和输出格式，这是一个很容易被忽视，但却极为关键的部分。举例来讲，我给大家做了一个示范，这是我经常用的两种提示词。

然后，我将对上述内容进行演示和展示。回到主界面，我将打开某个功能，进行现场演示。

实时笔录是一项强大的功能，在实际应用中，我们可以把它视作一个翔实的记录工具。当我们在与其他人进行会议时，例如采用腾讯会议等工具，我们可以打开实时笔录的功能，这意味着，所有的讨论和发言都将被精确地记录下来。

具体来说，当我们需要导出记录时，我们可以选择将它导出为纯文本。假设我们找到一个特定的会议记录，实时笔录功能就可以帮助我们将这次会议的所有讨论详细地记录下来。如果需要，我们甚至可以选择其中的一段，进一步进行整理。

此外，我们还可以通过实时笔录的功能来了解讨论内容的核心要点，这样，无论是进行进一步的分析，还是进行总结，我们都能轻松地找到我们需要的信息。

对于中文写作来说，之前我们需要抄写和粘贴的步骤，现在都可以通过书面语小助手来实现。这可能似乎是一个小细节，但实际上却非常重要。同时，由于现在我们无需进行重复的复制和粘贴操作，我们也可以节省大量的时间。

作为讲师，我注意到有些学生对这些操作可能比较不熟悉，我最初认为，这些都是助教能教给大家的东西，但后来我发现这并不尽然。这并不是说助教没有尽全力，相反，他们教了很久，可能是因为这些知识点有点复杂，所以并没有完全教会大家。

因此，我决定来给大家做示范，给大家详细地说明这个过程。该过程相当于一次学习过程，我们最终将经历讨论、理解、实践和合并各个步骤的完整路径。

我尽量将提示信息优化到一种状态，那就是无法再进行优化。这时，我们可以感觉到这个工具的效果非常好，能够帮助我们更好地完成各种文本工作。

例如，尽管括号看起来可能微不足道，但实际上，它们在中文写作中非常重要，因为它们有助于我们清晰地组织结构，进行有些步骤。

文本工作可能包括各种各样的任务，比如说，校对翻译，整理口语稿，或者改写成更有创意的稿件。对于改写稿件这一步，我们可以对参数进行调整，要求它在开头使用设问的句子，在结尾处用金句进行小结。这是一种有效的写作技巧，可以帮助我们生产出更有创造力的文本。

将这些提示词联合使用，最终会形成一个工作流，就像是一副面具，能够增强我们的效率和效果。这个工作流将使我们能够更好地处理文本工作，例如前面提到的任务，都可以尝试约束在大约 10 个提示值以内。

我们所构建的模型，默认使用的资源主要是 GPT-3.5 的，因为在大多数情况下，这种模型既能提供良好的效果，又能较好地节省资源。

总的来说，无论是进行翻译工作，还是整理口头语，在实现过程中我们不可避免地会遇到许多难题。但是，通过良好的策略和工具，我们可以找到优化问题的途径，让这些工作变得更加简单而有效。

让我现在为大家示范一个具体操作。这里我们会涉及到一个被称作 "自定义模型面具" 的概念。在这个自定义模型面具的设置界面里，我们可以看到一个「提示词」选项。提示词就是我们稍后将会讲解的那一部分。首先，来看看提示词包含了些什么内容。我们可以看到，提示词时代表角色的名称。

然后，我们会看到一个选项，问我们要用什么模型。再之后，我们会看到一个关于话题新鲜度的设置。这个话题新鲜度，我们当前设置的是一般般。

假设我们这时候要创建一个新的自定义模型面具，要它表现得更好一些，我们该如何操作呢？我们可以按照以下步骤进行。首先，找到并点击「克隆预审」的按钮。完成克隆后，这个新的面具就会出现在我们的面具列表中。然后我们为这个新的面具取一个新名字，就像小红书写手 2023 版这样。

接下来我们来调整这个模型的新鲜度设置。我们将它调大一点，但是 22.0 似乎太高了，我们调整为 1.0 应该就足够了。然后，还有一个选项是是否需要记下这个操作的摘要，但这个随你决定。完成这些修改之后，我们可以继续优化这个提示词。

通常，我们会设置三个系统用户，大部分时间使用第二个系统用户。接下来，我们点击确认按钮，完成这一步骤。

接下来，我将展示我们该如何使用这个新建的模型面具。这个功能是非常重要的，因为一个模型面具能支持多个提示词。我们单击模型面具选项，展开新的聊天窗口。你可以选择清除聊天窗口，或者重新打开一个新的聊天窗口，但我发现后者的效果通常更好。然后，在所有面具的选择列表中找到并选择刚才创建的小红书写手的模型面具。

用这个新的面具模型，如果我们提出一个请求，比如，「请推荐一本打动你的书？」然后看到生成的答案，比方说是，「推荐阅读杨志平的书《新知》」。这本书其实是很不错的。事实上，这个功能可以帮助我们在闲暇时间获得有意义的时间度过方式，它还可以帮助我们的工作。

实际上，我们对这个提示词还可以继续进行优化。比如说，我们可以在请求中附带一本书的图书简介，这样生成的结果会更准确。这个小技巧在实践中是非常有用的。当然为了快速演示，我在这里没有附带图书简介。

当我们完成所有的这些步骤后，你应该已经对这个 "自定义模型面具" 有了一些基本的理解。并且，你能知道，每一次生成的结果其实都可以进行进一步的优化。这就是我们今天关于模型面具的讲解，希望对大家有所启发。

作为计算机专家和科学作家，我想向您全面地介绍两种不同的人工智能框架，以及它们如何在不同的课程中应用。首先，这两种框架包括了 "阿卡" 框架和 "科" 框架。理解这两个框架，并了解它们如何操作，将有助于我们更好地巧妙运用提示词。

在我们的完整的 GPT（生成预训练转换器）课程中，使用的核心框架是 "阿卡" 框架。同样，我们的 AI 辩论课程中也使用了另一种框架，这就是「科」框架。这两种框架对应的课程即将上线，欢迎感兴趣的同学拭目以待。

大部分优秀的提示词的设计和应用，都可以在这两个框架中找到。除此之外，我们还会积极收集并整理大家在实践中发现的优秀提示词。例如，微博上一位名为 "宝约 XP" 的微软工程师，他不时会分享一些非常好的提示词。而在 GPT help 社区，也可以找到许多出色的提示词。这样，我们就能更全面地掌握和应用各种高效工具和策略。

当我们完成学习材料的整理和编写后，下一步就是要分享这些信息。此时，我们可以选择将信息导出为图片、JSON (JavaScript 对象表示法) 或 TXT (文本) 格式。通常，选用 TXT 格式更为便捷。在选择导出格式后，我们通常会取消默认选项，然后选择 "全部" 选项，即导出全部信息。然后可以查看预览，并下载文件。

一旦文件下载完成，我们就可以将其分享给他人。此外，还可以将这些文件分享到 "Sai" 社区，不过这个功能在过去可能存在一些问题，我们期望它能尽快得到改善。

这两种提示词框架和分享功能，可以帮助我们更高效地利用 GPT 技术。在了解了这几点之后，你将有一个全新的理解关于这些工具如何运作，以及如何将它们应用到实际的场景中去。希望这个介绍能帮助大家进一步深入理解人工智能和大数据语言模型，从而使我们能更好地使用这些工具，取得更多的进步和成就。

让我们继续进行下去。刚才我们的讨论停在了第二点，现在让我们回过头来再仔细看看：它是关于提示词工程和四川 "恰" 的知识。我们有时候在各类学习分享的平台上，能够看到别人写的一些非常好的提示词，这些词汇我们也可以参照收藏。比如说，可以找一些我们认为他们写的比较好、有用的提示词收藏过来，进行推广和应用。另外，我们也可以在网络上一些资源丰富的网站进行学习和查阅。在这里我特别想提醒大家，虽然这个网站并不是我们开发的，可能有些人根本不知道是谁开发的，但是它依然有价值。只是需要注意，不要因为对网站的开发者不清，而对合法使用产生顾虑。全面理解并适当应用，以提升我们的学习效果。我的介绍到这里暂告一段落。

在这个网站上，不管是谁开发的，我们都可以找到大量的提示词。比如说，我可以选择「咨询师」这个词汇，并且把它复制下来使用。这种复制粘贴的过程实际上就是一种衔接关系的构建。在未来，会有更多的提示词直接内置在里面，至于是如何内置的，逻辑非常清晰，大家应该都能明白。

接下来我想说的第二个关键点是关于批量化脚本的。但是因为这个脚本中有我的个人 API，出于安全考虑，我不能直接给大家看到。这里我只能给大家演示一下它的运行流程。

这份脚本原本是我们团队一起讨论和使用的，目前在其中仍然存有我的个人密钥。我只能告诉你们它是这样运行的：我们先完成对输入输出的预设，然后再运行这个脚本。我发现似乎屏幕共享中有些信息没有完全显示出来，让我重新共享一下。

现在，大家可以看到我的屏幕了，我想给大家示意的是，比如说我们需要对整本书或者其他长文档进行检查，我们就可以通过这种方式进行。我们预先准备好需要检查的文档，然后再调用相应的脚本。既然其中存在我的个人密钥，我在此处就不方便深入展示，但是大家应该能够理解整个过程到底是如何运行的。接下来，我们就可以正式调用这个脚本进行操作了。

在我们对现有脚本进行调研后，你们可能会发现一个问题，那就是不管你如何处理，可能都没能深入到问题的核心。现在这个问题有所改观，我们可以进行下一步的探讨。这部分我们无需纠结，大家应当明白这是什么问题。它其实界定了 API 的功能，这些 API 会直接产生一些结果，这些结果其实就是更定制化的输出。

可能是网络状态突然变差，导致大家在利用这些 API 时可能会遇到一些问题。如果大家对 API 如何调用，如何编写存在疑问，我强烈推荐你们参加我们的 AI 编程课程。广告时间结束，我们再回到我们的讨论。

感觉现在应该会有内容可以呈现出来，尽管不知道为何一瞬间好像产生了什么问题。现在的网络环境显然对我们的讨论产生了干扰，但我们不会因此影响主线的探讨。那我们停止等待网络恢复，接着进行下一个环节，并且会有时间给大家提问和交流。

下一个环节我大致给大家展示一下，我们如何将先前的工作部署为一个产品。我们这篇测试文档就是这样处理的，你们只需将目标文本输入即可。

我们来观察一下，当前的提示词并不完善，我们需要换一个更能引领我们的提示词。所以我们将输入端的提示词换成刚才的那条。这让我们认识到，提示词是非常关键的一环。这应该让你们越来越明白程序运行原理。

如果看完我们的演示，你们发现有错误未检出，或修正部分无何变化，可能是我们的提示词设置还存在待优化的部分。一旦我们更换了这个提示词，我们将能获取到更多的反馈信息。

但大家需要注意的是，尽管看起来我们做了许多改变，但核心关键点的改变却非常微小。大家注意观察一下，我们进行异议部分的处理，认知的改变来源于我们对大模型领域的理解，这就是一个理由。

这样一来，我们得到的结果将会呈现非常好的效果。所以，我备有一个推进计划或者说是脚本，我让大家看一下这个脚本的输出效果，你们打开查看一下。这是我提前准备的脚本，你看，实际上是将我在知心球上的一段话进行了一些工作处理，你们发现这段话可以进行很多的编辑工作。

所以说，借助这里的工具，你们可以进行大量的工作，并从中获取到一些思维方向和策略。

例如，假定你是一本书，书名含有十三个字。是否可以这样进行定制，即编辑可以按需定制文档呢？我想大家应该清楚了。所以，现在大家正在聆听这场研讨会，理解我所说的内容了吧？

很好，今天的预演结束之后，我发现部分内容可能存在一些问题，诸如翻墙问题导致的网络问题，我估计有些人可能理解起来会有困难。但是，我们今天先不要深究这些细节，我们将在演示结束后再回头看这些问题。大家需要先理解我现在讲的内容。

所以，我们真正关注的，对我们实际应用的最重要的一点，就是找出最优的提示词。为了找到这个最优的提示词，我们可能需要花费大量的时间进行测试。测试完成后，第二步，我们要做的事情就是提高我们的工作效率。我们把提示词嵌入到 AI 调用的脚本当中。

然后，第三步，我们想将这个工具提供给更多的人，我们会添加授权、美化界面、并设置收费机制，以此向用户提供服务。因此，这就是整个优化工作流程。过去的六个月里，我们一直在努力理解如何更高效地进行工作，就像攀登八万英尺的高峰一样。

好的，今天的讲座我已经讲完了。现在，也许是因为我喝了七杯酒导致我稍微有一些忘记了这一点，但是我希望大家能清楚理解今天的讲座内容。


