## CH1101实践策略1全局认识

### 课件汇总

P01 —— 本节视频说明

你平时是如何获取信息的？信息获取等同于「百度」或「谷歌」吗？

平时我们可能更多是从关注的公众号每天的推送、朋友圈里朋友的分享等被动地接受信息。百度或 Google 也并不等于信息获取，只是信息获取的一部分。那么，信息获取是什么？如何更好地进行信息获取？

本节你将学习实践策略「信息获取」，学习「一键查找」、「批量下载」、「智能获取」三个妙招，走好信息分析第一步，获取信息不对称。

P02 —— 什么是信息获取？

信息获取，也称为信息搜集，情报收集，是信息分析流程中的第一个关键环节。

信息获取从平时有意识积累的信息源的信息源开始。

P03 —— 信息获取：关键步骤

信息源：从一个信息源快速开始。

快速验证：内容是否符合需求。

调整信息源：更好的信息源。

保存到本地：批量下载与智能获取。

P04 —— 信息获取：学习要点

| 问题 | 妙招 | 工具/操作细节 |
| --- | --- | --- |
| 信息过载，如何快速定位信息 | 一键查找 | Ctrl+F; Ctrl+space |
| 如何将已有文件，批量下载到本地 | 批量下载 | Zotero; Downthemall 插件 |
| 将网络零散信息整理成可下载文件 | 智能获取 | 爬虫与 RPA 软件 |

P05 —— 一键查找：Ctrl+F

单个窗口中，如单个网页和单个文档内， 都可以用 cmd+F（win 系统按 ctrl+F）加上检索关键词，一键查找相关内容。

一键查找：Ctrl+F 适用场景

Ctrl+F 适用的场景包括：

1、明确关键词。有明确的信息目的，已经很清楚信息的关键词。比如，全文很长，只想要某一片段，或者记得一些字词，想找完整的片段。

2、排查筛选。有大量信息的时候，用于筛选靠近（排查）。比如，使用信息分析工具箱时。

3、填充信息框架。有想要填充的框架，比如，时间空间变量，可以利用一些关键符号快速填充。

一键查找：关键词

一键查找：注意事项

在 execl，word 等特定文件中， ctrl F 还有更多的功能，如支持查找后替换。

有的文件是纯图片不支持 ctrl+F，可以先用 OCR 文字识别工具先转为文字。

一键查找：电脑内搜索

整个电脑内，比如在 mac 中使用 cmd + space（空格），可以实现文件一键查找。

除了系统内置外，有相应的软件也可以实现相应功能。比如 Easyfind 是 mac 自带搜索的替代或补充，无需索引即可在任何文件中查找文件、文件夹或内容。以及 Alfred 是一款可以提升 mac 工作效率的软件，可以实现跨网页和文件的一键查找，不过该软件需要付费，网上已有大量教程，感兴趣的同学可自行研究。

Win 下可以使用 Everything，它能够基于文件名快速定文件和文件夹位置，也支持对文件内容进行检索。也可以安装 PowerToys，安装后使用快捷键 alt+space 可以实现与 mac 中 cmd+space 类似的功能。

部分软件的安装包备用下载链接：链接

[安装包\_免费高速下载|百度网盘-分享无限制](https://pan.baidu.com/s/1VZyluXvXEsl-s53Syi6Tzw?pwd=ok86#list/path=%2F)

密码：ok86

P06 —— 批量下载：Zotero 插件

进入官网：

[Zotero | Your personal research assistant](https://www.zotero.org/download/)

点击下载 Zotero 桌面客户端和浏览器插件，下载之后按照默认提示一键安装。

Zotero 插件备用下载链接：

[百度网盘 请输入提取码](https://pan.baidu.com/share/init?surl=rWt0krxVr4ys-wsbyyE8wQ&pwd=w25p)

网盘下载地址密码：w25p

温馨提示：不知道如何安装注册 Zotero，点此课前预习视频 Zotero 注册下载进行回顾。

装好 Zotero 和对应的插件后，Zotero translators 可从网页上直接获取元数据。

比如，可以在豆瓣上批量获取图书的元数据。

批量下载：千篇一律

千篇一律，是一个学习整个领域的经验法则。它指要获取一个领域全局认识，至少要下载 1000 篇论文以上，才有批量优势。

批量下载：千篇一律操作步骤 1

使用知网检索领域文献

导出 Refworks 参考文献

导入 Zotero

批量下载：千篇一律操作步骤 2

步骤回顾：导出为 Refworks 格式

选择若干篇论文（中国知网一次可以批量导出 500 篇论文），点击导出参考文献。

选择 refworks 格式导出。

批量下载：千篇一律操作步骤 3

步骤回顾：导入 Zotero 相应文件夹

选择导入到 Zotero 相应文件夹，就获取了这些论文条目的元数据。

P07 —— 批量下载：注意事项

按照学科线索检索领域而非单个关键词

学科论文过少/过多？

新电路：借助领域大牛

批量下载：注意事项 1

这个实践策略最反常识的地方不是下载了若干论文，而是通过这么多论文尝试构建一个更高维度的全局认识，一个大的框架。现在不一定细致入微学习每一个知识点，但通过这个操作，快速知道领域内有哪些重要知识点。未来当有需要时，可以快速想到去哪个学科中找到相应知识。

在实践操作过程中，有些同学习惯只用一个关键词检索论文。认为这是就下载领域所有论文，这是一个误解。比如了解人工智能领域，光深度学习这个词，它会有几个相关的词。比如深度学习，深度神经网络，神经网络等，使用单个关键词击中整个领域全局概率极低。

但是直接按照学科线索检索整个领域，并使用一些关键词辅助查全。将人工智能诞生以来，近四十年最核心的论文都找出来。这样的话击中整个知识体系的概率更大。再通过一定的分析、对比，就能将最重要的 20% 的知识找出来。

批量下载：注意事项 2

检索过后可以看一下检索论文数量有多少。

比如，直接检索心理学的所有论文，太多了，可以尝试缩小它的颗粒度。比如最能代表这一个学科的学术共同体的是年轻的博士生。我们可以以博士论文为代表，优先阅读这些论文。

第二个情况是有些学科领域的论文过少了，比如说婚姻心理学，你会发现它硕博方面的论文有点少。这意味着整个领域中的高质量的研究没那么多。这个时候可以尝试着扩大一下边界。把硕士论文也包括进来。假设把硕士论文包括进来，这个数量还是不够多，就把学术期刊论文也包括进来。

这里有一个经验法则，叫千篇一律。你要获取一个领域全局认识，至少要下载 1000 篇论文以上，才有这个批量优势。大多数情况，领域的博士论文往往都大于 1000 篇。

批量下载：DownThemAll! 插件

借助 DownThemAll! 插件，可以实现文件的批量下载，如批量下载某一学者主页公开论文的 pdf 文件，批量下载图片。

DownThemAll! 插件备用下载链接：

[百度网盘 请输入提取码](https://pan.baidu.com/share/init?surl=rWt0krxVr4ys-wsbyyE8wQ&pwd=w25p)

网盘下载地址密码：w25p

选择合适的浏览器，安装 DownThemAll! 插件，也可以实现文件的批量下载，如下载 pdf。

批量下载：注意事项 3

一次下载过多链接，可能被网站封禁。

解决方法：每次选择合适数量文件下载，分批下载。

P08 —— 智能获取：什么是爬虫？

智能获取：八爪鱼采集器

八爪鱼采集器一款是无需写代码的商业爬虫软件，提供免费版的服务。

八爪鱼中自带现成的模版，只需要选择相应模板后，填写相应信息就能完成数据采集。

如果没有现成的模板，可以使用自定义模式，操作也比较简单，在八爪鱼官网上有丰富的教程查看，感兴趣的同学可以自行练习。

八爪鱼安装包备用下载链接：

链接密码：ok86

智能获取：更多

除了八爪鱼采集器，爬虫软件还有开源软件 WebScraper、商业软件后羿采集器等可供使用。

因为爬虫主要是使用 Python 开发脚本，通过发送 http 请求获取 cookies 或者直接注入网页等方式获取数据，使用场景限于自动抓取网络数据，且不够智能。RPA（Robotic process automation）技术正逐步兴起。RPA 可以通过模拟人类在软件系统中的交互动作，自动执行那些基于规则、重复的业务流程，达到提升工作效率、减少人力成本的目的。

这里给大家介绍一家全球领先的 RPA 公司上海弘玑 Cyclone。其中， EasyPie 工作易是弘玑推出的首款 RPA 云端产品，提供免费使用，感兴趣的同学可以自行探索。

智能获取：注意事项

使用智能获取前，请阅读网站相关条例规定，在合规合法的范围内进行操作。

请勿使用爬虫或 PRA 技术抓取个人信息等法律禁止或网站禁止获取数据信息。

P09 —— 本节说明

本卡包内容为信息分析 2.0 实践策略视频：使用 Web Scraper 爬取薪酬数据。

部分操作可能已不适应用，仅供参考。

爬虫工具：Web Scraper

网络爬虫是指自动化批量下载数据的程序。对非程序员的普通用户，可以借助不用写代码的爬虫工具 Web Scraper 爬取 Web 页面数据。（访问页面需要梯子）

注意：进入 Web Scraper 的方式为右键选择检查。

Web Scraper 使用包括 4 步：

1、创建 Sitemap。

2、设置选择器 selectors（使用 Web Scraper 的关键与难点）。

3、检查爬取地图。

4、执行爬取、下载成功爬取的 csv 文档。

Web Scraper 提供了多种 selectors ，核心是：

文本选择器 Text selector

链接选择器 Link selector

元件选择器 Element selector

在 Web Scraper 官方文档左侧，你可以查找到每个选择器具体使用说明。

步骤回顾：爬取数据

视频中以拉勾网 —— 产品经理页面数据为例。

使用的选择器依次为：

1、element click 获取 30 个页面链接。

2、element selector 创建一个组块，方便将不同子选择器信息归结在这个元件下。

3、text selector 分别获取公司名、工资、公司地址、公司福利等信息。

设置核心逻辑如下图所示：

具体操作详见视频。你可直接复制 Sitemap-lagoudata.txt（密码:xkjv）原始代码，导入 web Scraper，抓取页面数据。如需抓取拉勾网同类页面，只需选择 sitemap lagoudata-edit metadata 修改为相应地址。

步骤回顾：数据预处理

在正式数据处理前，需对数据预处理，使软件可以识别、计算数据。视频演示使用办公软件 Excel 进行数据处理，你可选择其它软件处理数据。

主要涉及的操作包括：

将不同数据分列。如将移动互联网，教育/B 轮/150-500 人处理为 4 列：移动互联网教育 B 轮 150-500 人。

将数字与文本结合的数据列，处理为纯数字列。如将 10k-14k 处理为 2 列，一列数据为 10，另一列为 14。

步骤回顾：数据分析

视频演示分析以轮次和工作经验作为筛选条件，分析其平均值、标准差、最大值、最小值、25 分位、50 分位、75 分位值。

主要步骤：

1、利用筛选器获取目标数据。

2、使用公式计算相应数据（可构建数据分析模板，方便复用）。

主要使用 Excel 公式：

计算分位值、最大值、最小值 QUARTILE 函数 - Office 支援计算平均值 AVERAGE 函數 - Office 支援

Ps. 最大值、最小值还可通过 MAX 函数、MIN 函数计算。

爬取薪酬数据注意事项

爬取薪酬数据，可注意：

1、Web Scraper 官方文档是你学习 Web Scraper 最好的资源。多拿几个不同网站（如猎聘、前程无忧）试手吧。

2、你可以尝试定期爬取某网站、某岗位数据，用以分析不同时间招聘岗位、薪酬情况的变化。

3、你还可以尝试使用 GitHub 开源库提供的爬虫代码 nnngu/LagouSpider：爬取拉勾网职位信息的爬虫！ 爬取数据。

尝试抓取其他网站

课程以拉勾网为例，演示如何使用 Web Scraper 批量抓取数据。

尝试抓取猎聘、前程无忧、智联招聘、中华英才网等网站数据吧？

Ps. Web Scraper 也可抓取谷歌学术、豆瓣等页面数据。Web Scraper 功能强大，多数页面均可抓取，但设置复杂程度不同。如果你略懂一点 CSS 代码，更容易理解 Web Scraper 抓取原理。

### CH1102实践策略2信息获取P04一键查找

在开始之前，让我们先回顾一下日常获取信息的方式。我们通常会通过关注的公众号推送、朋友圈分享，或使用资讯软件浏览推送的文章、视频等渠道获取信息。说到这里，你可能意识到这些都是被动接收信息的方式。那么被动接收能算是真正的信息获取吗？

在日常生活中，我们还会遇到需要主动获取信息的场景。例如，当我们第一次遇到一个陌生术语时，往往会通过百度或谷歌搜索来了解其含义。得益于搜索引擎的进步，我们甚至不需要精确调整关键词就能获得答案。这使得许多人将信息获取等同于使用百度或谷歌搜索。

然而，我们经常会遇到这样的情况：搜索后点开第一个结果发现并非所需，即使逐个浏览多个页面或更换关键词，仍然找不到想要的信息。这说明仅依靠谷歌和百度并不等同于完整的信息获取过程，它们只是信息获取的一小部分。

那么，什么才是真正的信息获取呢？在本课程中，信息获取是信息分析过程的第一步，它始于我们有意识地积累和利用信息源。我们在进行信息获取时通常都有明确的目标，应该首先从自己积累的信息源着手。比如，我们可能会建立网页收藏夹，购物时会优先查看收藏的购物网站，而不是直接使用搜索引擎。

在检索个人信息源后，才会考虑使用搜索引擎。值得注意的是，百度或谷歌只是搜索引擎的一种，每个搜索引擎都有其局限性。例如，百度虽然能较全面地搜索中文世界的信息，但在搜索微信文章时就会遇到限制。

在互联网环境下，搜索结果中不会出现来自微信文章的信息。这是因为搜索引擎只能检测到公开信息以及允许它检测的信息。微信文章不允许被百度获取，但可以在微信和搜狗合作的搜狗引擎中检索到。除了主流搜索引擎外，还有很多其他的搜索引擎可供使用。

从分析的角度来看，我们不仅要获取信息，还要获取高质量的信息。信息质量对我们来说至关重要，因为「Garbage In，Garbage Out」（输入垃圾，输出垃圾），如果输入的是垃圾信息，无论如何加工都可能得到垃圾结果。这时我们需要借助专业数据库获取信息，比如存储学术论文等具有严谨证据支撑的信息的数据库，以及商业领域的相关数据库。

同时我们也要认识到，互联网上的信息只是信息世界的一部分，有些信息在信息世界中并不存在。因此，我们可能还需要通过实地调查或人物访谈的形式获取关键信息。通过联系信息员与开源信息员、使用搜索引擎、专业数据库、访谈或调查等方式获取信息，这些都是关于信息源的重要方法。科丹组为大家提供了信息获取中的四个关键步骤。

在信息获取的过程中，最常见的可能是从信息源到快速验证这两个步骤。例如，当你看到杨老师推荐的一本书想购买时，会在各个网站搜索并核实商品信息。而实际上比较实用但容易被忽视的是第一步和第三步：信息源和调整信息源。以购书为例，如果在京东搜索未找到，可能需要转向淘宝或当当。如果这些网站都找不到，可能需要去专门的售书网站如恐怖救赎网搜索。如果仍然找不到，可能需要调整到实体图书馆寻找。再比如疫情期间购买口罩的例子...

首先，我们可以从全国性电商平台如淘宝和京东开始搜索。但不必局限于此，可以考虑排名靠前的 100 个购物 APP 作为信息源，实际上前 20 个就足够了。如果仍未找到所需信息，可以更换信息源，比如考虑当地排名靠前的 10 家商场。还可以考虑全国的口罩生产基地，这些适合批量购买。例如，当时开智社团就是通过这种方式从武汉购得了 100 万个口罩。

此外，我们还可以尝试全国排名靠前的在线药店 APP，通过排名第 10 的 APP 最终买到了口罩。也可以寻找一些社区、区委会或机构提供的免费发放口罩渠道。同时，一些社交网站虽然不是专业购物网站，但用户可能会分享一些购买渠道，比如知乎网站等。这就是在调整信息源时的具体应用。

平时大家可能很少会用到信息获取的完整四个步骤。这里再举个例子，比如做教研时就会经常用到这四个完整步骤。例如，当我们需要查找 Stanovich 提出的三重心智模型相关资料时，不能仅仅依靠百度搜索就结束。我们需要使用专业的学术数据库找到更原始的资料，找到 Stanovich 发表的论文后，需要快速判断该论文是否与三重心智模型相关。如果在数据库中无法获取原文，我们会考虑到该学者可能在个人官网上公开发表了论文，于是调整信息源到 Stanovich 的个人网站。在那里不仅找到了论文，还可能发现更完整的数据。随后，我们会使用工具将网上的论文保存到本地，以便进行下一步处理。这就是完整的信息获取四个步骤。

针对这四个步骤中可能遇到的问题，为大家提供了三个妙招。第一个妙招是解决如何快速定位信息的问题。一般来说，我们遇到的信息量可能不会太大，比如维基百科中的一个词条，或者一篇不是很长的论文，这些都是可以轻松阅读的。

当你找到的论文可能有十多页甚至二十多页，或者找到一本有几百页的书时，若要进行阅读或做笔记就会相当耗费时间。这时我们可以借助检索功能进行快速定位。同时，我们还会涉及到网站上的内容文件下载。当文章数量很多时，有时会涉及到几十篇甚至上百篇，一个个点击下载显然效率不高，这时我们可以使用一些下载软件或插件来批量下载。另外，我们会发现有些信息并不是以文件形式提供的，比如一些表格形式的信息，如果逐个复制会非常耗时，这时我们就可以借助爬虫软件进行智能获取。

接下来，我将为大家介绍一键查找的相关技巧。在生活中，想必大家都遇到过这样的场景：我们记得在某处看到过某篇文章，当时对其中某一段印象比较深刻，后来想要找来看看时却很难找到了。比如有一次我和朋友聊到关于记忆的话题，我记得杨老师在一篇文章里讲过记忆那段话讲得很好，我想分享给他，但却记不清具体是哪一篇，只是隐约记得是在杨老师的博客上看到的。这时如果再去打开杨老师的博客，一篇篇点开去找会比较费时费力，还可能会有遗漏。

那么，有没有更快的方法帮助我们快速查找呢？我们可以借助「一键查找」这个妙招。因为是在单个页面中查找，我们可以使用 Ctrl+F 这个快捷键。在单个网页或单个文档内，我们都可以使用 Ctrl+F，然后输入要查找的词语来快速定位相关内容。这里也解释一下为什么使用「F」键，因为 F 是「Find」（查找）的首字母。回到我们要找的那篇关于记忆的文章，因为确定是在杨老师的博客里看到的，我们可以先在搜索引擎中输入「记忆」并找到杨老师的博客地址。

相关文章检索时，可以通过在浏览器中点击进入文章，然后使用 Ctrl+F 快捷键输入记忆关键词来快速定位相关内容。现在让我们一起体验这个过程：首先打开搜索引擎，输入「记忆」作为关键词，然后在扬声博客中进行检索。

在得到搜索结果后，我们打开前三个网页。在第一个页面中，按下 Ctrl+F 键，此时会弹出搜索框。在搜索框中输入「记忆」，系统就会自动定位到包含这个关键词的内容。我们可以通过点击箭头在不同位置间快速跳转，也可以直接翻页查看标记高亮的地方。

经过检索发现，第一篇文章虽然提到了记忆，但并非我们要找的内容。继续查看下一篇，提到「Conserve 已经固定了记忆的分子」，再往下找到标题中提到记忆的部分，这样就找到了我们需要的内容。值得注意的是，在屏幕右侧会通过颜色标记显示关键词出现的位置，方便快速查找。

有时候我们可能只记得一些片段内容，比如记得杨笠提到过苏德朗的一首诗，其中有「突破太阳系」这样的片段。这种情况下，我们同样可以使用这种方法：输入「突破太阳系」定位到相关文章，然后使用 Ctrl+F 进行精确定位。

以上演示的是在有明确关键词的情况下的搜索方法。此外，这个技巧还可以用于信息排除，特别是在处理大量信息时进行快速筛选。比如在使用信息分析开放工具时就很适用。当然，建议初次使用时先通览全文，了解各个大类，并重点查看相关类别。

将最有帮助的信息应用到日常决策和行动中。在实际使用时，我们可以借助关键词搜索进行一线查找。现在让我们打开提供给大家的开启信息分析工具箱的引导，可以发现里面有很多内容。第一次使用时，我们可以在左侧通过类别快速查看相应内容，找到自己感兴趣的类别。我们提供了商业、职业、实用以及通用等十大类。

在浏览类别之后，我们实际使用中可能需要借助关键词搜索这个技巧进行筛选。比如说，我们想了解一下杨老师经常提及的央门亨特的工具怎么样，那么就需要找一些企业分析的工具。央门亨特是一家企业，我们可能无法直接使用央门亨特作为关键词，但可以考虑到它是商业企业，因此在搜索框中输入「企业」，可以看到有 49 条结果。

我们可以直接对这些结果进行快速排查，搜索结果中含有「企业」的地方都会被标记颜色。通过快速浏览，我们发现有些是大数据分析工具，有些是为企业提供的服务，这些都不是我们想要的分析工具。我们可以同时打开几个标签页进行对比，比如发现有行业分析工具、咨询网站、微博平台、数据分析工具、消息传递程序等，但这些都不是我们需要的。

在进一步查看后，我们发现有专门提供信息的专栏，可以订阅到相关工具。通过这些工具，我们可以查看企业分析的内容，了解相关公司的信息，并提供类似的数据进行进一步分析。

这就是利用关键词搜索进行快速筛选的过程。在搜索过程中，我们可能还需要进行关键词的转换。比如使用「企业」搜索得到 49 条结果，如果找不到需要的信息，我们可以考虑使用「公司」或「法人」等相关词进行搜索。经过测试，发现其中并没有与法人相关的信息工具。

以想象一下，除了企业外，上市公司还需要发布年报。年报是我们进行分析的重要工具。通过输入「年报」这个关键词，我们可以直接定位到其他相关工具。这就是我们在快速实验中设计的一些操作方法，可以快速排查和切换关键词。

另外一种情况是填充清晰框架。比如说，当我们想填充一个使用分量的清晰框架时，可以利用一些关键符号快速查找并进行填充。例如，我们打开了「认识心理学」第 100 页，想要连同时空变量的框架一起进行信息填充。

首先了解一下可以使用的关键时间类。我们可以使用 Ctrl+F，输入与时间相关的关键单位「年」进行快速查找。这样可以找到很多关键时间节点，包括一些最早的研究年份以及 Newman 的研究时间等。除了使用「年」，如果我们想研究 19 世纪或 20 世纪的研究，还可以直接使用「19」这个数字来定位相关的时间阶段。

收集完时间信息后，我们可能还需要收集一些空间信息。最重要的空间信息可能与机构有关，我们可以输入「大学」这个关键词，找出相关的大学机构。除此之外，还可以使用「学院」、「研究所」等关键词。

在收集完时间和空间信息后，我们可能还想了解一些关键人物。由于美国人名通常使用点号作为间隔，我们可以通过搜索点号符号来快速定位相关人物。最后我们可以看到，文中整理了一个心理学家的名单表格。

我们可能还想知道维基百科中提到的一些重要书籍和期刊，这时可以使用书名号等特殊符号来帮助一键查找。这里定位到了《学生科学史》《人生心理学》等几本书，我们可以进一步搜集和筛选。这就是 Ctrl-F 在填充信息框架时的使用方法。

以下是 Ctrl-F 的常用关键词总结，主要分为两大类：文字标识符和形式符号。文字标识符通常是固定的文字搭配，我们可以反向拆解这些固定搭配的关键词来快速查找。例如，涉及时间时，我们会用到「某年某月某日」，这时可以使用「年月日」等关键词进行定位；涉及空间时，可能会涉及「城市"」区「"某某大学"」某某研究所「 等关键词，我们也可以用它们进行定位。同时，域名也是一种很好的文字标识符，我们可以借助域名中的 HTML、MW 等标识符来定位到相应的网页。

另一类是形式符号，比如书名号可以帮助我们快速定位到书籍，"@」符号可以帮我们快速定位到邮箱，等等。这里也提醒大家，在使用 Ctrl-F 时，在 Excel 等特定文件中还有更多功能，比如支持查找后进行替换。同时，有些文件是纯图片格式，这时使用 Ctrl-F 是不支持的，需要先使用文字识别工具将其转换为文字。

刚才我们提到的都是在一个屏幕内使用 Ctrl-F 的技巧进行定点查找，而在我们的电脑内部还有更多文件，如应用程序、文件夹等，我们可以借助 Command+F、Space 等快捷键进行一键查找。这是 Mac 系统提供的检索工具，包括了 PDF 文档、应用程序、图片、文件夹等。在整个 Mac 电脑系统中，我们就可以使用 Command+Space 这样的快捷键来进行检索。

在实现文件检查的同时，我们还可以借助一些相应的软件实现特定功能。例如，在 Mac 系统中可以使用 FindAir 等软件，在 Windows 系统中可以借助 AirportPower 软件，还有一些聚合搜索软件可以实现在多个页面中进行一键查找。

接下来，让我为大家进行一个具体的文件查找演示。比如说，我想找一份空飞同学之前整理的文件。这时，我们可以借助 Mac 自带的文件搜索功能，按下 Command + 空格键，就会出现一个搜索框。在搜索框中输入「信息分析」，稍后就会显示我们要找的内容，包括文稿、电子表格等与信息分析相关的内容。

由于系统支持多种文件类型的搜索，我们可以在系统偏好设置中的「聚焦」选项进行查看和设置。如果不想某些类型的文件出现在检测结果中，可以将其关闭。同时，这里还有隐私设置功能，如果不希望某些私密内容出现在检测框中，可以将相应文件夹添加到排除列表中。这是 Mac 自带的搜索功能，可用于检索文件、PDF 等多种格式。

除此之外，我们还推荐了两款程序。在 Mac 系统中，你可以使用 EasyFinder 软件，它与聚焦搜索略有不同，可以提供更高级的检索功能，而且速度快、占用内存少。我们来对比一下：首先使用 Mac 的聚焦搜索，然后打开应用程序 EasyFinder 进行新的检索分析，可以看到它速度很快。同时，该软件还支持文本内容的查看，不仅能搜索文件标题中的相关内容，还可以通过选择「Google Contents」选项来搜索文档内部的具体内容。

这些内容的信息分析页面中，信息分析的内容也会被我们检测出来，出现了比之前更多的结果。它不仅提供了 Pulse 标题，还能检测出内容中的信息分析文件。

另一款软件是 Alfred，它是一个 Mac 系统的生产级工具，功能相对更加强大。它提供了一些特定的搜索功能，还有许多扩展的工作流程，你可以将其导入来提高工作效率。启动 Alfred 后，使用 Option + 空格键的快捷方式就可以打开。当你想检索文件时，可以输入快捷命令「open」加上文件名称进行检索。

在打开网页时，Alfred 还支持一些特定的搜索功能。比如说，你可以设置豆瓣的快捷搜索。我设置了豆瓣的快捷搜索后，只需输入「douban」加上要检索的内容（如「人生模式"），它就可以直接在豆瓣中进行检索。

在 Windows 系统下也有相应的工具，这里就不再演示了。今天的分享就到这里。



### CH1103实践策略2信息获取P05批量下载

我们来学习如何批量下载朋友圈转发的文章。虽然朋友圈里的文章很多是鸡汤文，但偶尔也会有一些优质的科普文，比如关于德鲁克的《自我管理》这篇经常被转发的文章。在这篇文章中，德鲁克提到了一些新的概念，如自我管理、回馈分析法、读者心听者心的学习方式、价值观、价值体系、管理上市等。

很多人觉得这篇文章不错，那么读完之后他们会采取什么行动呢？每个人读完文章后的处理方式其实不太一样。这里介绍五种不同的处理方式：

第一种是最简单的，就是点击右上角收藏。虽然觉得内容不错想留着以后再读，但通常收藏后就不会再读了。

第二种是进一步了解，比如好奇德鲁克讲的自我管理具体是什么意思，会去百度或知乎搜索相关信息。

第三种是具有更好的学习习惯，会去查找文章的原始出处。比如发现这篇文章最早发表在《哈佛商业评论》上，并收录在《哈佛商业评论：自我发现与重塑》这本书中。

对于想要保存书籍元数据的读者，我们可以借助 Zotero 工具及其插件。安装好 Zotero 和相应插件后，我们就可以通过 Zotero 在豆瓣等网站上批量获取图书元数据。

现在给大家演示一下具体操作：首先打开豆瓣，搜索我们刚才提到的这本书。找到需要保存的图书后，因为已经安装了 Zotero 插件，我们可以直接保存数据。注意在保存之前，需要先打开 Zotero 软件，选择要保存到的目标文件夹，比如「小电脑录」文件夹，然后再进行保存。完成这些步骤后，我们就能看到图书数据已经保存成功了。

图数据包含书籍、标题、材料、日期、出版社等信息。在找到自我发现的童书后，发现这是一本论文集。如果想进一步了解德鲁克，我们会搜索他的书籍。比如说这里看到德鲁克评价最高的文章，我们可以使用 Telcel 插件保存下来。

仅学习一本书可能不够深入，我们可以尝试找到德鲁克所有的书籍进行系统学习。这时可以借助豆瓣的豆列功能，在搜索引擎中搜索相关项目。豆瓣豆列的地址是 douban.com/doulist，在这里可以找到一些德鲁克的豆列收集。对比前两个「by 德鲁克」的豆列，我们发现第二个收集更为全面，还包含一些其他参考资料。

这时可以使用 Telcel 插件进行批量获取。插件会显示为文件夹的形式，点击后选择全部确定，它就会开始保存整个页面的书籍信息。我们可以看到书籍在逐一保存，包括整个页面和下一页的内容。全部保存完毕后，这些数据就会形成一个实体。现在回过头来看，已经保存了刚才豆列中的所有数据，这体现了数据的保存功能和批量保存功能。

掌握了这些方法后，第四步最重要的是要能够运用到生活中。有些人在完成前面的操作后，认识到这些内容对自己有用，并制定了相应的行动计划，将这些内容应用到实际生活中。

第五种方式是较为反常识的，也是课程所提倡的：从一个点出发去学习整个领域。这种方式虽然非常耗时费力，但作为一名高级的信息分析者，你可能会想起我们之前提到的全局认识模型：整体大于部分，数量大于半数量，高层次超越低层次。当我们着手建立全局认识时，至少有两个明显的好处。第一个是当我们在朋友圈看到自我管理这样一个话题时...

这种学习方法至少有两个非常明显的好处：第一，当我们在朋友圈看到「自我管理」这样的词时，往往并不了解它的来龙去脉，也不清楚它在什么样的情境下被提出，适用于什么场合，以及它与其他相近的观点和知识点有什么区别。这时，如果我们运用第一步方法，通过平常学习建立知识圈子，将具体的知识点放在知识的集合中去理解，就能实现整体大于部分之和的效果。这种看似复杂的操作，实际上反而让我们对知识的理解更容易，记忆更加深刻。

第二个好处是有利于未来的应用。目前你可能只接触到自我管理的知识，但在未来可能会接触到团队管理等其他相关内容。如果使用这个方法去学习整个领域，你就能建立起一个宏观的高层次体系。当下次出现相关知识时，你就能更准确地判断并更容易理解。换句话说，这种方法虽然在第一次学习时会花费较多时间，但会节省未来的大量时间。当你用这个方法完成学习后，遇到新的信息点就可以将其放入自己已建立的知识体系中，使知识体系更加完善强大。

当然，大多数读书人还是会偷懒，不太习惯使用这种全局的学习方法，认为只读一两篇文章就足够了。在这里再次建议大家一定要亲自尝试，才能感受到这个方法的强大。虽然初次尝试时可能会花费较多时间，但像杨老师这样的高级学习者，完成一个领域的快速学习可能只需要几个小时的时间。

那么如何学习整个领域并建立体系呢？我们可以从「千篇一律」开始。"千篇一律」是学习整个领域的一个建议法则，即要获取一个完整领域的全貌，至少需要阅读一千篇相关论文。这是我们之前提到过的。这样做的原因是知识的产生通常源于......

知网文献共同体作为一种协议和知识权利认证，可以实现文献全文的下载。那么具体来说，当我们需要保存 1000 多篇论文数据时应该怎么办呢？有同学可能会想到使用 Zotero 的在线批量获取功能，但是当数量比较大时，通过 Zotero 在线抓取的效率会比较低，并且容易出错。最好的方法是借助中国知网（CNKI）将第三方网站提供的可快速导出文件作为中介，再导入到 Zotero 中，这也是课程组比较推荐的方法。

具体操作步骤有三步：
1. 使用知网进行文献检索
2. 将检索结果导出为 RefWorks 格式文件
3. 将文件导入到 RefWorks 中接下来进行具体演示：
首先打开中国知网，通过学术知网入口进入。知网提供的数据库包括文献、期刊、图书、学位论文、会议论文等全面的资源。我们可以在文献库中检索对应学科，以「自我管理」为例，它属于经济管理科学下的管理学二级学科。初步检索结果显示有 17 万条记录，数量较大。我们将范围缩小到博士论文，结果显示约 200 多篇。考虑到博士群体是领域内最年轻的研究群体，代表着领域的未来发展方向，我们可以选择博士论文作为研究对象。同时选择硕士论文，共计 2000 多条结果，这个数量比较合适，可以开始进行文献导出。

知网支持批量导出功能，单次最多可导出 500 条记录。在此演示中，我们选择导出 200 条记录，每页显示 50 项，依次选择第一页到第四页的内容。选中所需文献后，在导出选项中选择 RefWorks 格式，这是 Zotero 支持的格式之一。

我们选择这个格式后导出结果。这里是 Zotero 提供的一个快速的文献导出数据。现在我们将其导入到 Zotero 中，我们想把它导入到 MyLibrary 下面。可以先建一个子分类「管理」，"中国」。我们现在进行文件导入，刚下载的文件在默认情况下会保存在「下载」文件夹里。可以看到它已经开始进行导入了。

导入完成后，这是新建的文件。我们可以进行调整，这就是我们平常导入的 200 篇博士论文。它主要提供的数据包括：条目类型（期刊论文）、标题、作者、日期、DOI 码以及添加日期。这里主要是用于查看我们给予平台导入的摘要。

这就实现了一个 Retail 的导入。导入完成之后，这里也提醒大家在下载整个领域的文献数据时有几点注意事项：

第一，我们要按照学科的线索去检索整个领域，而不是依赖某一个关键词。这个策略最特别的地方在于，不是下载了若干篇论文就可以了，而是要通过这些论文去尝试构建一个更高维度的全局认识。你不必细致入微地去学习每一篇文献，但是通过这样的下载，你可以快速了解应该利用哪些重要的文献，以便在需要时可以快速确定去哪个学科寻找对应的知识。

在实践过程中，有些同学可能习惯使用单一关键词去搜索，比如检索人工智能时仅使用「人工智能」这个词去检索下载整个领域，这当然是个误解。实际上，以人工智能领域为例，仅深度学习这个方向就存在「深度神经网络」等多个关键词。即便要用关键词搜索，也一定要使用多个关键词组合去搜索。相反，如果我们通过找到人工智能相关学科，做一些关键词查询，找到更多在这个领域 40 年内最有影响力的人物，这样记住整个知识体系的概率会高一些。当你下载完这些论文之后，通过一定的分析和对比进行整理，就能快速找到这个领域最重要的 20% 的知识。

第二个注意事项是学科中 [音频未完待续]...

如果论文过多或过少应该如何处理呢？在刚才的案例中，管理学科的论文明显过多。正如数据演示所示，对于数学学科的颗粒度处理方式，首先我们可以检索博士论文。如果博士论文数量较少，那么可以进一步将数学论文纳入；相反，如果整个领域的论文数量过少，这也意味着该领域的高质量论文可能不够充足，这时我们可以扩大范围，比如先加入硕士论文，再进一步纳入学术期刊文章。

另一个方法是检索英文论文，因为英文研究往往比中文研究更加丰富，质量也更高。这里向大家推荐一个建议法则：下载论文的合适数量是一千篇，因为只有阅读至少一千篇论文后，才能形成系统的学习优势。而在大多数情况下，博士论文库的论文数量就已经超过一千篇。

第三点是获取领域论文的另一个思路，我们可以借助领域大牛的资源。在此做一个简单演示：比如我们选择了一位学界大牛 Schloovich。很多学者会将自己的论文放在个人网站上供大家下载。以 Schloovich 为例，输入「Schloovich homepage」（即主页的意思），就可以快速定位到他的主页。可以看到他在主页上确实提供了关于推理的文献下载。

关于如何批量下载，我们可能会想到使用 Onetab，但它虽然显示文件夹却信息不够完整。这时我们需要借助谷歌的下载工具 DownThemAll，它可以提供网页文件的批量下载功能。在连接处选择「文档」选项，将其保存到指定文件夹，点击下载即可开始批量下载。目前已经显示共有 82 篇文章，现已下载 42 篇。通常情况下，我们只需等待下载完成即可。让我给大家展示一下已经下载好的论文数量，可以看到已经下载了很多，而且还在持续下载中。

当我们下载到讲师的论文后，可以将其导入到主库中，重新汇入其中的原始数据。这是一个频繁抓取论文的示例。我们可以通过这种方式找到一个领域的论文，这是通过杨老师提到的方法找到该领域的大牛。我们也可以通过一些领域的学者排行榜，比如 H 指数排行等指标来定位该领域的大牛，再通过检索大牛的姓名加「homepage」来查找其官网，看是否提供了论文下载。如果有，我们就将其批量下载下来。这就是我们关于如何通过大牛建立一个领域的全部内容。

在演示过程中，我们使用了 WMOO 这个插件，它可以实现文件的批量下载。比如刚才我们下载 PDF 文件，当然它还支持更多类型文件的下载，例如图片。下面为大家进行演示：我们打开一个图片网站，想要下载一些关于 3M 的图片用于科研制作。我们在搜索框中输入 3M，就可以看到一些相关图片。接着我们使用 DownloadAll 进行下载，可以看到点击 DownloadAll 后有四个选项可供选择：

1. DownloadAll：下载当前标签页的可下载文件
2. DownloadAll（可依）：采用上一次的配置下载文件
3. DownloadAll 插件：下载浏览器中打开的所有标签页中可下载的链接
4. OneClick：采用上一轮的配置下载标签页这里我们以第一个选项为例。点击 DownloadAll 后，左上角会出现「链接」和「媒体」选项。选择「链接」只下载所需的链接会更快，当你只需要保存链接时可以选择这一项。这次我们选择「媒体」来下载图片。在媒体选项中还有筛选器，可以选择下载所有文件（这里有 234 项可下载），也可以进行筛选，比如只下载音频或只下载图片。

对于图片下载功能，我们可以根据需要进行格式筛选。除了格式筛选外，该功能还支持关键字筛选。例如，如果我们想搜索雪山相关的图片，可以在快速筛选栏中输入「雪山」，系统会显示符合条件的图片数量。

接下来，我们可以设置下载文件的保存位置。比如将文件保存到「雪山」文件夹中，点击下载按钮后，系统会发出提示音表示下载完成。我们可以在 Mac 系统的下载文件夹中查看已下载的图片。

关于标签页的使用，如果我们访问另一个图片网站并想下载图片，可以选择「当前标签页下载」或「所有标签页下载」。选择「当前标签页下载」只会下载当前页面的图片，而选择「所有标签页下载」则会下载所有打开的标签页中的图片。此时可以看到 VT 界面会显示所有相关标签页的链接。

我们可以根据需要为下载的文件设置不同的保存名称。例如，可以将文件保存为「雪山 2」或「山雨树」等名称。下载完成后，可以在相应文件夹中查看已下载的图片。

这是 Dunlop 工具的一个实用场景，主要用于批量下载图片。相信还有更多应用场景等待大家去探索发现。

最后需要提醒大家注意：在进行批量下载时，一次性下载过多链接可能会被网站封禁。建议大家采用分批下载的方式，每次选择合适数量的文件进行下载，以避免这个问题。




### CH1104实践策略2信息获取P06智能获取

让我们来看一遍标准招式 —— 证明获取。一休比较爱读书，有一天他在读《超越智商》时觉得比较难，想看看其他人是如何理解这本书的。于是他打开豆瓣查看了大家的评论，发现这些评论写得很好，想把它们保存下来。然而，他发现没有现成的文件可以下载，而评论有 700 多条。这时该怎么办呢？我们可以使用秒招「自动获取」。

自动获取是指当没有现成文件可下载时，利用爬虫软件或 RPA 技术从网络中获取信息，比如将书评整理成文件保存到本地。我们可以使用商业爬虫软件八爪鱼来自动获取豆瓣的书评。

在介绍八爪鱼之前，我们先了解一下什么是爬虫。首先要明白爬虫的本质是模拟浏览器从服务器上批量获取数据。我们使用爬虫的真实目的就是想要模拟浏览器从服务器上获取大量数据。

要理解浏览器和服务器是什么，我们需要了解上网背后的逻辑。这里涉及 HTTP 协议，我们的不同电脑之间、服务器之间以及数据之间都是机器，它们会遵循协议来进行交流。比如我们平时都知道，打开豆瓣界面时会存在很多个子界面，打开豆瓣后输入网址就会进入到相应的界面，不同界面有不同的效果。

这个过程是这样实现的：当我们输入或点击一个网址（URL）时，它会指向不同的位置。在这个过程中，作为用户，我们的电脑与服务器之间会有一个交流过程。当我们点击链接后，电脑会发送请求给服务器，服务器会向数据库请求数据（因为服务器本身不储存数据），然后数据库将数据返回给服务器，服务器再将数据发送给我们的电脑。

电脑获取数据包时，由于数据包可能全是数字和代码，无法直接查看，需要经过处理后传递给浏览器（如谷歌浏览器）。浏览器再对数据进行处理和美化，最终呈现出我们看到的页面样式。这就是一个数据获取的过程。

如果我们要进行爬虫，实际上就是模拟浏览器向服务器请求数据。因为服务器无法分辨是人还是浏览器在操作，如果是真实操作就需要打开浏览器访问网站，而现在我们把这个动作交给爬虫来批量执行。这就是爬虫的工作原理。爬虫的工作其实很简单，就是抓取网页内容，比如我们可以编写爬虫去抓取豆瓣的书评。

在整个互联网中，有一个我们经常使用的巨型爬虫，就是搜索引擎。以百度为例，它本身就是一个巨型爬虫，需要成千上万的工程师来维护。当我们在百度中搜索「爬虫」时，它会显示很多结果。这是因为百度一直在不断地抓取所有网站的内容，当用户输入关键词搜索时，它会将抓取的网站内容与关键词进行算法匹配。所以它的前期准备工作就是一个巨大的爬虫工程，将众多网站的内容爬取后再返回相应的数据。

这里我们为大家介绍的是一款商业爬虫软件。它具有美化界面、丰富的教程和现成的模板，便于上手，所以推荐给大家。简单来说，只需要三步：选择模板、填写信息（类似填空题），就能完成数据采集，无需编程知识。当然，它还提供一些自定义模板，适用于复杂场景，操作也比较简单，不需要很多编程知识。

接下来我们将为大家演示，我们直接打开这款软件的网站链接。在爬取书评之前，我们再补充一个知识点：我们看到的网页是经过 HTML 处理后的美观页面，而在其背后是一些更原始的数据，这时我们可以借助 Google 浏览器来查看。

在浏览器中查看网页源代码时，可以通过右上角的三个点选择「更多工具」，然后选择「开发者工具」。通过开发者工具，我们可以查看网页的源代码。当点击某一行代码时，对应的网页内容会以蓝色高亮显示。比如，点击对应「豆瓣图书」的代码部分，网页中的「豆瓣图书」文字就会高亮。我们可以进一步展开代码，看到更详细的内容。同样，我们也可以在网页中选择文字，右键选择「检查」，开发者工具就会自动定位到对应的代码位置。

这些代码包含了出版社信息和链接路径等内容。与网页美观的界面相比，代码更接近底层，主要用于获取文本和链接。爬虫的基本工作原理就是通过直接读取代码来获取所需的文本和链接信息。

接下来进行演示。我们已经安装了爬虫采集器，这需要注册登录。在首页可以看到各种采集模板，包括豆瓣电影评价、发帖、图书等项目。点击评价后可以看到模板使用说明，主要针对图书和电影两个版块。使用方法是输入翻页次数和网址，就可以开始采集。

我们还可以尝试其他采集器，比如最后一个更详细的频率爬虫，可能需要输入用户名和密码。我们可以尝试输入「超越智商」这本书的信息。为了保护隐私，可以选择输入虚拟账号。根据统计，大约有 782 条评论，每页显示 20 条，需要爬取 40 页。我们可以设置爬取 40 页，然后输入图书首页的网址进行采集。

填空器填写完成后，点击「存币启动」按钮，在采集选项中可以选择「本地采集」（因为其他采集方式可能需要付费）。点击「立即启动」后，爬虫程序便开始模拟人工操作：首先打开并访问目标网页，然后进入登录界面。

在登录界面，可以选择密码登录方式，输入预设的手机号和密码。如果需要，也可以点击「暂停」按钮，通过扫码或输入密码登录自己的账号。登录过程中可能需要完成滑块验证，可以选择手动操作或由程序自动处理。虽然验证过程可能不够智能，系统会有一定延时，但验证完成后就会自动跳转并开始数据爬取。

程序会自动进入短评论页面进行数据采集。爬取的数据包括：主编名字、主编 ID、主编面积、爬取时间、评论价格、评论者 ID、评论者主页、评论内容、发布时间、点赞数、评分等信息。在日志界面可以查看程序运行状态，包括登录、采集等操作的记录。

相比手动操作，这种自动化爬取方式更加便捷。虽然过程可能较慢，但非常高效。以实际案例为例，一次完整的爬取用时 28 分钟，采集到 220 条数据。爬取完成后，可以选择导出数据，支持 CSV 格式导出。导出的数据文件包含作品名称、链接、ID、发布时间、评论者 ID、短评内容、发布时间、用户数、评分等完整信息，便于后续进行数据处理和分析。

这个爬取过程操作简单，用户可以根据需要自行尝试使用。

感性无关性的进一步练习中，我们有时可能想要爬取一些较长的评论。比如说，当面对 80 条较长的书评时，我们也想对其进行抓取，那该如何操作呢？

首先，我们可以查看软件提供的模板。在停止运行后，可以发现在提供的模板里并没有产品选择功能。此时，我们可以使用自定义功能进行选择。这里建议大家查看教程，因为它提供了模板采集和自定义采集两种方式的说明。我们可以先查看中文教程，在教程中直接找到相关场景说明，比如读书评论采集的具体操作步骤，然后学习并进行相应调整。

以长篇评论采集为例，操作步骤如下：
1. 制定新任务，输入网址（即图书的网址作为爬虫起点）
2. 新建任务组（超越指上商品），保存设置
3. 系统会自动打开网页并进行识别，抓取一些初始数据在这个阶段，如果不想使用自动识别，还可以切换识别效果。如果需要抓取页面中的其他数据，可以直接进行深层采集。

第一步是打开网页，第二步是打开产品页面并选择。我们需要告诉爬虫软件要点击具体的网页位置，它会进行实时识别。爬虫会识别网页代码并匹配相关文字内容。此外，系统还会生成翻译采集功能。如果当前识别结果不满意，我们可以切换其他选项。

最后，我们可以进行采集设置。可以先取消某些采集项，也可以进行自定义试验。例如，如果要查看每条评论，包括用户名、标题和摘要等信息，我们可以自行选择和设置共享范围。

由于我们需要提取页面中的特定元素，可以选择指定元素进行抓取。这就是我们已经获取的 tablet 内容。由于后续内容格式相似，我们可以选中全部要采集的内容。

回顾自动识别的结果，可以发现它并未采集数据标签。我们可以先勾选「生成采集设置」和「侦测采集设置」选项，然后通过模拟人工操作的方式对网页数据进行抓取。我们可以通过观察网页数据的变化情况来判断数据是否正确，并对每个步骤进行相应修改。

在界面中，我们可以看到缺少一项数据。点击相应位置选择「采集概念数」后，文本框中会增加一个采集概念数选项。接下来，我们可以对需要采集的内容进行处理：标题及其链接、图片、用户名称及链接、用户 ID、发布时间、内容等项目都没有问题。对于不需要的展开选项可以删除，点赞数（同意数）、回复数可以根据需要选择是否保留，这些就是我们要采集的所有内容。

系统已经设置了循环翻页功能，我们可以点击每个框查看对应元素。这个框用于采集框内元素并提取列表，还包含「下一页」的翻页功能。点击后可以实现实际翻页。循环翻页的设置是基于特定地址的，这个代码对应着相应的链接。我们可以选择使用系统默认的获取方式，修改后依然可以正常运行。

完成设置后，我们可以保存并开始采集测试。首先显示网页，打开我们设置的地址和评论，然后开始采集工作。采集完成后，我们可以将数据导出为 Excel 或 CSV 格式并下载保存。打开保存的文件，我们可以看到所有需要的数据都已经保存下来，包括标题、用户名称、用户 ID、发布时间、内容、点赞数等信息，这些数据可以进行进一步分析处理。

在进行数据分析和浏览之后，刚才我们讲解了制定过程。当然，更简单的方法是直接采用其提供的模板。该网站提供了丰富的采用模板，大家可以点击查看说明后进行使用。如果想进一步熟悉使用方法，可以查看其上面的教程，也可以到官网查看帮助文档。网站提供了大量图文教程和视频教程，包括京东、唱片、地表彩底等内容，大家可以进行进一步学习。

最后给大家分享一个彩蛋：在进行数据新闻的数据获取之后，我们还可以对数据进行加工。比如说，我可以对下载的数据进行最简单的词性分析。这是我利用词性分析软件进行分析，可以看到文本中出现最多的词性其实是形容词。当你回到这些日期时就不会出错，当然你也可以查看具体改进了什么，然后进行阅读。有时这里会提到贝斯（Base）等内容，以及关于预算（Budget）的介绍，感兴趣的话大家可以自行练习。

刚才我们介绍的是一款爬虫软件，还有更多的爬虫软件，比如 Carrot 爬虫软件、WebSquare 以及火车头等，都可以去自行探索。除了爬虫之外，还有 API 接口。像我们的爬虫更多是做一些常规的数据获取工作，但功能还不够完善。比如说，如果我只想要那些好评或者高分的书评，爬虫就无法进行判断。这时我们可以使用 API 接口，它可以进行智能判断，就像我们人类可以判断哪些是需要的、哪些是不需要的信息。

除了数据获取之外，它还能完成更多我们难以完成的工作，只要告诉它应该怎么做。比如在生活场景中的抢购活动，我们需要在可以抢购的时间点去点击购买然后完成下单，这种人工操作其实比较费时费力，而且有时抢购活动时间较晚。这时我们就可以把这种任务交给程序来完成。

工作可以交给 Apple 软件处理，在按钮可以点击时按步骤填写相关信息即可完成下单。这里给大家介绍一下开智同学所在的两家公司。他们公司开发的一款 RPA 软件可以对大众点评进行数据抓取。由于该软件还不太成熟，一直在进行优化，这里已经逐步演示和介绍。

工作益公司也是专注于 RPA 技术研发的企业，已经开发了多个机器人应用。他们公布的应用场景包括大众点评评论抓取等，这些都是普通爬虫难以实现的功能。大家可以尝试使用这些工具，但由于技术层面还不够成熟，且仅支持 Windows 系统，我们就不做具体演示了。如果大家感兴趣可以自行尝试。

最后需要提醒大家，在使用工具获取数据之前，请务必遵守网站相关条例规定，在合法合规的范围内进行操作。一般来说，抓取公开可见的数据不违法，但如果网站明确声明禁止爬虫或拒绝数据采集，继续爬取将存在法律风险。同时提醒大家，请勿使用爬虫或 RPA 技术抓取个人信息、法律禁止的信息，或在网站明确禁止的情况下获取数据信息。

本节小结：希望大家在信息获取过程中，能够学会识别信源、快速验证、动态调整和批量下载等技能，为信息分析打好基础，做好获取信息不对称的第一步。现在我们开始练习吧。

### CH1105实践策略2信息获取P07爬取薪酬数据

医生作为一个喜欢思考的求知者，会想到能否通过获取这些数据，进一步了解不同发展阶段的公司、不同地域的公司以及不同经验年限的工作人员之间的工资差异。要实现这个目标，我们需要获取更加细致的数据并对其进行深入分析，这就需要用到我们这一节要学习的数据爬取和处理知识。

在此，我要隆重向大家介绍一个实用的谷歌插件 ——WebScriber，它被称为「无需编码的爬虫工具」。简单来说，爬虫是一种用于批量抓取数据的程序。传统上，批量抓取互联网数据通常需要编程能力，而 WebScriber 则做出了重大创新：用户只需要能够从网页源代码中识别规律，明确自己的需求并掌握一些基本规则，就可以获取大量网页数据。这个插件最大的优势在于不需要编程知识就能够有效地获取和分析数据。

让我们开始学习这个实用工具。首先在谷歌中搜索 WebScriber，第一个搜索结果是它的官网，第二个是谷歌的插件安装页面。完成安装后，我们可以回到 WebScriber 的官网查看详细信息。官方文档提供了全面的使用指南，包括安装说明、系统环境要求、使用方法等。

在基础使用介绍中，主要包含以下步骤：

1. 创建 SizeMap（对于多个 URL 有多种创建方式）

2. 创建选择器（文档中提供了具体案例，展示了如何在新闻网站页面中处理多个词链接）

在具体的新闻页面中详细介绍了 WebScraper 的工作原理。首先需要设置起始 URL，然后在起始 URL 下使用 LinkSelector（链接选择器）选择所有相关链接。针对每个页面，再使用 TextSelector（文本选择器）来抓取页面中的数据。

分析可知，获取这样一个网站的最小单元需要两个步骤：一是获取页面中的链接，二是获取该链接中所需的最小元素。有了这样的最小单元后，只需要不断循环重复操作即可获取所有页面的数据。

WebScraper 提供了多种选择器，官方文档提示至少需要了解三个核心选择器，这三个选择器在后续操作中都会用到。其他选择器可以在左侧查看。对于相对简单的网站，使用前面介绍的三个选择器就足够完成数据抓取；而对于较复杂的网站，有时需要借助更特殊的选择器才能实现数据抓取。

完成前期设定后，需要检查整个爬虫以确保数据无误，最后开始爬取网站数据并可导出为 CSV 格式。现在我们对 WebScraper 已有初步了解，接下来将进行实际操作演示。

本节的主题是抓取新闻数据，整个流程分为三步：第一步是使用 WebScraper 爬取数据，第二步是数据预处理，第三步是数据分析。第二步和第三步可以使用常用的办公软件来处理，当然如果有其他更好的数据处理工具也可以自行选用。

接下来将以拉勾网为例，演示如何使用 WebScraper 批量爬取数据。进入拉勾网后，例如我们想要抓取产品经理的相关数据，可以看到搜索结果页面有多种筛选方式。

在每个企业的数据展示中，包含了岗位、工作地点、发布时间、薪资范围、工作经验要求、学历要求、公司名称、公司发展情况、公司所属阶段、公司规模以及公司福利等信息。每个企业都按照这样的信息结构进行呈现。在最后一个页面中，数据共有 30 页。如果我们想批量获取每个企业的这些数据以及所有 30 页的数据，可以通过以下操作实现：

首先调出 WebScraper 工具，通过右键选择「检查」即可看到 web scraper。我们需要先创建一个 site map，可以将其命名为「拉勾数据」（名称可以自定义），然后输入起始 URL 并保存，这样就完成了项目的创建。

接下来进入第二步：创建选择器。这是使用 WebScraper 最具挑战性的部分，但实际并不复杂。对于抓取拉勾网页面的数据，我们需要通过点击 1 到 30 页的「下一页」按钮来获取每一页的数据，然后在每一页中获取各个企业的信息。

具体操作步骤如下：

1. 添加第一个选择器，实现点击下一页的功能。由于需要抓取的数据涉及翻页和进入二级页面，我们选择「Element Click」选择器。

2. 首先选择整个页面范围，在此范围内选择「下一页」按钮。

3. 设置为多次点击，并将特殊设置改为 css select，选择 multiple 以实现重复操作。

4. 为选择器命名（如「link」，可自定义）并保存。

5. 通过数据预览（data preview）功能检查结果，可以看到页面实现了翻页，说明我们已经达到了预期目的。

需要创建下一个层级的选择器。这里的操作步骤请大家注意：首先我们要点击进入 link 的二级目录，然后再添加一个选择器。此时，我们就进入到每一个企业的信息集合里了，因为要抓取的岗位名称等数据都在这一整个区块之下。

首先创建一个区块 element，然后选择这个区块。在移动鼠标时，它会不断更换范围，我们需要花一些时间，小心地选中相应的数据。选中想要的范围后，我们选择数据预览。关于区块预览的作用需要解释一下：在页面中，如果只选择了一个区块，勾选 multiple 选项后，它就会将页面里类似区块的数据都抓取下来，所以我们都需要勾选 multiple。

勾选后发现它依然没有抓取剩余的区块，这需要一点推理：刚才那串字母实际上影响了我们抓取整个数据，所以我们把后面那一段删掉。删除后发现可以获取到多个区块的数据了。这里我们将它命名为「区块」或「公司」，并确保它是在 link 之下。

保存后，我们可以看到根目录下有 link，然后是 element。继续点击 element 进入下一层选择器的设置。这一层比较简单，只需要获取每个区块内的多条信息。我们需要的选择器是 test，选择相应信息。首先获取公司名称并勾选 multiple，将其命名为「commy」并保存。可以看到数据已经成功获取。同理，我们继续获取其他数据，比如职位信息等。

完成职位选择后，我想要获取地址信息。这里需要注意的是，虽然我想单独选择某个元素，但最小选择单位只能是整行数据。因此，我们只能选择整行数据，虽然这一行包含多个数据项，但不用担心，我们可以在后续对数据进行处理。

这里显示的是公司服务率的说明，而我们最关注的薪酬数据实际上包含在之前的 requirement 部分中。我们可以预览一下，确认所需的薪酬数据已包含其中。

设置完成后，我们来检查整个设置情况。选择「Select Graph」选项，这是一个图形化展示界面。点击按钮后，我们可以查看之前设置的逻辑：首先抓取链接，然后在每个组块下分别抓取公司、职位等信息。

接下来我们可以开始批量抓取了。在 scraper 设置中，有一个延迟选项，比如翻页后需要等待 2000 毫秒才能进行下一页的抓取。设置延迟的主要目的是应对网站的反爬虫机制，因为如果抓取速度过快，可能会触发验证码或被封禁。根据不同网站的情况，我们可以调整这个设置，这里我们使用默认设置即可。

选择开始抓取后，可以看到整个页面在不断跳动。在这个过程中，我们不要干预，以免影响数据抓取。抓取完成后重新加载数据，我们发现数据有些错乱，需要回去调整设置。

检查后发现，每个 Element 下的 multiple 选项都被开启了，但实际上我们只需要它内部的数据。由于每个 element 已经限制在它的组块之内，所以 multiple 选项是不需要选择的，我们将其全部修改即可。

当然，这里的处理需要一些经验，不太熟悉的情况下可以多尝试几次。由于 WhatsSQL 爬取数据的速度非常快，所以多试几次就可以了。我们重新调整后再次爬取数据，看看这次是否正确。在完成这次数据抓取后，我们刷新页面可以预览到抓取的数据。我们发现这次终于正确了，可以选择导出为 CSV 格式将数据下载下来。

到目前为止，我们完成了第一步的数据抓取。通过刚才的操作，我们已经成功爬取了 30 页的老公网数据。大家可以根据自己的需要去爬取更多数据。

接下来我们进入第二个步骤：数据预处理。数据预处理是指对爬取下来的数据进行处理，因为如果直接进行数据统计分析可能还不够完善。比如我们获取的数据在一行里面集合了三种不同类型的数据，我们需要把它分离成不同列的数据才可以进一步处理。

具体操作如下：首先打开下载好的拉公网数据。这里需要提醒一个关键步骤：由于这是 CSV 格式的文档，我们一定要将它另存为 Excel 格式的文档。这样当你新增更多表格以及做一些格式化处理时才可以保存下来，因为 CSV 是纯数据格式，无法保存多个表格和多种格式。

保存为 Excel 格式后，我们可以看到这里有 H 列数据。第一列数据是 WhatsSQL 自动生成的随机码，我们对它进行从小到大的排序后就可以获取数据最原始的次序。我们可以看到第一个数据是自如网，第二个是连接生活，已经按照原始次序排序了。原先的表头被放到了最后一行的位置，我们需要使用快捷键 Ctrl+X 将它放回第一行的位置。

在操作过程中，多使用快捷键可以提升处理效率。

对于数据处理，我们首先注意到 Square Pro 开始的 URL 都是相同的，这两列数据没有分析价值，可以直接删除。接下来观察到有些单元格同时包含了多个信息：一列包含公司范围、经验范围和学历要求；另一列包含所处行业、发展阶段和公司规模。还有一列是公司的吸引力或说服力描述，由于后续分析用不到，可以不进行处理。

我们重点需要处理这两列多重信息，将它们分离成不同的列。以薪资信息为例，比如「20K-40K」，因为 Excel 只能对纯数字进行计算，所以需要将其分成 20 和 40 两列。

在处理过程中发现，点击某些单元格时，数据存在换行现象，这在 Excel 中难以直接处理。解决方案是：先新建一个表格，将数据复制到新的空白工作表中，这样可以清除原有的数据格式，使其变为纯文本。重新复制粘贴回原表格后，数据就变成了单行格式。

接下来，为了进行数据分列，需要使用特殊符号作为分隔符。我们先将空格替换为竖线符号，选择「匹配单元格」后进行全部替换。然后使用数据分列功能（在 Data 菜单下），选择「其他」分隔符，输入竖线符号，这样就可以将一列数据分解为三列。

对于工资信息，在进行类似处理之前，需要先增加几个空白列，避免分列后的数据覆盖后面的内容。然后将「k」（包括大小写）替换掉。

我们使用大写符号将工资数据进行分离，得到工资的最小值、最大值、工作经验和实力等信息。稍后我们可以将这些数据重新粘贴回去。同理，对于另一列数据，我们先按照左斜杠进行分裂，将独立的列分离出来，然后再按照逗号进行分裂。为了使数据更加美观，我们可以将双引号替换掉。

整理后的数据呈现为一个表格，包含表头和不同的数据字段，如工资最小值、最大值、公司融资轮次规模等信息。此外还有一个用于计算的表格。获取这些数据后，可以进行多种分析，比如可以比较北京地区和广州地区在相同岗位、年限和公司条件下的工资差异，进行线性相关分析。

在数据分析方面，我们主要计算以下几个统计值：均值、标准差、最小值、25 分位数、50 分位数（中位数）、75 分位数和最大值。我们的分析对象包括工资的最小值和最大值。

举例来说，如果我们想分析工作经验为 1-3 年、在 A 轮公司担任产品经理的工资情况，我们需要先筛选出相关数据，然后使用 Excel 函数进行计算：

- 使用 AVERAGE 函数计算平均值

- 使用 STDEV 函数计算标准差

- 使用四分位数计算公式来获取最小值、各个分位点和最大值（当参数为 0 时计算最小值，参数为 1-4 时分别计算各个四分位点）

为了保证表格的扩展性，我们可以设置较大的数值范围。

9 到 B2000 的最小值从 0 开始。在这里，我们先固定行的数据，这样可以通过拖动公式来修改相应的参数。这样我们就完成了第一个数据统计。

接下来，我们可以新增一个表格来存放对比数据。在粘贴时需要选择「粘贴值」选项。这是 B 轮工作一到三年的数据。同理，我们可以对其他数据进行操作。这里是 A 轮 1 到 3 年的数据，我们可以进行同样的操作。比如说，现在选择工作 5 到 10 年的数据进行对比，先将原有数据删除，然后复制粘贴新数据。这样我们就获取了工作 5 到 10 年的薪资数据。以此类推，我们可以进行其他数据分析。

让我们来看一下分析结果：工作一到三年和 5 到 10 年的薪资差异大约是每月 6K，标准差没有太大区别。再看 25 分位值，工作一到三年的均值是 17K，最小工资是 13K；而工作 5-10 年则是 15K。可以看到，工作 1-3 年到工作 5-10 年的 25 分位点的差异其实只有 2K，而 75 分位点会相差 5K。这就是一个简单的薪资数据分析示范，大家可以根据自己的情况进行其他类型的分析。

最后提几点注意事项：

1. 我已经初步介绍了官方文档。虽然官方文档内容并不多，写得非常简明，但想要更好地掌握这个工具，最好还是要学习一下官方文档并多尝试几个网站。

2. 现在我们已经有了非常便捷的爬取数据方式，建议可以定期爬取数据，这样就能获取某个职位随时间变化的情况。

3. 我们目前使用的是无代码爬虫，但在 GitHub 上有很多开源爬虫。如果你有一些编程基础，也可以考虑使用这些开源爬虫来获取数据。

比如说像拉勾网就有这样的一个开源库，这个库是用于爬取拉勾网的职位信息。我们可以看到下面有非常详细的使用说明，所以如果你有一定的编程基础，就可以直接利用这个来爬取数据。

最后，我们这期视频也是讲解拉勾网。正如刚才提到的，如果你想要更好地掌握 Scraper，最好能够多尝试几个网站。在举一反三方面，希望大家能够自己去实践，尝试切换到其他网站去爬取你需要的数据。你会如何去快速地获取大量的新充数据？你会借助什么样的工具去爬取这些数据？你会如何去使用这些工具？你有哪些方法技巧？欢迎与我们分享。