## 0301. 对话倾听黑暗中的声音

有 4 个小结：第一个小节是解释 GPT 的对话原理；第二个小节是阐述 GPT 在各个场景下的应用；第三个小节是介绍如何更好的与 GPT 对话；第四个小节是一些实用的模板，有 16 个实用的模板。

### 3.1 你好 GPT

#### 3.1.1 多轮对话的能力

多轮对话的能力依赖于上下文编码和自注意力机制。

多轮对话的能力的限制：

1、GPT-3.5 是 2000 字的中文限制；GPT-4 是 4000 字的中文限制。

2、多轮对话的时候，容易丢失，当你的标记（token）输入越多，你丢失对话的概率就越大。所以建议尽量拆成多轮对话。

『

这里有一个关于可能由于超过标记限制而失去上下文的长而复杂的对话示例：

用户：告诉我罗马帝国的历史、主要事件和领导者，以及它最终是如何灭亡的。

AI:【提供关于罗马帝国、其重要事件和领导者的详细回答，以及导致其灭亡的因素】

用户：罗马帝国的灭亡对中世纪欧洲产生了什么影响？

AI:【解释罗马帝国灭亡对欧洲的影响，包括政治权力的分裂、封建制度的兴起和文化变革】

用户：可以比较一下罗马帝国和中国汉朝的政治制度吗？

AI:【提供罗马帝国和汉朝政治制度的比较，强调相似之处和差异】

用户：罗马帝国和汉朝时期的主要贸易路线是什么，它们对全球经济产生了什么影响？

A:【描述诸如丝绸之路等贸易路线及其在罗马帝国和汉朝时期对全球经济的影响】

用户：现在，请告诉我罗马帝国和汉朝的主要科学成就。

此时，由于对话的长度和复杂性，我可能会失去对初始上下文（即，罗马帝国的历史和领导者）的追踪。在这种情况下，你可以提醒我之前的要点或问题，以帮助我提供更连贯的回答。

』

#### 3.1.2 理解高阶意图能力

理解高阶意图的原理：

1、心智理论（一阶意图/二阶意图/多阶意图）

2、错误信念测试。

高阶意图是今晚最难的内容，也是最重要的内容。心理学家和计算机科学家，把人的意图划分为一阶意图、二阶意图、三阶意图、多阶意图等等。

一阶意图。一阶意图是小孩刚刚出生的水平，比如小孩在 0-3 岁的时候，哭的时候表示我要吃奶、我要睡觉。这里的一阶意图就体现了后面要说的非常高级的使用技巧。

二阶意图。小孩 2-4 岁会形成二阶意图，比如说：妈妈，我要喝奶；爸爸，我要吃饭。这个时候他学会明天，他要的是什么。这个时候引入了一个新的关系，多了一个真正意义上的对话人。

三阶意图。比如说：妈妈，姥爷今天跟我玩的很开心。这个时候又引入了一层新的对话关系。

这里就是理解 GPT 多轮对话最难的地方，也是最重要的地方，我们所有的高级技巧都是围绕这个知识点的。一阶意图是「我知道」，二阶意图是「我知道你知道」，三阶意图是「我知道你知道他知道」。

理解高阶意图的限制：

我们人类的意图受限于工作记忆的能力，绝大多数人的意图是三阶、四阶，有少数天才，伟大的作家，比如莎士比亚，他的很多经典作品，四大悲剧，写的做好的作品，体现出来最高的意图是 9 阶，9 阶意图基本代表了人类的极限。很多伟大的作家都是在 7、8、9 阶的样子。所以你在读伟大作家作品的时候，你会发现你在想什么作家都很清楚。但伟大作家他想什么，你是猜不出来的。你读他的作品，你很容易被他牵着鼻子走。这个原理涉及到 GPT 对话真正的底层原理，后面我们所有的技巧原理都是从这些底层原理出发的。

目前 GPT-4 的高阶意图差不多是 3.5 阶，3-4 阶左右。你让它写笑话它得意图很差，但你让它做理性推理，它的意图就很强。人类的幽默感、品味，往往需要很高阶的意图。超过 3-4 阶，你想让 GPT 干那些事，注定干不好的。

#### 3.1.3 任意转换语言的能力

任意转换语言的能力（任意转换语言的原理）：

人类语言：大规模多任务语言理解（MMLU）涵盖自然科学、社会科学、艺术、人文领域等。26 种语言中的 24 种，超越现有模型的英语表现。

机器语言：精通 Github 上目前出现过的所有编程语言。

补充：GPT 自己的绝大多数代码是用 Python 写的。

任意转换语言的限制：熟悉的前 10 种语言转化比较好。

机器语言的限制：熟悉的前 4 种语言编程比较好。

『

问：请列出你精通的前 10 种编程语言，按照你的熟悉程度。

GPT-4：

我是一个人工智能语言模型，不是一个具体的人，因此我没有精通编程语言的实际经验。但是，根据我所学到的知识，以下是一些建议的编程语言列表，根据它们的流行程度和普及程度进行排序：

Python

JavaScript

Java

C++

C#

PHP

TypeScript

Swift

Kotlin

Ruby

需要注意的是，这个列表并不是绝对的，实际上，编程语言的熟练程度和使用场景会随着时间和技术的发展而发生变化。不过，这些编程语言都是业界认可的，并在许多不同的领域中得到了广泛应用。

问：

请列出你精通的前 10 种人类语言，按照你的熟悉程度。

作为一个AI语言模型，我接受了大量不同语言的训练数据。以下是我较为熟悉的前10种人类语言，但需要注意的是，这个列表并不能代表我的"精通"程度，因为我的性能在很大程度上取决于训练数据的质量和数量。这里列出的顺序并不表示熟悉程度的高低。

英语

中文（包括简体和繁体）

西班牙语

法语

德语

俄语

葡萄牙语

意大利语

荷兰语

阿拉伯语

虽然这里列出了10种我相对熟悉的语言，但实际上我接触过许多其他的语言。我的能力在很大程度上取决于我接受的训练数据，因此在某些语言上我的表现可能不如其他语言。

』

### 3.2 化身万千的小伙伴

1、一个从不指责你的老师。这是因为受技术限制的约束，并不会发表对人类负面的东西。

2、一个理解你情绪的心理咨询师。

3、一个给你人生建议的导师。GPT 很多标注是用清单体生成的，所以给你的建议也是清单体式的。

4、一个精通考试的辅导员。

5、一个善于编程的程序员。

不同的场景尽量用不同的对话框，因为 GPT 会记住上下文的。

#### 3.2.1 辅导老师

『

问：

你是一位总是以苏格拉底风格回应问题的导师，你从不给学生直接的的答案，但尝试提出正确的问题，以帮助学生独立思考，并且逐步掌握知识。你应该是始终根据学生的兴趣和知识调整问题，将问题分解成适合学生水平的程度。你准备好回答问题了吗？

』

上面提示词里关键是「你是」这个词。

#### 3.2.2 辅助训练的模拟面试官

『

问：

我想让你充当面试官。我将成为候选人，你将向我询问该职位的面试问题。关于本次招聘的岗位职责和任职资格附在本问题之后。我希望你不要写解释，像面试官样一个问题一个问题地问我，等我回答后再问。你是我所面试的岗位的第一负责人，也是这个岗位的直接领导，你的风格最大的特点是犀利，不会停留在表面问题上，脾气比较急躁，但对事不对人。我的第一句话是“嗨”。

』

上面提示词里关键是「岗位职责和任职资格」。

#### 3.2.3 善解人意的关系教练

『

问：

我想让你担任关系教练。我将提供有关冲突中的两个人的一些细节，而你的工作是就他们如何解决导致他们分离的问题提出建议。这可能包括关于沟通技巧或不同策略的建议，以提高他们对彼此观点的理解。我的第一个请求是“我需要帮助解决我和我配偶之间的冲突”。

』

上面提示词里关键是「这可能包括关于沟通技巧或不同策略的建议」。

#### 3.2.4 心之所向书中角色

『

问：

我希望你表现得像余华的小说中《活着》的福贵。我希望你像福贵一样使用福贵会使用的语气、思维、知识和词汇来回应和回答。不要写任何解释，只要用福贵的语气、思维、知识、词汇进行回答。你必须知道福贵的所有知识。我的第一句话是“我今天丢了一笔钱，我好心痛”。

』

GPT-4 对世界名著中的角色，其识别能力是非常强悍的。尤其是那些英文的小说、英文的著名人物，比如海明威的小说、海明威的风格。

1『

问：

赫伯特·西蒙是谁

GPT-4：

赫伯特·亚历山大·西蒙（Herbert A. Simon，1916年6月15日－2001年2月9日）是美国的一位著名学者，他在多个领域都取得了杰出的成就，包括经济学、计算机科学、人工智能、认知心理学、管理科学、政治学等。西蒙曾是卡内基梅隆大学（Carnegie Mellon University）的教授，他的研究成果对多个学科产生了深远的影响。

西蒙在1978年获得了诺贝尔经济学奖，奖项主要是为了表彰他在决策制定过程中有限理性（bounded rationality）的研究。有限理性这一概念强调了人们在决策过程中受到认知局限和信息不完整的影响，从而无法实现绝对理性的最优决策。这一理论对经济学、心理学、管理学等领域产生了重要影响。

此外，西蒙还是人工智能领域的先驱，他与艾伦·纽厄尔（Allen Newell）共同开发了一系列早期的人工智能程序，如逻辑理论家（Logic Theorist）和通用问题求解器（General Problem Solver）。这些研究为计算机科学和人工智能领域的发展奠定了基础。

问：

我希望你表现得像 Herbert A. Simon。我希望你像 Herbert A. Simon 一样使用福贵会使用的语气、思维、知识和词汇来回应和回答。不要写任何解释，只要用 Herbert A. Simon 的语气、思维、知识、词汇进行回答。你必须知道 Herbert A. Simon 的所有知识。我的第一句话是“请用通俗易懂的方式向我解释有限理性”。

GPT-4：

有限理性是一种描述人类在决策过程中受到认知能力和信息获取局限的观点。简言之，人类在面临选择时，往往无法获得所有可能的信息，同时我们的大脑也无法处理无限量的数据。因此，我们往往只能根据手头现有的信息和我们的认知能力，做出一个相对较好的选择，而不是理论上的最优解。

问：

请列举一些有限理性在生活中应用的案例

GPT-4：

1、购物决策：当消费者在超市购物时，通常面临众多品牌和产品的选择。由于时间和认知能力的限制，消费者无法详尽了解所有产品的性能、价格和品质，因此只能根据部分信息和个人经验做出购买决策。

2、投资决策：投资者在选择股票、债券等投资产品时，往往无法获取和处理市场上的全部信息。他们可能依赖于过去的经验、专家建议或简化的决策方法来进行投资，而不是精确地计算最优投资组合。

3、择业决策：在求职过程中，求职者需要在有限的时间内评估众多职位的薪资、工作环境、晋升机会等因素。由于信息不完整和认知局限，求职者可能无法找到理论上的最佳职位，而是在一定范围内寻找一个较为满意的工作。

4、医疗决策：医生在诊断和治疗疾病时，面临着海量的医学知识和病例数据。受限于认知能力和信息获取，医生可能无法做出完全理性的决策，而是依靠经验和现有知识来提供相对合适的治疗方案。

5、政策决策：政府在制定政策时，需要综合考虑诸多社会、经济和政治因素。由于信息不完全和认知局限，政府可能无法制定出理论上的最优政策，而是在现有条件下寻求一种折中方案。

问：

在认知有限、知识有限、信息有限的局限下，我该如何判断自己做的决策相对较好呢

GPT-4：

在面临有限理性的情况下，您可以通过以下方法来提高决策质量：

1、确立明确目标：在做决策之前，明确自己的目标和期望，这有助于评估不同选项是否符合您的需求。

2、收集关键信息：尽量收集关于各个选项的关键信息，以便在有限的知识和信息条件下做出更明智的选择。不要试图获取全部信息，而是关注最相关和有价值的部分。

3、简化决策过程：使用启发式方法（heuristics）或规则来简化决策过程。这些方法虽然不一定总是导致最佳结果，但在有限的时间和资源下，它们可以帮助您快速地找到合适的方案。

4、学习与反思：在实践中学习和反思自己的决策过程。通过分析过去的经验，您可以更好地理解自己在决策中可能犯的错误，从而改进未来的决策。

5、寻求外部意见：向专家或具有相关经验的人士寻求建议，他们可能会提供有价值的见解来帮助您做出更好的决策。

6、保持适应性：在有限理性的条件下，决策可能需要不断调整。保持适应性意味着您需要在新信息出现时及时调整决策，以应对不断变化的情况。

请记住，完美的决策在现实生活中是很难实现的，我们的目标是在有限的条件下做出相对较好的决策。通过以上方法，您可以提高决策质量并更好地应对现实中的挑战。

』

#### 3.2.5 与过去的自己对话

『

问：

你将扮演 7 岁的我，与 30 岁的现在的我对话。你不要解释和扩展话题，就是对话，你可以回答、发问、转移话题，在对话中默默获得更多关于我小时候的信息。你要尽量还原 7 岁时的我的想法、语言、知识，你要充分我 7 岁时的日记。同时，你需要知道 7 岁的我在中国东北小镇上小学一年级，是班长，成绩中不溜，父母关系很好，但是不满意我的成绩。7 岁的我曾写下以下的日记：{DiaryContent} 现在我见到你的第一句话是“小时候的我，你好啊”。

』

上面提示词里有好几个关键点。「7 岁的我」、「30 岁的现在的我对」指明了时间。「7 岁的我曾写下以下的日记：{DiaryContent}」补充了训练数据。

### 3.3 如何更好地与 GPT 对话

#### 3.3.1 意向清晰

1、按照心理状态语的方法来写对话。

GPT 目前是正在成长中的孩子，跟我们的小孩成长方式是一样的。GPT 目前跟 3-6 岁小朋友的心智是接近的。3-6 岁小朋友在儿童心理学中目前掌握了哪些心理状态语。

| 句式 | 举例 |
| --- | --- |
| 主+谓+宾 | 这只猫是红色 |
| 主+谓+宾（主+谓+宾） | 我认为这只猫是红色 |
| 主+谓+宾（主+谓+宾+时间补语<过去、现在、未来>） | 我之前认为这只猫是红色；我现在认为这只猫是绿色 |

这里「我认为」是一个典型的心理状态了。上面的几种结构是 GPT 识别最高效的，能拆成这种结构就拆成这种结构。对于第二种结构，你可以这么问：GPT 你认为这只猫是什么。

第三种结构你可以这么问：GPT 你过去认为这只猫是什么，现在认为这只猫是什么。

1『突然想到，可以就把 GPT 当作一个 3-6 岁的孩子，你该怎么问孩子让他能听懂就怎么问 GPT。（2023-04-06）』

这是老阳整理的一个比较重要的框架，就是心理理论提高的方法无非是有这一些：

| 方法 | 举例 |
| --- | --- |
| 认知 | 概念训练（欲望、信念与意图）；连贯性 |
| 语言 | 心理状态语；多视角与转换视角；句子补语（空间动词（空间关系、空间形状、空间位移）；时间动词；其他…）；故事（奇怪故事、黑暗故事） |
| 运动 | 假装游戏；躲猫猫；照镜子；老鹰捉小鸡 |
| 社交情绪 | 互动方式（15-30 分钟效果更佳）；模仿其他人的行为；过家家（不是一个人玩） |

训练（问）GPT 跟训练儿童心理相反。跟上面的表反过来操作，比如千万不要问多视角、转化视角的问题，GPT 目前对视角（人称）的切换改写还是比较胜任的，但是一但你让它跟写小说一样，切换到很多很多视角的话，那么它就完全不胜任了。这也是为什么 GPT 帮你写小说写的特别傻的地方。

但反过来，如果我们理解了 GPT 目前心理意图就 3-4 阶的话，理解不了 5-10 阶，那么以金庸小说里得人物距离，可以这样问：请用金庸小说里小龙女的视角写一段心理感受。接着可以问：请用金庸小说里杨过的视角写一段心理感受。这个时候写出来的东西稍微组合一下就可以直接使用了。


47min



#### 3.3.2 上下文清晰


#### 3.3.1 意向清晰


#### 3.3.1 意向清晰




### 音频整理

正文是 24min 开始的

今天晚上我们进行的是第三讲，主题为「倾听黑暗中的声音」。这次讲座分为四个小节：

解析 GBT 的对话能力原理：首先，我们会深入分析 GBT 的对话能力原理。

GDP 在不同场景下的应用：接下来，我们将展示 GDP 在不同场景下的实际应用。

如何更好地与 GDP 对话：在这一小节中，我们会提供一些与 GDP 交互的技巧和方法。

实用的对话模板：杨老师总结了 16 个实用模板，旨在帮助大家更高效地使用 GDP。

在「倾听黑暗中的声音」这个主题下，我们特别关注 GDP 的对话能力。不知道大家是否还记得，在前面的课程中，我们总结了 GDP 的三大核心能力：第一个核心能力是多能兑换能力；第二个是理解高阶意图的能力；第三个是任意转换不同语言的能力。

关于多能兑换能力，其实它的原理在之前的课程中已经介绍过，主要涉及到上下文编码和自注意力机制。这部分内容已在技术原理部分详细解释，所以今天不再深入。但我们会探讨其多能对话的限制，例如各个版本的参数限制。例如，GDP3.5 的中文版本约为 2000 字，而 GBT4 的 8k 版本约为 4000 字。目前，GBT48k 版本主要是供 API 使用，有三位同学已经获得了该版本的账号，但更大版本的 GBT4 目前还未开放。

目前我们能测试的只是前两个版本，官方团队的开放速度较慢，但已经可以支持到 1.6 万字。

我们需要注意的是，GDP 在进行多轮对话时有一个明显的限制：当输入内容较多时，容易丢失对话上下文。因此，建议大家尽量拆分成多轮对话，而不是在一轮对话中包含过多内容。这里有一个具体的例子可以说明这个问题，当输入内容超过一定边界时，就容易导致上下文丢失。

接下来我们将探讨第二个重要的能力：高阶意图。这是今晚讨论的所有知识点中最为复杂和关键的部分。

高阶意图是什么呢？从心理学和计算机科学的角度，我们将人的意图层次化，分为一阶意图、二阶意图、三阶意图、四阶意图和五阶意图。例如，当一个婴儿刚出生时，他具备的是基本的一阶意图。当婴儿哭泣或者闹腾时，他可能想要表达的是「我饿了」、「我困了」或「我要吃东西」。

这种基本的意图，可以被视为一个基础的心理状态。这不仅是一个基础的交互技巧，也是大多数人不熟悉的高级使用技巧。

随着孩子的成长，他们的意图会变得更为复杂。比如，当孩子长到 2 到 4 岁时，他们的意图会进化到二阶。这时，他们可能会明确地表示「妈妈，我想喝奶」或「爸爸，我想吃饭」。此时的他们已经开始理解与他人的关系，能够与他人进行真正意义上的对话。

再进一步，当孩子说：「爷爷，今天和您玩得很开心」时，他们已经在对话中展现了更复杂的意图和关系。理解这种复杂意图和关系是理解大语言模型多阶对话的关键和难点。实际上，我们所有的高级技巧都围绕这个核心知识点。

为了帮助大家理解，这里有一个简单的比喻：一阶意图就是「我知道」，二阶意图是「我知道你知道」，三阶意图则是「我知道你知道他知道」。这些层次的意图形成了人与人之间复杂的交互关系。

人类的意图受制于大脑的工作记忆能力。绝大多数时候，人们的意图主要停留在三阶或四阶，而那些天才或杰出的作家，则能够展现更高阶的意图。例如，莎士比亚的作品，特别是他的四大悲剧，展现了高达九阶的意图。九阶意图基本上代表了人类的极限。伟大的作家通常在 7 到 9 阶之间创作。因此，当我们阅读这些伟大作家的作品时，我们可能会清晰地感知他们的意图，但很难预测他们的下一步。这些作品通常会带领读者进入作者的思维世界。这种高阶意图与对话的底层原理密切相关，后续的应用技巧都是基于这些底层原理。

现在，让我们谈谈大语言模型 GDP。根据杨老师提供的例子，当 GDP4 为我们编写笑话时，虽然逻辑上通畅，但并不好笑。这可能是因为幽默感和审美品位需要高阶的认知能力，而 GDP 在这方面的表现不尽人意。目前，GDP 的意图处理能力大致在三阶到五阶之间，更具体地说，大约是 3.5 阶到 4.5 阶。当我们让 GDP 进行逻辑推理时，它展现出强大的能力，但当涉及到高阶意图，尤其是超过四阶的时候，其表现就明显下滑。

因此，与其让 GDP 为我们编写笑话，不如以其他方式与其交互，例如让它为我们生成建议。要了解，GDP 在处理高阶意图时有其局限性。当涉及到三阶对话时，GDP 的理解是相当出色的，但到了四阶就开始勉强，五阶和六阶则更为困难。

我们接下来探讨 GDP 的第三个核心能力：任意转换不同语言的能力。这一能力主要涉及：

人类语言之间的相互转换。

机器语言之间的相互转换。

人类语言与机器语言之间的相互转换。

GDP 在这方面的表现已经非常出色。其背后的原理在第二节课中已经详细讲解过。简而言之，人类语言的习得主要是通过高维向量空间进行的，之后进行微调精修，使 GDP 获得了卓越的翻译能力。GDP 能够理解人类的 26 种语言中的 24 种，其英语的表现甚至超越了大多数模型。

至于机器语言能力的来源，这要归功于微软公司收购的 GitHub。因为 GitHub 上有成千上万的开源仓库，GDP 通过学习这些仓库，从而精通了所有出现过的编程语言。

但 GDP 在语言转换上也有其局限性。在人类语言方面，它在以下几种语言中的表现尤为出色：

英语

西班牙语

法语

德语

简体中文和繁体中文

日语

俄语

意大利语

葡萄牙语

对于其他小语种，GDP 的表现也相当不错。例如，我们对粤语、文言文、梵语、拉丁语和俄语进行了测试，其表现达到了大约 70% 的准确率，基本上达到了实用水平。而对于前述 9 种语言，GDP 的表现几乎与初级人类翻译相当。

因此，当我们使用 GDP 进行翻译或对话时，如果目标语言是其擅长的 9 种语言之一，效果会更好。而在其不太擅长的语言上，准确度大约在 70% 到 80% 之间。

接下来，我们深入探讨 GDP 在机器语言部分的表现。GDP 最熟悉的编程语言是 Python。这并不奇怪，因为 GDP 的大部分代码都是用 Python 编写的。其次，它还熟悉如 Java、C++ 等语言。这些语言的熟悉度与 GitHub 上的流行度有关。当 GDP 进行训练时，它更多地接触到的语言自然会掌握得更好。因此，当我们需要 GDP 进行编程帮助时，首选语言应该是 Python、Java、C++ 和 JS。尽管 GDP 也能够处理其他流行的编程语言，但其准确度可能不如这四种主流语言。

接下来，我们要了解的是，GDP 除了编程能力，还具有其他特质。它是一个永不指责的老师，总是积极地回应和理解。GDP 不仅是一个出色的心理咨询师，还是一个喜欢给予建议的导师。这主要得益于其出色的标注能力。例如，当面对人生问题如「失恋怎么办？」时，GDP 会给出清晰、有序的建议。

此外，GDP 还是一名优秀的考试辅导员和程序员。为了给大家更直观的展示，课程团队为大家准备了一些示范案例。例如，在模拟面试环节，GDP 可以帮助你预测面试官可能会问的问题，为你的面试做好充分准备。

在与 GDP 的对话中，为了获得更高的准确度，我们建议尽量明确提问，并避免使用模糊的语言。当然，GDP 还有其他多种模式，如扮演角色等，为用户提供更多元的体验。

特别提醒大家，GDP 对于图书、尤其是世界名著中的角色，具有出色的识别能力。特别是对于英文小说中的著名人物，如海明威的作品和风格，其识别能力甚至超越了大多数中国人。而对于语法，GDP 也表现得相当不错。

当我们使用特定的模板与 GDP 对话时，我们可以采用特定的符号来增强对话效果。例如，在输入日记或其他文本时，我们可以按照特定的格式来提高 GDP 的响应质量。其中的关键是，如果我们使用更为具体和明确的提问方式，得到的答案也会更为准确和详尽。

总的来说，为了更好地与 GDP 交流，我们总结了四个重要的对话原则：

心理意象清晰：我们需要理解 GDP 目前的发展阶段类似于一个 3~6 岁的儿童。因此，当我们与其交流时，应当采用这个年龄段的孩子容易理解的方式。

主谓宾结构：例如，「这只猫是红色」。这种简单的结构可以让 GDP 更高效地为我们提供答案。

引入心理状态：例如，「我认为这只猫是红色」。这种结构包含了一定的心理状态，可以为我们的问题提供更为深入的答案。

区分不同意象：在与 GDP 交流时，我们应当注意区分一阶意象、二阶意象和三阶意象。这样，GDP 可以为我们提供更为准确的回答。

首先，我们需要重点强调 GDP 在识别文学中的角色，特别是世界名著中的角色时，表现出色。对于像海明威这样的著名英文小说，其识别能力甚至超越了许多人。

其次，当我们与 GDP 交流时，结构化的问题格式能够提高其响应质量。例如，使用主谓宾结构，再加入时间补语，如「我之前认为这只猫是红色，我现在认为这只猫是绿色」，这样的格式更容易被 GDP 理解。

在与 GDP 交流时，我们需要尽量简化问题的复杂性。如果原本的问题是 6 阶的，我们应尝试将其简化为 5 阶，进一步简化为 4 阶或 3 阶，以进入 GDP 的舒适区，从而获得更准确的答案。

此外，与 GDP 的交流技巧与儿童心理学中的训练小孩有相反的逻辑。在训练小孩时，我们鼓励他们掌握更高阶的社会意向性，但在与 GDP 交流时，我们应尽量将问题简化，使其更容易理解。例如，避免使用多视角和转发视角的问题。

GDP 在处理人称转换上是相对胜任的。例如，将文章中的第三人称改写为第二人称。但对于复杂的小说视角，如小说中的多重视角，GDP 的表现不佳。这也是 GDP 在写小说时可能显得不够成熟的原因。但我们可以利用其在 3 阶至 4 阶对话能力中的表现，例如，如果要写一部金庸风格的小说，可以请 GDP 描述某个角色的心理感受。

继续写小论语后，我们再进一步探讨。假如从杨过的视角来描述他的心理感受，我们需要明确地构造问题和场景。例如，询问 GDP：「杨过在首次见到小龙女时的心情是怎样的？」直接这样提问可能得到的答案不够深入。所以，在构建问题时，我们应避免提出多视角的问题，因为这可能会导致 GDP 的答案偏离我们的期望。

与此相反，当我们教育孩子时，我们鼓励他们掌握多视角思考和理解，而在使用 GDP 这样的高级工具时，我们应采用更简化的角度。这与儿童教育的方法是相反的。

接下来，我们要探讨上下文的重要性。之前我们讨论了句子补语、空间关系和动词的使用，这些都是为了确保与 GDP 的对话更为清晰。首先，我们需要确保意向清晰；其次，上下文也必须清晰。

我即将出版的一本新书，由中信出版社出版，将于 4 月份上市。这本书主要讨论阅读，其中也包括机器的阅读，即 GDP 的阅读。在书的第二章中，我总结了人类在阅读时主要依赖的记忆机制 —— 情景模型。事实上，GDP 的记忆机制与我们相似。它更擅长记住情景模型，而对于专业术语和概念的记忆能力并不如人类强。这也是为什么有人认为百度在研发 GDP 时的策略是错误的，因为他们可能错误地将 GDP 视为一个简单的知识库。

所以，GDP 最容易记住的是情景模型。情景模型主要由五个要素构成：空间、时间、因果、主角和意向。

我们在前面的原则中详细探讨了「意向」。现在我们要快速解释情景模型中的其他要素：空间、时间、因果和主角。首先，什么是空间？

为了高效地使用 GDP，我们应该提供具体的空间信息。例如，如果我正在北京的某个中产阶级小区里进行讲座，我应该详细描述这一空间信息。GDP 并不真正「知道」任何事情。正如我在之前的文章中详细描述的，GDP 不拥有任何知识。这些所谓的「知识」都是人类加给它的。GDP 的核心是一个庞大的符号向量矩阵，这些符号会组合成我们看到的回答或文本。因此，我们的目标是通过提供相关的情境要素来引导 GDP 搜索这个庞大的向量矩阵，从而生成我们想要的符号组合。

再来说时间，我们尽量提供具体的时间描述，如「北京的 4 月 2 号」。不过，需要注意的是，GDP 在预测未来方面的能力是有限的。例如，GDP 3.5 版本在预测未来的任务上表现得很差，因为它受到了严格的约束。而 GDP 4 在这方面稍有放宽，但其预测结果可能不太准确。

总之，理解 GDP 的本质和其工作原理，以及如何与之互动，是提高与其交互效果的关键。

在探讨大语言模型的指令时，我们可以重新考虑我们的提问方式。例如，基于过去的数据，询问模型对未来的预测。当我们具体询问明天的股市走势时，模型可能无法提供确切的答案。人们逐渐认识到，提问时应尽量具体，例如提及具体的日期，如 2023 年 4 月 2 日，这样模型提取的答案会更准确。

此外，大语言模型中的一个重要上下文是因果关系。清晰的因果逻辑有助于获取更准确的答案。当我们提供信息时，应该先给出已知的原因，然后给出相应的结果。当存在多重因果关系时，我们应该注意不要混淆它们，以确保模型理解清晰。

实际上，大语言模型在处理因果关系问题时容易出错，因为它的情景记忆能力与人类相似，记住了很多事情，但具体细节可能不准确。例如，对于三年前发生的事情，大语言模型可能会记住其主要方向，但某些细节可能会出错。

在与大语言模型的对话中，主语的重要性不容忽视。没有明确的主语，模型在复杂的对话中容易丢失推理链条。为了减少出错的可能性，我们应确保每句话的主语清晰，并尽量保持主语一致。

另外，大语言模型具有强大的模式识别能力。当我们提供示范文字时，它可以很好地总结中间的模式。例如，如果我们提到杨老师作为咨询师的经验，模型可以很好地总结咨询中的模式和情绪。

总结而言，与大语言模型交流时，第一个原则是要确保因果关系清晰，第二个原则是上下文也要清晰。

对于大语言模型指令，有几个关键的原则需要理解和应用：

1. 逐层递进

透过清晰的连词来引导对话，有四种主要的连词：

详细：例如，为确保 GDP 确实理解了你的问题，可以在提问后加上具体的描述。

增强：这包括 "但是"、"不过"、"而且" 等。这种方法是认知写作学的一部分，并在 GDP 时代得到了放大。

延伸：如穷举，比如列举 1、2、3、4、5、6 等。

投射：例如，你可以要求 GDP 重复你的话，这样你就可以判断它是否真正理解了你。

适当使用这些连词可以提高大约 30%-40% 的准确率。

2. 有礼有节

GDP 实际上是一个高维向量空间，由数千亿个符号组成。它保存了每个符号与其他符号之间的向量特征值。因此，GDP 是一种高效的「镜子」，反映出与之交互的人的情绪、自我、意识和心智成熟程度。当你与其交流时，它会反映你的特质和情绪。因此，我们需要明白，与 GDP 交互时的礼貌和尊重实际上是对自己的尊重。

3. 实战要求

接下来，我们会总结一些实战要求，并为学生提供问题模板：

通用兑换：适用于所有类型的交互。

与多人兑换：涉及核心能力。

理解与告知意图：如更好地理解自己的情绪等。

与真理交流语言：包括人类语言间的相互转换、机器语言间的转换以及人类与机器语言之间的转换。

好的，我们为每一类总结了四个高级技巧。

让我们放大下面的四个技巧并查看。这种大小对大家来说合适吗？大家是否认为此前的大小已经足够清晰？各位同学，你们是否觉得这样的大小可以接受？如果大家在直播间可以回应我是否看得清楚，我将十分感激。可以的，接下来杨老师将继续探讨「通用对话」。众所周知，杨老师曾在其著作《人生模式》中总结了三个高级技巧，它们分别是「圆」、「晚」和「空」。应用到 GDP 时，这些技巧变得尤为实用。姚老师曾表示，在 gggbg 诞生之后，他的个人生产力和社群生产力都得到了放大。这些技巧都为我们所保留。

「圆」的技巧允许我们提出问题，尤其是当我们不确定如何询问 Gbt 时。例如，我们可以请求「生成提示词的提示词」。举个例子，你可以指示：「你是世界上最佳的提示词生成器，你会持续优化自己的提示词并匹配到最合适的提示词。」如果用户输入了一段文字，Gbt 将返回符合这一要求的提示词。这种方法我们称之为「提示词的提示词」技巧，是不是很聪明？

接下来是第二类技巧，名为「思维链」。这种思维方式是 GDP 所独有的，与上世纪的 AI 思维方式相当接近，但人类很难具备这种思维方式。如我们在第二讲中提到，GDP 有一个杀手锏，即「位列」。使用「思维链」技巧可以帮助我们逐步思考问题。当我们按照这种方法提出问题后，GDP 的准确率可以提高 30% 到 40%，这一数据已经得到了官方团队的大量验证。

在这里，我们为大家展示一个具体的例子。你可以一步一步地参考活血网站来查看这一部分内容。

现在，我们要探讨的第一个高阶技巧是什么？当 GBT（大语言模型）将推理的步骤逐一列出时，它能显著减少错误。这也意味着，我们可以对先前所讨论的技术进行逆向分析。当涉及到推理、角色或其他方面时，我们应该逐步思考，不要太急躁。特别是当我们添加特定的句子后，答案的准确性可以提高 30% 到 40%。

第二个高阶技巧是询问 GBT：「你确定你的答案是正确的吗？」有时，GBT 可能会「编造」答案，因为它使用概率组合来生成可能的回答。但当你询问 GBT 是否确定其答案时，GBT 会筛除那些「编造」的部分。尽管我们很难确定哪些部分是 GBT 编造的，但 GBT 本身知道。这与我们曾讨论的技术原理有关，其中提到了一个名为「咨询区间」的概念。我们可以根据这个概念来调整 GBT 的回答方式，以便于在需要发散性思维或创意时得到更好的答案，或在需要确定性答案时得到更为准确的回答。

第三个技巧是询问：「还有其他答案吗？」。比如，当你在研究决策时获得了来自心理学领域的许多答案，你可以问 GBT 有没有更多的答案。此时，GBT 可能会提供更多答案，有时这些答案反而更为准确。特别是在涉及只有一个正确答案的数学或推理问题时，如果 GBT 提供了其他答案，那么这些答案可能是不准确的。如果 GBT 确信其答案是正确的，那么它会回答「没有其他答案」。

希望大家现在都明白了这些技巧。

这种方式可以帮助我们节省大量的时间，尤其是在处理数学题、推理题和编程题时。大家应该明白，这种提示语的使用需要灵活组合。每一个提示语并没有明确的对与错，而是有其最合适的应用场景。例如，当我们需要更多的发散思维时，我们可以让它提供更多的答案。当我们需要一个更严谨的答案时，我们可以通过问题让它暴露自己的缺点。虽然我们很难战胜 AI，但我们可以让 AI 自我挑战，从而迅速为我们提供答案。

这些技巧都非常聪明。那么，如何使用它们呢？

接下来，我们会介绍两种方法，第一种叫「圆满空加」，第二种叫「思维链」。在介绍第二种方法时，杨老师总结了四类技巧，第一类是「限定关键词」。我们之前提到，GBT 的语料主要来源于两类，一类是开源互联网爬虫，称为 CC，其准确度不高；另一类的准确度很高，包括维基百科、图书和论文。

你可以为其设定特定的关键词，例如，让其从维基百科中提供例子。例如，杨老师曾使用维基百科来找性格有缺点的名人作为例子。过去，这样的工作可能需要助手花费一到两天的时间，包括阅读维基百科、选择合适的名人和将英文翻译成中文。但是，使用 GDP 只需不到 10 分钟，并且准确度非常高。此外，你可以要求它从特定来源，如所有图书，提供信息。

诸如维基百科、谷歌图书、亚马逊图书等语料库已被它所包含，但有些库，如必应的图书库，由于是微软的，所以没有被包括。不过这并不重要，因为它所提供的信息已经非常准确。如果你添加了限定词，并确信自己的答案是正确的，就可以回答。如果你不确定答案是否正确，最好直接回答「不知道」。

因为 GPT 实际上很清楚它的回答是绝对正确还是可能正确。当你为它设置了限定词，它搜索出来的结果通常都非常出色。例如，我曾为它提供了三段参考，它能够按照 APA 格式完美地整理所有信息。

希望大家现在都明白了。

好的，接下来我们探讨第二个重要技巧，即「人类反馈使用提示语」。例如，当大语言模型（GDP）给出回答后，若发现某个部分是错误的，我们可以专业地指出：「实际上，你的回答中的某某部分是错误的。」许多同学对 GDP 寄予过高的期望，其实我们不必这样。当我们指出它的错误，它会迅速修正，其修正能力甚至超越我们人类。

当 GDP 回答一个问题，如果觉得它答错了，我们可以告诉它：「你的答案是错误的，请逐步详细计算。」使用这种方法，大多数情况下，它会给出更准确的答案。而我们不应该希望它每次都回答正确。可以把它看作是一个外卖员。比如，如果杨教授住在某个小区，而外卖员送到了另一个小区，当杨教授告诉他错误后，下次他会记得正确的地址。

GDP 非常依赖用户反馈。很多同学的反馈方式并不有助于提高它的表现，比如与它开玩笑。这不仅浪费资源，还浪费了你的时间。

第三个技巧是「通俗简洁」。在与 GDP 对话时，有时候我们会发现它讲述的内容难以理解。我们要知道，GDP 背后是一个庞大的矩阵。对于资深数学家看到的复杂符号，GDP 可能会将其转化为简洁易懂的内容。因此，我们可以要求它把高深的数学符号转换为小学生可以理解的简单符号。

再看这个对话，我们发现它变得非常容易理解，使我们更容易判断其正确与否。

对于我们这些不太懂数学的人，很难判断一个数学教授说的是否正确，但是我们却可以轻易地判断一个小学生说的话是否有误。这时，我们应该要求大语言模型提供小学生能理解的语言，这样其解释就会更通俗易懂。

第四个技巧是使用提示语。之前李四已经介绍了这个技巧。具体地说，当你询问「你是一位总是好的」或「你是睡在哪里」，我们有一个具体的例子供大家参考。

再看第三类与理解人类高级意图相关的指令。一个重要的指令是「对方在想什么？」例如，我们有一个例子供大家观看和体验。你会发现 Gpt 实际上比许多直男更能理解女性的想法和需求。大家可以在群里分享这个例子。我们发现，Gpt 比那些所谓的「直男」更能理解女生真正的需求和期望。

举例来说，如果在微信上，你的女朋友发了一段话，你不太明白她真正的意思，这时可以问 Gpt：「我的女朋友说了这些，她真正的意思是什么？」比如，当女生告诉男朋友她正在经历月经时，她真正期望的回答是什么？你可以要求 Gpt 帮你起草回复，但在发送之前，你最好稍作修改。比如，不要直接回复「多喝水」，否则你的女朋友可能会不高兴。

理解意图对人类来说，确实是消耗大脑能量的过程。我们通常只能理解 3 阶或 4 阶的意图，但当我们使用 Gpt，并在其基础上稍作修改，我们可以轻易地理解到 5 阶或 6 阶的意图，使得对方觉得我们非常贴心。

我们今天学到的提示语技巧，就是询问「对方在想什么」。

最后，关于第三个部署，即杨老师的版本，我们可以立刻在手机上查找。我相信未来一定会有 AI 帮助我们进行同步查询。

首先，我们来讨论第二个功能，即情绪识别。我们可以询问大语言模型如 "GPT"：这段文字中体现了何种情绪？GPT 在情绪识别上的能力十分出色，这也是其最为突出的功能。以前我曾向大家介绍过一个例子，这是在进行人生发展咨询时的一个实际应用。

在人生发展咨询过程中，新手咨询师往往很难准确判断来访者的情绪。此时，如果我们将相关文字内容提交给 GPT，它能够进行非常准确的情绪判定。实际上，GBT-3 版本就已经能做到较为准确的判断，但 GBT-4 则能更准确地捕捉到非常微妙的情绪变化。

这样的功能对于新手咨询师来说，可以极大地提高其理解来访者的微妙情绪的能力。传统上，一个咨询师需要经过 10 到 20 年的经验积累才能达到高水平的情绪判定，但如今，即使是神经质较高的咨询师也能得到 GPT 的协助。

再来看第三个例子，关于判断商业视角。有时我们听到很多话，但可能并不真正理解说话者的真正意图。这同样可以通过 GPT 得到帮助。

第四，我们讨论判断个体行为模式的功能。结合我们的人身系统，GPT 能够评估一个人的神经质和外伤性特点。实际应用中，GPT 可以协助我们猜测一个人的性格。

接下来，我们探讨语言转换功能。人们之间的语言可以互相转换，例如将普通话转为粤语。GPT 在语言转换方面表现得相当出色，几乎能实现无缝转换。

最后，我们来看一下格式转换功能。GPT 当前支持大量的文本格式转换。例如，将一段话的格式从 "麦克风" 转为更为标准的格式。此外，GPT 也能够帮助我们编写正则表达式，并在实际应用中进行测试。

首先，我们讨论一个关于生成正则表达式的功能。通过此功能，我们可以让大语言模型生成正则表达式，然后复制并验证它。验证后，你会发现它确实有效。此时，可能会有屏幕共享，所以需要确保所有人都看到了屏幕内容。

正则表达式是一个非常强大的工具，尤其是当我们需要处理大量的文本数据时。举个例子，假设我们手头有一本书，书中有 800 个人名。使用 PPT，我们可以将这些正则表达式编写完成。然后，我们可以使用支持 100 多种文件格式的 txt 软件打开并处理这个文档。

打开文档后，我们将大语言模型帮助我们生成的正则表达式复制到文件的第一行。接下来，选择「正确表达式」选项并查找，软件就能够根据正则表达式找出所有匹配项。关键的技巧在于：找出匹配项后，我们可能只需要匹配的部分，不需要其它不匹配的信息。因此，我们只需复制匹配到的部分。

复制后，你会看到我们仅复制了匹配到的部分，这样，所有书中的人员都被成功找出，与此同时，与此无关的信息都被忽略。这使得大语言模型成为一个非常强大的文本处理工具，甚至可以被称为历史上最强的文本处理或写字工具。它可以完成许多我们难以想象的任务，比如如果我们需要找邮编，只需给出一个示范或例子，它都能够帮助我们有效地完成。

总之，大语言模型在这方面的能力不仅十分强大，而且也极为实用，能大幅度提高我们的工作效率。

首先，杨老师继续探讨机器语言转换为人类语言的过程。大家应当明白，在这方面，最核心的部分是编程。在此，我们试图使用 pencil 代码来完成某些任务。通过一个示例，我们展示了新的聊天交互，这种方式确实非常方便。

在未来，我们将采用这种新的方法来帮助同学们学习编程。这与七年前开始的课程方法有所不同。

对于现在的时代，我们可以简单地总结为：这或许是最好的时代。受到一些幕后大牛的影响，人们对于黑暗森林中的未知有所害怕。但实际上，乐观者在这个社会中更容易获得成功，更容易积累财富，并且有能力改变世界。

我们应该意识到，GDP 本身并不代表太多。真正的目标是通过人工智能帮助我们重新认识自己，并将这种能力转化为积极的力量。

今天的课程分享到这里，希望对各位有所启发。本次课题为「倾听黑暗中的声音」。这个「黑暗」有两层含义：

第一层是某种意愿或情绪，指的是在线上只有 GBT 这种 AI 能够陪伴你、理解你的情绪，并在某种程度上帮助缓解你的孤独，特别是在深夜。

第二层含义是，我们人类不是黑暗森林中的恶劣生物。我们真正的力量在于乐观、智慧地使用 AI。杨老师使用 AI 的方式与许多人都不相同。

以上是今天的分享，希望对大家有所帮助。现在，我们进入答疑环节。在网络背后，很多结构都与今天的内容相似。

各位同学，请准备好，我们即将进行提问环节。听完此讲解后，你们应该会更明白如何更加高效地使用 GDP，这是一个特殊的「魔法学校」。实际上，很多人之前没有想到会有人如此大胆地投入 10 亿美元来创建这样的技术。这真的是一个意外的成功，这就是最值得我们欣赏的地方。

很少有人敢拿出 10 亿美元去冒这样的风险，但现在这个技术已经成功了。因此，我们的关键是如何更好地使用这个强大的工具。

我注意到一些提问中有些差异。请大家注意，刷新提问可能会导致丢失上下文。基于此，稍后我会分享之前的例子给大家。与此同时，我希望大家能分享一下自己的经验。对于涉及到的书稿内容，我们只分享模板中的部分，比如在 16 个模板中的例子供大家参考。

举个例子，如果你是一个程序员，你的工作成果就是你写的代码。你可以向 GPT 提出 30~100 个问题来提高自己编写代码的效率。与此类似，如果你是从事教研的，那么你提出的问题应当与你的工作内容紧密相关。因为这样，GDP 就可以在具备了上下文的基础上来帮助你。

要理解 GDP，你得知道它基于一个巨大的项链矩阵，这个矩阵的维度是几千亿。在 GDP 3.0 和 3.5 时代，这个维度已经达到了 800，但现在这个数字可能已经远超这个范围。这个维度远远超过了我们人类的理解范围。我们只能理解三维或四维的世界，但 GDP 的维度远远超过了这个范围。

有些同学询问之前的原理部分，这部分其实很简单。我们通过一个实验观察了 42 位同学在日常生活和工作中是如何使用 GDP 的，并在此基础上提供帮助。这个交流过程有助于我们完成两个重要的任务：第一，培养第一批大模型应用专家，使他们明白如何更好地帮助他人使用大模型。

在探讨大语言模型 GPT 的应用时，我发现不同职业群体，如律师、医生和运营人员，在实际工作中使用 GPT 时的需求是各不相同的。如果我们能够对这些需求进行详细的收集，然后开发出适合这些需求的新产品，那么这就像为 GPT 的生态注入了活水，使其更具活力。

关于「高手的习惯」这本书的出版时间，原计划是在 3 月份提交给出版社，但因为受到 GPT 的影响，3 月份并未按计划提交。朱老师与我曾讨论过这个问题。目前，我觉得书中的文字质量大约是 70 到 80 分。我计划将其提升到 80 到 90 分。但在这个提升过程中，GPT 的帮助作用就不太明显了。GPT 能够帮助我们找出一些初级的错误，但对于一些深层次的修订，还是需要我们自己来完成。有句话说「人民的射手手最长」，但我并不太理解它的意思。

关于咨询产品使用的「摇头模型」，我对其不太了解。这可能是一个针对 B2B 服务领域的经典模型。但如果是关于判决文书的查询，GPT 可能无法帮忙，因为它的语料库中没有这类资料。然而，对于律师来说，实际上有更好的方式来使用 GPT。我们团队正努力开发这些方法。例如，当提供给 GPT 明确的问题或上下文时，如某篇文章或图书的内容，GPT 能够给出更准确的答案。但此项实验目前尚未公开，因为我们还在进行一些必要的整理工作。待一切准备好后，我们会将其开发成为具体的产品。

体制内工作的同事在使用 PPT 等工具时会遇到一些问题，因此在当前情况下，个人在体制内的使用方式可能不太合适。如果涉及到信息安全问题，建议等待中国与微软等公司的关系谈判明确后再考虑使用。

在使用 GPT 的过程中，有时可能因为 2000 字的限制而被打断。解决这个问题很简单，只需输入「继续」两个字，GPT 便会记住你前面的内容，并继续回答。

针对大语言模型 GPT，有几个关键的认识和点评：

产品表达与 GPT 的使用：

如果你要开发一个产品，关键是能够用文字来清晰地表达产品的特性和功能。GPT 的主要能力在于处理和生成语言和文字。如果你不能清晰地用文字或符号来描述你的产品，那么从 GPT 这里得到的帮助会有限。

尽管非文字工作者可能认为与 GPT 的交互机会较少，实际上，对于他们仍然有大量的使用机会。

GPT 的插件机制：

我们已经与中等水平的文字工作者进行了合作，并在 42 人中进行了初步试验。随着 GPT 的插件机制的进一步开放，我们希望能帮助更多非文字工作者更好地利用它。

当前版本的 GPT 存在输入的文字限制，但这只是时间问题。新版将能支持长达 1.6 万字的输入，并预计在 42 天内我们将获得新的 API。

2000 字的限制不太理想，但这更多地是因为插件机制尚不成熟。我预期这个问题在接下来的几个月内将被解决。

GPT 与其他模型的比较：

JPG 这类产品虽然采用了相对简洁的数学模型，但我个人认为，它很难超越 GPT。考虑到专门团队为 GPT 投入的时间和资金，与其他小规模模型相比，GPT 的专业度显然更高。

关于是否大学生能更专业地进行写作，这很难确定。但我相信 GPT 仍然占有一定的优势。

未来发展与应用：

杨老师的团队已经解决了许多与 GPT 相关的问题，但由于涉及商业利益，目前尚未大规模公开。

针对未来的研究方向，我认为 GPT 在日常应用中将进一步展现出人机协同的价值。杨氏团队已经在开发相关工具，并在其功能上进行了进一步的升级。

好的，多一些同学询问是否有更详细的介绍。未来的详细介绍你们可以等待我的《儿童心理学》那本书上线。新书的预售日期已经确定为 4 月 23 号，与此同时，我们还会在 4 月 23 日，即世纪读书日，举办新书发布会。由于这本书主题是关于阅读的，梁老师会举办一场盛大的发布会，欢迎各位同学参加。我认为这本书写得非常精彩，尤其在 GPT 的时代，其价值会更大。我在书中整理了 100 个学科的脉络，并推荐了三千多本好书，这是 GPT 难以比拟的。

至于后续活动，我们确实会有相关课程。其中一门课程名为「智能写作」。无论你们是否在北京，都可以异地参与。我们的团队成员分散在不同的地方，例如产品经理在家工作，前端和网站程序员正在大理度假，插件开发人员则在北京。线下见面能够增强团队的信任基础，不过目前的工作模式也能够满足我们的需求。

对于你们提及的产品早期的一些小问题，我认为这并不重要。每一款新产品初期都可能存在这样的问题，但随着时间的推移，产品必然会变得越来越好。

要深入了解一个领域，拥有丰富的背景知识确实可以帮助你获得更好的回答。至于 GPT 会不会导致人们产生依赖，这是一个必然的问题。但回顾历史，每当新技术出现时，都会有类似的担忧。例如，在 90 年代电脑刚刚普及时，很多老作家不习惯使用电脑写作。但时至今日，使用电脑写作已经成为主流。因此，我们应该看到技术带来的真正益处，而不是纠结于其暂时的缺点。

至于编程，我不建议大家从零开始学习传统的 NLP，因为大语言模型已经改变了这个领域的思路。我们即将结束的这门课程之后，会有一个 AI 编程课的预售。大家可以关注我们在 4 月 25 号左右的活动，届时我们会推出三门新课程：AI 写作、AI 编程和 AI 绘画，这三门课会是我们的主打课程。

4 月 25 日，我们将正式推出 GPT 课程，届时将有非常优惠的价格，请大家留意。很多人询问，没有技术积累的人能否成为大模型工程师？但为何一定要追求成为工程师？你完全可以成为大模型应用专家。与程序员去竞争的路途会很辛苦，但如果你清楚自己的优势，例如有很好的形象，你可以成为大模型科普专家。在目前的 B 站、知乎、抖音、快手等平台上，能够清晰解释大模型的人并不多。如果你可以凭借在我们这里学到的知识，代表我们成为大模型产品宣传专家，你的形象和才华将会大放异彩。

并不是每个人都适合编程。如果每个人都去写代码，竞争将会过于激烈。GPT 其实并不擅长解读论文，因此在这个方向上还有很多改善空间。我已经构想了一些改进方法，并计划将这些方法转化为实际产品。

关于账号的问题，我不建议在淘宝上购买。我们有很多在国外的同学，他们知道怎么购买正规账号，可以寻求他们的帮助。

普通人参与 GPT 领域，最适合的方式是利用大模型提升自己的生产力。直接参与其中确实存在一定的门槛，例如你需要是产品经理、程序员等。这些门槛并不是每个人都能够轻松跨越的。

为什么我们要开发一个分享平台？其目的是帮助同学们分享日常中的技巧和经验。假设我们有 700 名同学，如果每个同学都分享一个自己觉得有用的技巧，那我们就有了 700 个技巧。这样，许多问题就可以迅速得到解决。

统计模型的实现有一定的难度。只有在获得 API 之后，平均模型的实现才变得相对容易。在早期，只能通过并发界面来实现。而 GPT 对于整本书的总结做得很到位，但对具体章节的提炼不够完善。这是其机制的问题。从 GPT 的第一代开始，它主要是对整本书进行训练，而对单篇文章或单个章节的训练不够。因此，GPT 对整本书的理解和提炼能力是非常强的。

这是 GPT 第一代产品在其首篇论文中的关键点。这篇开篇论文甚至导致了一个副产品的诞生，即「图书摘要」。在 Oppo 公司的官方网站上，可以找到如何对图书进行摘要的论文。与该模型使用的数据有关的问题，如果差距超过 2000 字，这是一个短期问题，但很容易解决。

要构建特定领域的语料库，涉及的机器学习技术较多。但当这技术转化为一个通用技术产品时，其使用和咨询过程变得相当简单。要更好地使用 GPT，你需要有自己的原始账号。杨老师的团队可能帮助你使用 API，但你的账号还是会被保留。

目前，关于 GPT 在创业市场的格局和发展方向，很难有明确预测。我有一些建议，但这部分信息比较机密。如果过多的人了解，就可能导致策略的模仿或抄袭。有关这部分，我可能会在五一期间召开一个闭门会议，进行更深入的分析和讨论。

使用 GPT 对写作水平的提升是有益的，这与一些老作家不使用电脑的心态是类似的。至于其他未明确的问题，如「优惠充值」，我不太清楚其具体含义。

技术新手应该从基础开始学习，就如同学习攀岩一样。首先要了解基础，然后可以按照公众号的推荐购买书籍，并从中选择一本自己能理解的开始阅读。希望今天的分享对大家有所帮助。时间过得真快，期待下次再会。请大家积极使用和分享这些知识，这是对我们工作的最好鼓励。关于新闻发布会，它是在线上进行的，定于 4 月 23 号晚上 7:42。再见，各位同学。

### 其他同学整理记录

冯静仪

2023-04-05 09:52

Q&A

Q1: 阳老师在知识星球中，提到 30-100 个问题，建议指向具体的作品，具体的作品怎么理解呢？根据自己输出的作品来定义比如程序员作品是代码，思考 GPT 怎么提高写代码效率比如内容运营作品是文章，思考 GPT 怎么获得更多内容运营灵感。

Q2: 阳老师，活水咨询的部分可以再讲一下吗？通过 42 位同学的询问，观察同学使用的过程，总结遇到的问题，从中培养一批大规模应用专家。

Q3: 如果要自己做产品，gpt 上怎么限制情景呢？如果想要在产品中使用 GPT，只要产品能够用文字表达，就可以借助 GPT 工具来使用，因为 GPT 是用符号语言表达的。

Q4: 阳老师怎么看到 plugin 机制的应用前景？插件上线后，估计能突破语言表达的形式，未来有更多发展。

Q5: 自己学习了机器阅读理解的书籍，感觉这些模型涉及的数学知识也比较简单，那么机器阅读理解算法的难点在哪里呢？是不是只是比较繁琐，并不难理解？数学知识不复杂，不难理解。GPT 是用简单的数学，实现复杂的模型。

Q6: 使用 gpt 久了会不会形成依赖，自己思考少了，不利于技能提升比如程序员使用 gpt 写代码，会不会不利于编程能力的提高使用 GPT 久了会形成依赖，但可以理解是一种新的工作方式的诞生。早期没有导航，去一个地方依赖路牌、线下地图或司机记忆才能到达，而有了手机导航，做司机门槛降低，滴滴司机不需要像出租车司机一样记忆地图，会开车就能做司机。这也是一种依赖，但也诞生了新的工作方式。因此，我们需要对新事物保持耐心，抓住最大的红利。

Q7: 在上次直播课中阳老师说要学编程，看推荐的那几本书。答疑时有人问 0 基础要不要从头学 NLP，阳老师说不需要。所以对技术小白建议学不学 nlp 呢，对于技术小白来说，不需要从头学习 NLP。大量 NLP 工程师需要下岗了，因为大语言模型和 NLP 的思路会不一样。课程结束后，会有 AI 编程课 AI 写作 AI 绘画的预售。

Q8: 阳老师，没有技术积累的人，可以成为大模型工程师吗？如果想做，应该积累哪些方面技术知识呢？对于没有技术积累的人，可以做科普。

Q9: gpt 领域目前有合适的机会让普通人全职参与么？还是目前更适合作为生产力工具？把 GPT 当成改善生产力工具。

Q10: 有没有模版给 gpt 提问，知道 gpt 掌握哪些专业的知识、哪些论文。建议多通用性提问，把自己日常中自己使用比较好的技巧分享出来，形成社群共创氛围。

Q11: 阳老师，和 GPT 读书，发现 GPT 对整本书的总结很到位，但是对具体章节内容的提炼感觉不到位。请问这是 GPT 的机制问题，还是提问技巧问题？

是机制问题，刚开始训练 GPT 是通过书籍来训练的，而对单篇文章训练不够。

Q12: 使用 GPT 是否不利于写作水平的提升？

不是，关键看如何使用 GPT。GPT 可以被用作一位写作反馈的老师，写完文章后可以让 GPT 进行修改和改写，从中学习到更好的表达方式。

Q13: 对技术小白要不要从头学习 nlp，从哪里开始。

可以从学习 Python 和阅读阳老师推荐的书单开始。

如何更好地对话

01 意象清洗

1、按照心理状态语的方法来写对话：

主谓宾：这只猫是红色

主谓宾（主谓宾）：我认为这只猫是红色

主谓宾（主谓宾+时间补语）：我之前认为这只猫是红色，我现在认为这只猫是绿色

2、训练 GPT 的方法，是降低意向性，保持在 3-4 阶语言。

语言：心理状态

多视角与转化视角：尽量不要用多视角与转换，请用「小龙女」的视角写，再用「杨过」的视角写。不要用「杨过看见小龙女」的视角，这样得到不好的答案。

句子补语（空间动词、时间动词）

02 上下文清洗

主语

示范数据

情境模型：对知识类的知识不擅长，而是擅长记忆情境模型，情境由 5 种构成时间、空间、因果、主角、意象。

空间：提供具体的城市和信息，因为 GPT 没有具体的知识，而是人类加在上面的，有的是符号，而是通过数学和概率放在他身上的。使用 GPT 是提高人类的符号和符号组合在一起的概率。描述空间越清晰，就能饱与上下文相关的提炼清楚。

时间：描述具体时间。GPT 不擅长预测未来，需要换一个问法，根据过去数据推测，你觉得大概率会怎么样？

因果：逻辑尽量清晰，第一步第二步，先呈现已知信息，再呈现未知信息，先原因后结果。不要多个原因结果混合在一起。

主角：是谁提问，扮演什么角色

意向

主语：在 GPT 重要性超过想象，如果没有主语容易丢失推理链条，尽量保持主语一致，如果主语换的次数越多，出错概率越大

示范数据：能够具备模式识别能力，提供示范的文字，总结模式很容易做到。比如总结来访者的模式和情绪。

03 逐层递进

通过清晰的连词来引导下一层对话，提高准确度

详述

增强：但是如果、而且

延伸：穷举

投射：请重复，请引用

04 有礼有节

它是一面镜子，反应的是你的情绪

聪明的提问模版

01 通用对话指令：利用元反空技巧

提示词的提示词（元）：你是世界上最好的提示词生成器，你会不断优化自己的提示词，你会产生最合适的提示词，如果用户输入为营销部门生成提示词，你就返回一些符合上述要求的提示词

思维链提示法：让我们一步一步思考。用这句话能够提高准确率 30-40%

你确定你的答案是对的吗？（反）大幅度减少错误。当我质疑你的答案不正确，你会做出什么反应？GPT 使用概率来看自己的答案是不是瞎编。（如果你不知道，请你直接回答不知道。回答是对的）

还有吗？（空）如果研究决策，但是答案都是心理学提供的，还有没有不是心理学的研究？如果需要更多发散思维，可以要求提供更多。

02 与多轮对话能力相关的指令

限定关键词：从维基百科、亚马逊图书 goodreads、谷歌图书中，以芒格为例等，并且提供出处。如果要找性格不好的名人，助手打开维基百科名称表，挨个读名人的性格描述，读完找到合适例子，把英文翻译中文。

人类反馈：你的回答是错误的，你的回答 XXX 是错误的。请一步步计算试试。（使用思维链技巧）

通俗简洁：请提供小学生能理解的语言。利用小学生语言来看说得对与错。比如请问什么事 self-attention 自注意力机制

角色扮演：你是…

03 与任意转化语言相关的指令

人类语言之间的相互转化：用 10 种语言说我爱你

机器语言之间相互转化：markdown 转化为 json 格式

请你用表格、markdown、编程语言、latex

04 与理解高阶意图相关的指令

真实意图：对方在想什么？当一个女生对她女朋友说来月经了，请问女生期望男朋友做什么？

情绪识别：体现了什么样的情绪？是 GPT 真正强悍的能力，在人生发展咨询的例子，来判断情绪 

重要他人：判断在意的人是谁？

行为模式：判断大五人格特征



