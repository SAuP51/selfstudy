## 2024009What-Is-Information

Information 2012, 3, 68-91; doi:10.3390/info3010068

What Is Information?: Why Is It Relativistic and What Is Its Relationship to Materiality, Meaning and Organization

Robert K. Logan 1,2

Received: 15 January 2012 / Accepted: 2 February 2012 / Published: 14 February 2012

Abstract

We review the historic development of concept of information including the relationship of Shannon information and entropy and the criticism of Shannon information because of its lack of a connection to meaning. We review the work of Kauffman, Logan et al. that shows that Shannon information fails to describe biotic information. We introduce the notion of the relativity of information and show that the concept of information depends on the context of where and how it is being used. We examine the relationship of information to meaning and materiality within information theory, cybernetics and systems biology. We show there exists a link between information and organization in biotic systems and in the various aspects of human culture including language, technology, science, economics and governance.

Keywords: information; language; Shannon; biology; relativity; meaning; organization

什么是信息：它为何是相对的以及与物质性、意义和组织的关系

Robert K. Logan 1,2

我们回顾了信息概念的历史发展，包括香农信息与熵的关系，以及由于其缺乏与意义的联系而对香农信息的批评。我们回顾了 Kauffman、Logan 等人的研究，表明香农信息未能描述生物信息。我们引入了信息相对性的概念，并展示了信息的概念取决于其使用的上下文。我们探讨了信息与意义和物质性在信息理论、控制论和系统生物学中的关系。我们展示了信息与生物系统中的组织之间，以及在人类文化的各个方面（包括语言、技术、科学、经济和治理）中的联系。

关键词：信息；语言；香农；生物学；相对性；意义；组织

### 01. Introduction

Information… arises… as natural selection assembling the very constraints on the release of energy that then constitutes work and the propagation of organization–Kauffman, Logan, Este, Goebel, Hobill and Shmulevich [1].

We have represented a discrete information source as a Markoff process. Can we define a quantity, which will measure, in some sense, how much information is ‘produced' by such a process, or better, at what rate information is produced?–Shannon [2].

To live effectively is to live with adequate information–Wiener [3].

Information is a distinction that makes a difference–MacKay [4].

Information is a difference that makes a difference–Bateson [5].

We live in the Information Age and we are surrounded by information. Thanks to "new media" like the Internet, the Web, blogs, email, cell phones, iPods, iPads, eReaders, Blackberries and iPhones we are blanketed in information—drowning in information according to some.

In addition to this everyday interaction with information by the users of computer-based digital "new media" there is the role that information plays in the sciences of artificial intelligence (AI) and artificial life (AL). In AI intelligence is posited to be a form of information that can be downloaded from the human brain onto a computer. In AL life is posited to be a form of information that first became embodied in carbon-based chemicals but now can exist in silicon-based computers. Some AL scientists like Edward Fredkin [6] insist that the universe is a computer and that life including human life is merely a program running on that computer.

The irony of our total immersion in information as well as the prominent role it plays in AI and AL is that for the most part we do not really have a clear understanding of exactly what information is. Information is not a simple straightforward idea but rather it is a very slippery concept used in many different ways in many different contexts. Linguistically and grammatically the word information is a noun but in actuality it is a process and hence is like a verb. A consideration of the concept of information gives rise to a number of interesting questions.

Is there only one form of information or are there several kinds of information? In other words is information an invariant or a universal independent of its frame of reference or context?

What is the relationship of information to meaning and organization?

Is information a thing like a noun or a process like a verb?

Is information material or is it a form of energy or is it just a pattern?

Is information a uniquely human phenomenon or do non-human forms of life contain information?

What is the relationship of energy and information?

These are some of the questions we will address in this article as we try to flesh out our understanding of exactly what it is that we call information. We will consider the historic development of the concept of information to get a handle on the exact meaning of this thing or process that defines our age and is also the engine of economic growth. We trace the development of the concept of information from the earliest uses of the word to the beginning of information theory as formulated by Shannon and Wiener, to MacKay's [4] critique of Shannon information [2], to Bateson's [5] formulation of information as the difference that makes a difference to the inclusion of information in biotic systems. We also examine the relationship of information, energy and entropy arguing, as have many physicists before us, that information and entropy are opposites and not parallel as suggested by Shannon.

In attempting to answer the questions we have formulated above, we will review the work of Kauffman, Logan, Este, Goebel, Hobill and Shmulevich [1] that demonstrated that Shannon information does not fully describe the information contained in a living organism. Shannon information was developed to deal with how to transmit information with as few errors as possible and not with such concerns as meaning or evolution. We next introduce the notion of the relativity of information and show that the concept of information depends on the context of where and how it is being used. Next we will examine the relationship of information to meaning and materiality within information theory, cybernetics and systems biology. And finally we examine the link between information and organization showing that in biotic systems that information and organization are intimately linked. We will also identify a similar link between information and organization in the various aspects of human culture including language, technology, science, economics and governance.

The literature on information theory is vast and it would be impossible to survey all of it. Two recent books that came out after the research reported here was completed is worth mentioning and that is The Theory of Information: Fundamentality, Diversity and Unification by Mark Burgin published in 2010 that provides an encyclopedic survey of information theory. The book A Brief Review of Molecular Information Theory by T. D. Schneider requires special mention because of the way in which Shannon information theory is used to study genetic systems and precisely characterize the sequence conservation at nucleic-acid binding sites. This seems to contradict our assertion that biotic information is different than Shannon information but as the reader will discover we are talking about different forms of information. The reader is also referred to this book that describes a relationship between energy and information.

01 引言

信息…… 产生…… 作为自然选择组装的能量释放的约束，这些约束构成了工作和组织的传播。

—— Kauffman、Logan、Este、Goebel、Hobill 和 Shmulevich [1]

我们已经将一个离散信息源表示为一个 Markov 过程。我们能否定义一个量，在某种意义上测量这种过程‘产生'了多少信息，或者更好的是，信息是以什么速率产生的？

—— Shannon [2]

有效地生活就是拥有足够的信息。

—— Wiener [3]

信息是产生差异的区别。

—— MacKay [4]

信息是产生差异的差异。

—— Bateson [5]

我们生活在信息时代，周围充满了信息。由于互联网、博客、电子邮件、手机、iPod、iPad、电子阅读器、Blackberry 和 iPhone 等「新媒体」的出现，我们仿佛被信息淹没了。

除了日常使用这些基于计算机的数字「新媒体」进行信息互动外，信息在人工智能（AI）和人工生命（AL）科学中的作用也十分重要。在 AI 领域，智能被认为是一种可以从人脑中提取并下载到计算机上的信息形式。在 AL 领域，生命被认为是一种最初体现在碳基化学物质中的信息形式，但现在可以存在于硅基计算机中。一些人工生命科学家，如 Edward Fredkin [6]，坚持认为宇宙就是一台计算机，生命，包括人类生命，只是运行在这台计算机上的程序。

讽刺的是，尽管我们完全沉浸在信息中，并且信息在 AI 和 AL 中扮演着重要角色，但我们大多数人并不真正清楚信息到底是什么。信息不是一个简单明了的概念，而是在许多不同的上下文中被以多种方式使用的复杂概念。从语言学和语法上讲，「信息」是一个名词，但实际上它更像是一个过程，有点像动词。对于信息概念的思考引发了许多有趣的问题。

信息只有一种形式，还是有多种形式？换句话说，信息是否是不变的，还是独立于其参考框架或上下文的普遍存在？

信息与意义和组织之间的关系是什么？

信息是像名词一样的物体，还是像动词一样的过程？

信息是物质，还是一种能量形式，或者只是一个模式？

信息是人类独有的现象，还是其他生命形式也包含信息？

能量和信息之间有什么关系？

在本文中，我们将探讨一些关于信息的问题，试图深入理解我们所称的信息究竟是什么。我们将回顾信息概念的历史发展，从而弄清这一概念的确切含义。信息不仅定义了我们的时代，也是经济增长的动力。我们将追溯信息概念的发展历程，从最早的使用到香农（Shannon）和维纳（Wiener）创建的信息理论，再到麦凯（MacKay）对香农信息的批判 [4]，以及贝特森（Bateson）提出的信息是「产生差异的差异」[5] 的观点，还包括信息在生物系统中的应用。我们也将探讨信息、能量和熵之间的关系。许多物理学家认为信息和熵是对立的，而不是香农所提出的并行关系。

在尝试回答我们提出的问题时，我们将回顾 Kauffman、Logan、Este、Goebel、Hobill 和 Shmulevich [1] 的工作，他们证明了 Shannon 信息并不能完全描述生物体包含的信息。Shannon 信息是为了解决如何以尽可能少的错误传输信息而开发的，并未考虑信息的意义或进化等问题。接下来，我们将介绍信息相对性的概念，展示信息的概念取决于其使用的背景和方式。然后，我们会探讨信息在信息论、控制论和系统生物学中的意义和物质性之间的关系。最后，我们将探讨信息与组织之间的联系，展示在生物系统中信息与组织的紧密联系。我们还将识别信息与组织在语言、技术、科学、经济和治理等各方面的人类文化中的相似联系。

信息论的文献非常广泛，不可能对所有文献进行全面调查。有两本在本文研究完成后出版的书籍值得一提：一本是 Mark Burgin 于 2010 年出版的《信息论：基础性、多样性和统一性》，该书对信息论进行了百科全书式的概述。另一本特别值得一提的是 T. D. Schneider 的《分子信息论简要回顾》，因为它使用 Shannon 信息论来研究遗传系统，并精确地表征核酸结合位点的序列保守性。这似乎与我们所说的生物信息不同于 Shannon 信息的观点相矛盾，但读者会发现我们讨论的是不同形式的信息。读者还可以参考这本书，它描述了能量和信息之间的关系。

2『原来自己在 2020 年已经下载了 Mark Burgin 的书籍「2020051Theory-of-Information」。而且也下载了他的另一本书「2020052Theory-of-Knowledge」，接下载了一本他和其他人合著的也是有关信息的书籍「2024040Information-and-Computation」。』

### 02. Origins of the Concept of Information

We begin our historic survey of the development of the concept of information with its etymology. The English word information according to the Oxford English Dictionary (OED) first appears in the written record in 1386 by Chaucer: "Whanne Melibee hadde herd the grete skiles and resons of Dame Prudence, and hire wise informacions and techynges." The word is derived from Latin through French by combining the word inform meaning giving a form to the mind with the ending "ation" denoting a noun of action. This earliest definition refers to an item of training or molding of the mind. The next notion of information, namely the communication of knowledge appears shortly thereafter in 1450. "Lydg. and Burgh Secrees 1695 Ferthere to geve the Enformacioun, of mustard whyte the seed is profitable".

The notion of information as a something capable of storage in or the transfer or communication to something inanimate and the notion of information as a mathematically defined quantity do not arise until the 20th century.

The OED cites two sources, which abstracted the concept of information as something that could be conveyed or stored to an inanimate object:

1937 Discovery Nov. 329/1 The whole difficulty resides in the amount of definition in the [television] picture, or, as the engineers put it, the amount of information to be transmitted in a given time.

1944 Jrnl. Sci. Instrum. XXI. 133/2 Information is conveyed to the machine by means of punched cards.

The OED cites the 1925 article of R. A. Fisher as the first instance of the mathematicization of information:

What we have spoken of as the intrinsic accuracy of an error curve may equally be conceived as the amount of information in a single observation belonging to such a distribution… If p is the probability of an observation falling into any one class, the amount of information in the sample is S{(∂m/∂θ)^2/m} where m = np, is the expectation in any one class [and θ is the parameter] [7].

Another OED entry citing the early work of mathematicizing information is that of R. V. L. Hartley [8]. "What we have done then is to take as our practical measure of information the logarithm of the number of possible symbol sequences." It is interesting to note that the work of both Fisher and Hartley foreshadow Shannon's concept of information, which is nothing more than the probability of a particular string of symbols independent of their meaning.

02 信息概念的起源

我们从信息概念的发展历史调查开始，首先从其词源说起。根据《牛津英语词典》(OED)，英语单词「information」首次出现在 1386 年，由 Chaucer 首次记录：「Whanne Melibee hadde herd the grete skiles and resons of Dame Prudence, and hire wise informacions and techynges」。这个词通过法语从拉丁语派生出来，由「inform」（意为给心智以形式）与表示动作名词的后缀「ation」组合而成。这个最早的定义指的是对心智的训练或塑造。接下来关于信息的另一种概念，即知识的交流，出现在 1450 年：「Lydg. and Burgh Secrees 1695 Ferthere to geve the Enformacioun, of mustard whyte the seed is profitable」。

直到 20 世纪，信息才被定义为可以存储或传输到非生命物体的概念，以及作为一个数学定义的量。

《牛津英语词典》引用了两个将信息抽象为可以传递或存储到非生命物体的来源：

1937 年《发现》11 月刊 329/1 整个难点在于 [电视] 图像的定义量，或者像工程师们所说的，在给定时间内要传输的信息量。

1944 年《科学仪器杂志》 XXI. 133/2 信息通过打孔卡传递给机器。

《牛津英语词典》引用了 R. A. Fisher 1925 年的文章作为信息数学化的首例：

我们所说的误差曲线的固有准确性，也可以理解为单个观测值在该分布中的信息量。如果 p 是观测值落入某一类别的概率，那么样本中的信息量是 S{(∂m/∂θ)^2/m}，其中 m = np，是某一类别的期望值【θ 是参数】[7]。

OED 的另一个条目引用了 R. V. L. Hartley 的早期工作 [8]，他对信息进行了数学化处理。他说：「我们所做的就是将信息的实际度量定义为可能的符号序列数量的对数。」有趣的是，Fisher 和 Hartley 的工作预示了 Shannon 的信息概念，即特定符号串的概率，而不考虑其意义。

### 03. Shannon and the Birth of Information Theory

Despite the early work of Fisher and Hartley cited above the beginning of the modern theoretical study of information is attributed to Claude Shannon [2], who is recognized as the father of information theory. He defined information as a message sent by a sender to a receiver. Shannon wanted to solve the problem of how to best encode information that a sender wished to transmit to a receiver. Shannon gave information a numerical or mathematical value based on probability defined in terms of the concept of information entropy more commonly known as Shannon entropy. Information is defined as the measure of the decrease of uncertainty for a receiver. The amount of Shannon information is inversely proportional to the probability of the occurrence of that information, where the information is coded in some symbolic form as a string of 0 s and 1 s or in terms of some α-numeric code. Shannon [2] defined his measures as follows:

We have represented a discrete information source as a Markoff process. Can we define a quantity, which will measure, in some sense, how much information is "produced" by such a process, or better, at what rate information is produced? Suppose we have a set of possible events whose probabilities of occurrence are p1, p2, ..., pn. These probabilities are known but that is all we know concerning which event will occur. Can we find a measure of how much "choice" is involved in the selection of the event or of how uncertain we are of the outcome? If there is such a measure, say H(p1, p2,..., pn)… we shall call H = pi logpi the entropy of the set of probabilities p1..., pn... The quantity H has a number of interesting properties, which further substantiate it as a reasonable measure of choice or information.

A story is told that Shannon did not know what to call his measure and von Neumann advised him to call it entropy because nobody knows what it means and that it would therefore give Shannon an advantage in any debate [9]. This choice was criticized by Wicken [10], who argued that in science a term should have only one meaning. Schneider and Sagan [11] referring to the use of the term entropy in both thermodynamics and information theory also suggests Shannon's use of the term is confusing when they wrote: "There is no simple correspondence between the two theories."

03 Shannon 与信息论的诞生

尽管 Fisher 和 Hartley 进行了早期的研究，但现代信息理论的开端被归功于 Claude Shannon [2]，他被誉为信息论之父。Shannon 将信息定义为发信者传递给接收者的消息。他致力于解决如何最佳编码发信者希望传输的信息的问题。Shannon 通过概率赋予信息一个数值或数学值，这一概率是根据信息熵（更常称为 Shannon 熵）的概念定义的。信息被定义为接收者不确定性的减少量。Shannon 信息量与信息发生的概率成反比，其中信息以某种符号形式编码为 0 和 1 的字符串或某些字母数字代码。Shannon [2] 这样定义了他的方法：

我们将离散信息源表示为一个马尔可夫过程。那么我们能否定义一个量来衡量这个过程「产生」信息的多少，或者说信息产生的速率呢？假设我们有一组可能的事件，其发生概率为 p1, p2, ..., pn。这些概率是已知的，但我们仅知道这些概率，不知道具体哪个事件会发生。我们能否找到一种方法来衡量选择事件时的「选择自由度」或我们对结果的不确定性呢？如果存在这种度量，比如 H（p1, p2,..., pn)... 我们将 H = pi logpi 称为概率集 p1..., pn... 的熵。熵 H 有许多有趣的性质，使它成为衡量选择或信息的合理度量。

有一个故事说 Shannon 不知道如何命名他的度量，von Neumann 建议他称之为熵，因为没有人知道它是什么意思，这样 Shannon 在任何辩论中都会占据优势 [9]。这个选择受到了 Wicken 的批评 [10]，他认为科学术语应该只有一个含义。Schneider 和 Sagan [11] 也提到在热力学和信息理论中使用熵这个术语，认为 Shannon 的用法令人困惑，他们写道：「这两种理论之间没有简单的对应关系。」

### 04. The Relationship of Information and Entropy

Understanding the efficiency of a steam engine through thermodynamics led Clausius to the idea of entropy as a measure of the mechanical unavailability of energy or the amount of heat energy that cannot be transformed into usable work. He referred to it in German as Verwandlungsinhalt, which may be translated roughly into English as "transformation content". Clausius then coined the term entropy deriving the root tropy from the Greek word trope (τροπή) meaning transformation. He added the prefix "en" because of the close association he felt that existed between energy and entropy. One can therefore roughly translate entropy from its etymology as energy transformation. Clausius felt the need to define entropy because the energy of the universe is conserved but its entropy is constantly increasing.

The relationship between entropy and probability is due to the work of Boltzman from his consideration of statistical mechanics, which is an alternative way of looking at thermodynamics. He showed that the entropy of a gas is proportional to the logarithm of W where W is the number of microstates of the gas that yield identical values of the thermodynamic variables of pressure, temperature and volume. The formula he derived, namely, that S = k lnW where k is the Boltzman constant is what inspired Shannon to call his expression for the measure of information content of a message information entropy despite the difference in sign and the fact that the proportionality constant or Boltzman constant has the physical dimensions of energy divided by temperature.

The relationship between entropy and information as developed by physicists arose from a consideration of Maxwell's demon and is quite opposite to the one proposed by Shannon. Maxwell in 1867 postulated a gedanken experiment in which a demon standing in a doorway between two rooms filled with gas would allow only fast moving molecules to pass from one room to another so as to create a temperature difference in the two rooms from which usable work could be extracted in violation of the second law of thermodynamics. Leo Szilard in 1929 analyzing the problem that Maxwell's Demon presented showed that to obtain the information he needed the demon caused an increase of entropy elsewhere such that the net entropy did not decrease. He suggested that the demon is only able to temporarily reduce entropy because it possesses information, which is purchased at the cost of an increase in entropy. There is no violation of the Second Law because acquisition of that information causes an increase of entropy greater than the decrease of entropy represented by the information. As a result of Szilard's analysis one must conclude that entropy and information are opposite. He also pointed out that the net energy gained by the demon was not positive because of the energy cost in obtaining the information by which the demon selected the fast moving molecules and rejecting the slow moving ones. Since the information was purchased at the cost of an increase in entropy the information has an effective net negative entropy. Following Szilard, Gilbert N. Lewis [12] also saw an inverse relationship between information and entropy. He wrote, "Gain in entropy always means loss of information, and nothing more".

Schrödinger [13] in his famous and highly influential book What is Life? first explicitly introduced the notion of negative entropy: 

Every process, event, happening—call it what you will; in a word, everything that is going on in Nature means an increase of the entropy of the part of the world where it is going on. Thus a living organism continually increases its entropy—or, as you may say, produces positive entropy—and thus tends to approach the dangerous state of maximum entropy, which is death. It can only keep aloof from it, i.e., alive, by continually drawing from its environment negative entropy—which is something very positive as we shall immediately see. What an organism feeds upon is negative entropy. Or, to put it less paradoxically, the essential thing in metabolism is that the organism succeeds in freeing itself from all the entropy it cannot help producing while alive (Chapter 6).

Both Wiener [3] and Brillouin [14] both adopted Shannon's definition of information and its relation to entropy with the one exception of its sign, likely influenced by the arguments of Szilard and Schrödinger [13].

Wiener [3] wrote,

Messages are themselves a form of pattern and organization. Indeed, it is possible to treat sets of messages as having entropy like sets of states in the external world. Just as entropy is a measure of disorganization, the information carried by a set of messages is a measure of organization. In fact, it is possible to interpret the information carried by a message as essentially the negative of its entropy, and the negative logarithm of its probability. That is, the more probable the message, the less information it gives (p. 39)…. This amount of information is a quantity which differs from entropy merely by its algebraic sign and a possible numerical factor.

Brillouin [14] also argued that a living system exports entropy in order to maintain its own entropy at a low level. Brillouin used the term negentropy to describe information rather than negative entropy.

The reason that Wiener and Brillouin consider entropy and information as opposites or regard information as negative entropy follows from the tendency in nature for systems to move into states of greater disorder, i.e., states of increased entropy and hence states for, which we have less information. Consider a system, which is in a state for which there is a certain finite number of possible configurations or microstates all of which are equivalent to the same macro state. The tendency of nature according to the second law of thermodynamics is for the number of microstates that are equivalent to the macrostate of the system to increase. Because there are more possible microstates as time increases and we do not know which particular microstate the system is in, we know less about the system as the number of possible microstates increases. It therefore follows that as the entropy increases the amount of information we have about the system decreases and hence entropy is negative information or vice￾versa information is the negative of entropy. In other words the second law of thermodynamics tell us that when system A evolves into system B that system B will have more possible redundant or equivalent micro states than system A and hence we know less about system B than system A since the uncertainty as to which state the system is in has increased.

Wiener and Brillouin relate information to entropy with a negative sign whereas Shannon uses a positive sign. Hayles [6] notes that although this difference is arbitrary it had a significant impact. Observing that Shannon used the positive sign she also noted that "identifying entropy with information can be seen as a crucial crossing point, for this allowed entropy to be reconceptualized as the thermodynamic motor driving systems to self-organization rather than as the heat engines driving the world to universal heat death." For Wiener, on the other hand she wrote, "life is an island of negentropy amid a sea of disorder [6]."

Despite the difference in the sign of information entropy assigned by Shannon and Wiener, Shannon was heavily influenced by Wiener's work as indicated by the way Shannon [2] credits Wiener for his contribution to his thinking in his acknowledgement: "Credit should also be given to Professor N. Wiener, whose elegant solution of the problems of filtering and prediction of stationary ensembles has considerably influenced the writer's thinking in this field." Shannon also acknowledges his debt to Wiener in footnote 4 of Part III:

Communication theory is heavily indebted to Wiener for much of its basic philosophy and theory. His classic NDRC report, The Interpolation, Extrapolation and Smoothing of Stationary Time Series, contains the first clear-cut formulation of communication theory as a statistical problem, the study of operations on time series. This work, although chiefly concerned with the linear prediction and filtering problem, is an important collateral reference in connection with the present paper. We may also refer here to Wiener's Cybernetics [3], dealing with the general problems of communication and control.

04 信息和熵的关系

通过研究蒸汽机的效率，Clausius 提出了熵的概念，认为熵是能量中无法转化为机械功的部分，或是无法利用的热能。他在德语中称其为「Verwandlungsinhalt」，大致可以翻译为「转化内容」。Clausius 进一步创造了「熵」这一术语，其中「tropy」来自希腊语 trope(τροπή)，意为转化。他加上了前缀「en」，因为他认为能量和熵之间有密切的关系。因此，从词源上看，熵可以理解为能量转化。Clausius 认为有必要定义熵，因为尽管宇宙中的能量是守恒的，但熵却不断增加。

熵与概率之间的关系是 Boltzman 通过统计力学研究发现的，统计力学是观察热力学的一种方法。他证明了气体的熵与 W 的对数成正比，其中 W 是气体在相同压力、温度和体积条件下的微观状态数。他推导出的公式 S = k lnW，其中 k 是 Boltzman 常数，这启发了 Shannon 将其用于度量信息内容的表达称为「信息熵」，尽管符号不同，且比例常数 Boltzman 常数具有能量除以温度的物理维度。

物理学家对熵和信息的关系进行了研究，这一研究源于对麦克斯韦妖的思考，而这与 Shannon 的观点完全相反。1867 年，麦克斯韦提出了一个思想实验：一个妖怪站在两个充满气体的房间之间的门口，只允许快速移动的分子从一个房间通过到另一个房间，从而在两个房间之间制造温差，并从中提取可用功，这违反了热力学第二定律。1929 年，Leo Szilard 通过分析麦克斯韦妖的问题，表明为了获得所需的信息，妖怪会在其他地方引起熵的增加，从而使得整体熵并没有减少。他提出，妖怪之所以能够暂时减少熵，是因为它掌握了信息，而这些信息的获取是以熵增加为代价的。因此，获取信息导致的熵增加大于信息所代表的熵减少，这样就没有违反热力学第二定律。根据 Szilard 的分析，可以得出结论，熵和信息是相对的。他还指出，妖怪获得的净能量并不是正的，因为获取信息需要消耗能量，而这些信息是妖怪选择快速移动分子和排除缓慢移动分子的依据。由于信息是以熵增加为代价获得的，因此信息具有有效的净负熵。继 Szilard 之后，Gilbert N. Lewis [12] 也看到了信息和熵之间的反向关系。他写道：「熵的增加总是意味着信息的丧失，仅此而已」。





Schrödinger [13] 在他著名的《生命是什么？》一书中首次明确提出了负熵的概念：

每个过程、事件，无论你如何称呼它，总之，自然界中发生的一切都会导致该部分世界的熵增加。因此，活的有机体会不断增加其熵，或者说产生正熵，从而趋向于最大熵的危险状态，即死亡。只有通过不断从环境中获取负熵，它才能避免这种情况，即保持活力。实际上，有机体所需的是负熵。换句话说，新陈代谢的本质在于有机体成功地摆脱了它在活着时不可避免产生的所有熵（第六章）。

Wiener [3] 和 Brillouin [14] 都采用了 Shannon 对信息及其与熵关系的定义，唯一的例外是其符号，这可能受到 Szilard 和 Schrödinger [13] 论点的影响。

Wiener [3] 写道：

信息本身是一种模式和组织形式。实际上，可以将一组信息看作是具有熵的，就像外部世界的状态集一样。正如熵是无序的度量，信息量是有序的度量。事实上，可以将信息量解释为熵的负值，以及其概率的负对数。也就是说，消息越可能，提供的信息量就越少（第 39 页）…… 这种信息量在代数符号和可能的数值因子上仅与熵不同。

Brillouin [14] 也认为，生命系统通过输出熵来保持其自身的低熵水平。Brillouin 使用「负熵」一词来描述信息，而不是负的熵。

Wiener 和 Brillouin 认为熵和信息是相反的，或者说信息是负熵。这是因为自然界中的系统倾向于进入更无序的状态，即熵增加的状态，这也意味着我们掌握的信息减少。设想一个系统处于某个状态，该状态有一定数量的可能配置或微观状态，这些微观状态都对应同一个宏观状态。根据热力学第二定律，自然界倾向于增加系统宏观状态对应的微观状态数量。随着时间推移，可能的微观状态越来越多，而我们无法确定系统处于哪个特定的微观状态，因此我们对系统的了解会减少。因此，熵增加意味着我们掌握的信息减少，熵是负信息，反之亦然，信息是负熵。换句话说，热力学第二定律告诉我们，当系统 A 演变为系统 B 时，系统 B 将有更多可能的冗余或等效微观状态，因此我们对系统 B 的了解比系统 A 少，因为系统状态的不确定性增加了。

Wiener 和 Brillouin 在将信息与熵联系起来时使用了负号，而 Shannon 则使用了正号。Hayles [6] 指出，尽管这种差异是任意的，但它产生了重大影响。她注意到 Shannon 使用正号，并评论道：「将熵与信息等同是一个重要的转折点，这使得熵被重新定义为驱动系统自组织的热力学引擎，而不是导致世界走向普遍热寂的热机。」对于 Wiener，她写道，「生命是无序海洋中的一个负熵岛屿 [6]。」

尽管 Shannon 和 Wiener 在信息熵符号的使用上存在差异，但 Shannon 受到了 Wiener 工作的深刻影响。Shannon [2] 在他的致谢中提到，「也应感谢 N. Wiener 教授，他对静态集群的过滤和预测问题的优雅解决方案极大地影响了我的思考。」Shannon 还在论文第三部分的脚注 4 中提到：

通信理论在很大程度上受到了 Wiener 的基本哲学和理论的影响。他的经典 NDRC 报告《静态时间序列的插值、外推和平滑》首次将通信理论明确为一个统计问题，即对时间序列的操作研究。尽管这项工作主要关注线性预测和过滤问题，但它是与本文相关的重要参考资料。我们还可以提到 Wiener 的《控制论》 [3]，这本书讨论了通信和控制的一般问题。

### 05. MacKay's Counter Revolution: Where Is the Meaning in Shannon Information?

According to Claude Shannon [2] his definition of information is not connected to its meaning. However, as Shannon suggested, information in the form of a message often contains meaning but that meaning is not a necessary condition for defining information. So it is possible to have information without meaning, whatever that means.

Not all of the members of the information science community were happy with Shannon's definition of information. Three years after Shannon proposed his definition of information Donald Mackay [4] at the 8th Macy Conference argued for another approach to understanding the nature of information. The highly influential Macy Conferences on cybernetics, systems theory, information and communications were held from 1946 to 1953 during which Norbert Wiener's newly minted cybernetic theory and Shannon's information theory were discussed and debated with a fascinating interdisciplinary team of scholars which also included Warren McCulloch, Walter Pitts, Gregory Bateson, Margaret Mead, Heinz von Foerster, Kurt Lewin and John von Neumann. MacKay argued that he did not see "too close a connection between the notion of information as we use it in communications engineering and what [we] are doing here… the problem here is not so much finding the best encoding of symbols… but, rather, the determination of the semantic question of what to send and to whom to send it." He suggested that information should be defined as "the change in a receiver's mind-set, and thus with meaning" and not just the sender's signal [6]. The notion of information independent of its meaning or context is like looking at a figure isolated from its ground. As the ground changes so too does the meaning of the figure. 

Shannon, whose position eventually prevailed, defined information in terms of the transmission of the signal and was not concerned with the meaning. The problem with MacKay's definition was that meaning could not be measured or quantified and as a result the Shannon definition won out and changed the development of information science. The advantage that Shannon enjoyed over MacKay by defining information as the signal rather than meaning was his ability to mathematicize information and prove general theorems that held independent of the medium that carried the information. The theorizing that Shannon conducted through his combination of electrical engineering and mathematics came to be known as information theory. It is ironic that the OED cites the first use of the term "information theory" as that of MacKay's who used the term in a heading in an article he published in the March 1950 issue of the Philosophical Magazine.

Shannon's motivation for his definition of information was to create a tool to analyze how to increase the ratio of signal to noise within telecommunications. People that shared MacKay's position complained that Shannon's definition of information did not fully describe communication. Shannon did not disagree–he "frequently cautioned that the theory was meant to apply only to certain technical situations, not to communication in general [2]." He acknowledged that his definition of information was quite independent of meaning; however, he conceded that the information that was transmitted over the telecommunication lines he studied often had meaning as the following quote from his original paper written at the Bell Labs indicates:

The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one that will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. [2]

I ask the reader to note that Shannon requires the number of possible messages to be finite as this will be a critical concern when we examine biotic information. I admire Shannon's frankness about his definition of information, which he devised to handle the engineering problems he faced. He was quite clear that his definition was not the unique definition of information but merely one definition of information suited for his engineering requirements. In the abstract to his paper, The Lattice Theory of Information he wrote, The word "information" has been given many different meanings by various writers in the general field of information theory. It is likely that at least a number of these will prove sufficiently useful in certain applications to deserve further study and permanent recognition. It is hardly to be expected that a single concept of information would satisfactorily account for the numerous possible applications of this general field. The present note outlines a new approach to information theory, which is aimed specifically at the analysis of certain communication problems in which there exist a number of information sources simultaneously in operation [2].

What I find extraordinary is that his definition of information limited in scope by his own admission became the standard by which almost all forms of information were gauged. There have been some slight variations of Shannon information like Kolmogorov information more often referred to as Kolmogorov complexity used to measure the shortest string of 0 s and 1 s to achieve a programming result or represent a text on a computer or a Turing machine. But despite these small variations Shannon information has been accepted as the canonical definition of information by all except for a small band of critics.

I have purposely bolded the term selected and selection in the above quote of Shannon to highlight the fact that Shannon's definition of information had to do with selection from a pre-determined set of data that did not necessarily have any meaning. MacKay used this selective element of Shannon information to distinguish it from his own definition of information, which, unlike Shannon, incorporates meaning explicitly. He also defended his definition from the attack that it was subjective.

Mackay's first move was to rescue information that affected the receiver's mindset from the "subjective" label. He proposed that both Shannon and Bavelas were concerned with what he called "selective information", that is information calculated by considering the selection of message elements from a set. But selective information alone is not enough; also required is another kind of information that he called "structural". Structural information indicates how selective information is to be understood; it is a message about how to interpret a message — that is, it is a metacommunication [6].

Structural information must involve semantics and meaning if it is to succeed in its role of interpreting selective or Shannon information. Structural information is concerned with the effect and impact of the information on the mind of the receiver and hence is reflexive. Structural information has a relationship to pragmatics as well as semantics where pragmatics tries to bridge the explanatory gap between the literal meaning of a sentence and the meaning that the speaker or writer intended. Shannon information has no particular relation to either semantics or pragmatics. It is only concerned with the text of a message and not the intentions of the sender or the possible interpretations of the receiver.

Part of the resistance to MacKay information was that its definition involved subjectivity, which orthodox scientists could not abide in their theories. Rather than deal with the fact that the exchange of information among humans involves a certain amount of subjectivity proponents of Shannon information theory chose to ignore this essential element of information and communications. Taken to its logical conclusion this attitude would limit science to study those areas that do not involve subjectivity, which would forever condemn linguistics and the other social sciences to non-scientific analysis. Rule out subjectivity in science or social studies and social science becomes a contradiction in terms.

This raises the question of whether subjectivity can be studied scientifically. I would suggest that an approach that parallels quantum physics is needed. Just as the measurement of sub-atomic particles changes their behaviour and requires a quantum mechanic representation that includes the Heisenberg Uncertainty principle, something similar is required for a science of the subjective—something I would call quantum rhetoric. What is the study of communications and media ecology after all but the study of how one set of subjective humans communicates with another set of subjective humans. Shannon successfully exorcised the subjectivity from communications, which was fine for his engineering objectives. I totally respect Shannon because he always warned that his definition was not intended to be a theory of communications. My problem is with those that misuse his work and over extend it.

05 MacKay 的反击：Shannon 信息中的意义何在？

根据 Claude Shannon [2] 的说法，他对信息的定义并不涉及其意义。不过，Shannon 也提到，信息在作为消息时通常包含意义，但这种意义并不是定义信息的必要条件。因此，信息可以没有意义，不管这意味着什么。

信息科学界并非所有成员都认同 Shannon 的信息定义。在 Shannon 提出定义的三年后，Donald Mackay [4] 在第 8 届 Macy 会议上提出了另一种理解信息本质的方法。极具影响力的 Macy 会议探讨了控制论、系统理论、信息和通信等领域，会议在 1946 年至 1953 年期间举行，Norbert Wiener 的新控制论理论和 Shannon 的信息理论在会议上被深入讨论和辩论，参与者包括 Warren McCulloch、Walter Pitts、Gregory Bateson、Margaret Mead、Heinz von Foerster、Kurt Lewin 和 John von Neumann 等跨学科学者。MacKay 认为，「我们在通信工程中使用的信息概念与我们在这里所探讨的问题并不完全一致…… 这里的问题不在于找到最佳的符号编码…… 而是确定要发送什么以及发送给谁。」他建议信息应被定义为「接收者心态的变化，因此与意义相关」，而不仅仅是发送者的信号 [6]。信息独立于其意义或上下文的概念如同将一个图形从其背景中孤立出来。背景变化时，图形的意义也会随之改变。

Shannon 的观点最终占了上风，他从信号传输的角度定义了信息，而不涉及其意义。MacKay 的定义问题在于意义无法测量或量化，因此 Shannon 的定义胜出并改变了信息科学的发展方向。Shannon 将信息视为信号而非意义的优势在于，他能够将信息进行数学化处理，并证明一些与信息载体无关的一般定理。Shannon 通过结合电气工程和数学所进行的理论研究被称为信息理论。讽刺的是，OED 引用了「信息理论」一词的首次使用，而这是 MacKay 在 1950 年 3 月的《哲学杂志》上发表的一篇文章的标题。

Shannon 定义信息的动机是为了创造一种工具，以分析如何提高电信中的信噪比。支持 MacKay 的人抱怨 Shannon 的定义未能完全描述通信的所有方面。Shannon 并不否认 —— 他「经常警告说，该理论仅适用于某些技术情况，而不适用于一般通信 [2]。」他承认，他对信息的定义与意义是独立的；然而，他也承认，他所研究的电信线路上传输的信息通常具有意义，正如他在 Bell Labs 撰写的原始论文中所述：

通信的基本问题是如何在一个地方精确或大致地重现从另一个地方选取的信息。通常，这些信息是有意义的，也就是说，它们与某些物理或概念实体相关联。然而，这些语义方面与工程问题无关。关键在于实际传递的信息是从一组可能的信息中选出的。系统必须针对每一种可能的选择进行设计，而不仅仅是针对最终会选择的那一种，因为在设计时无法预知具体选择。如果这组信息的数量是有限的，那么这个数量或者它的任何单调函数可以视为信息量的度量，因为所有选择都是同样可能的。[2]

我请读者注意，Shannon 要求可能的消息数量是有限的这一点，因为当我们研究生物信息时，这将是一个关键问题。我钦佩 Shannon 对他所定义的信息的坦率，他制定这个定义是为了处理他所面临的工程问题。他非常清楚自己的定义不是信息的唯一定义，而只是适合他工程需求的一个定义。在他的论文《信息的格论》(The Lattice Theory of Information）的摘要中，他写道，「信息」一词在信息理论的广泛领域中被不同的作者赋予了许多不同的含义。很可能至少有一些定义在某些应用中会被证明是足够有用的，值得进一步研究和永久认可。几乎不能期望单一的信息概念能够令人满意地解释这个广泛领域的众多可能应用。本文概述了一种新的信息理论方法，专门针对存在许多信息源同时运行的某些通信问题进行分析 [2]。

我认为非同寻常的是，他自己承认范围有限的信息定义却成为了衡量几乎所有形式信息的标准。在 Shannon 信息的基础上有一些细微的变化，例如 Kolmogorov 信息（更常被称为 Kolmogorov 复杂度），用于测量实现编程结果或在计算机或图灵机上表示文本的最短 0 和 1 串。但尽管有这些小变化，除了少数批评者外，Shannon 信息已被接受为信息的权威定义。

我在上面引用的 Shannon 语录中故意加粗了「selected」和「selection」这两个词，以强调 Shannon 对信息的定义是基于从预先确定的数据集中选择，而这些数据集不一定具有任何意义。MacKay 利用 Shannon 信息中的选择性元素来区分他自己的信息定义。与 Shannon 不同，MacKay 在定义中明确包含了意义。他还为自己的定义辩护，反驳了其主观性的指责。

Mackay 首先试图将影响接收者心态的信息从「主观」标签中解脱出来。他指出，Shannon 和 Bavelas 都关注所谓的「选择性信息」，即通过从一个集合中选择消息元素来计算的信息。但是，选择性信息本身并不够用；还需要另一种信息，MacKay 称之为「结构性信息」。结构性信息指示如何理解选择性信息；它是关于如何解释消息的消息 —— 也就是一种元沟通 [6]。

为了成功解释选择性或 Shannon 信息，结构性信息必须涉及语义和意义。结构性信息关注信息对接收者心灵的影响和效果，因此具有反思性。结构性信息与语用学和语义学有关，其中语用学试图弥合句子的字面意义与说话者或作者意图之间的差距。Shannon 信息则与语义学或语用学没有特别的关系。它只关注消息的文本，而不关注发送者的意图或接收者的解释。

部分科学家对 MacKay 信息的抵制，主要是因为其定义带有主观性，而正统科学家无法在他们的理论中接受主观性。相比处理人类之间的信息交换中所涉及的主观性，Shannon 信息理论的支持者选择忽略这一信息和通信中的基本要素。逻辑上，这种态度将科学研究局限在不涉及主观性的领域，这将使语言学和其他社会科学沦为非科学的分析领域。如果在科学或社会研究中排除主观性，社会科学就成了自相矛盾的术语。

这引发了一个问题：主观性是否可以被科学地研究？我认为需要一种类似量子物理学的方法。正如对亚原子粒子的测量会改变它们的行为，并需要一个包含海森堡不确定性原理的量子力学表述，对主观性的科学研究也需要类似的东西 —— 我称之为量子修辞学。通信和媒体生态的研究，其实就是研究一组主观的人如何与另一组主观的人交流。Shannon 成功地将主观性从通信中排除，这对他的工程目标来说是好的。我非常尊重 Shannon，因为他总是警告说，他的定义并非意在成为一个通信理论。我的问题在于那些滥用并过度延伸他工作的人。

### 06. Information: The Difference That Makes a Difference

Although Shannon's notion of information divorced from meaning became the central theme of information theory MacKay's counter-revolution was not without some effect and resulted in a slight shift in the way information was regarded. No doubt the reader is familiar with Gregory Bateson [4] famous definition of information as "the difference that makes a difference." Buried in this one-liner is the notion that it is the meaning of the information that makes the difference. Although Bateson gets credit for this idea it is likely that he was influenced by MacKay [3] who in his book Information, Mechanism and Meaning published four years before the appearance of Bateson's one-liner wrote: "information is a distinction that makes a difference." Bateson, MacKay and Shannon were all participants in the Macy conferences so Bateson was quite familiar with MacKay's ideas. The use of the term "distinction" in MacKay's one-liner is more closely tied to the idea of "meaning" than the term "difference". It is ironic that MacKay who pointed out the shortcomings of Shannon information, was the first to use the term "information theory" and was the first to point out that the importance of information is its meaning and the fact that it makes a difference. MacKay is certainly a scholar who made a difference and he deserves more credit and attribution than he usually receives. And both MacKay and Bateson deserve credit for reminding us that meaning is a critical part of information and that Shannon's notion of information is not the whole story.

Another one line definition of information that incorporates the notion of its meaning is this one by Ed Fredkin which I would put in a league with Mackay and Bateson's one-liners. "The meaning of information is given by the processes that interpret it." This is a very insightful definition because it explicitly incorporates the notion that information depends on context.

If information is the distinction (McKay) or the difference (Bateson) that makes a difference then if there is no distinction or no difference then there can be no information. This would mean chaos or random numbers contain no information because there is no difference or distinction in one part of the stream of numbers as opposed to another part of the stream because of a lack of organization. This is opposite to the conclusion of Shannon who claims that a stream of random numbers contains the maximum information. While it is true each element is different from the next and is a complete surprise it is also true that the overall pattern of chaos and randomness is the same and hence there is no distinction nor is there any difference in the stream of random numbers. A gas, which remains uniformly at the same temperature, pressure and volume, is constantly changing but one cannot make a distinction between the gas at one moment and the gas at another moment. There is no difference in the way the gas behaves at these different moments. The only information one can discern about the gas is its volume, pressure and temperature, which is unchanging. No work can be done by this gas. If however in this volume of gas there is a temperature differential then work can be extracted from the gas and there is information in the gas by virtue of the way in which the temperature differential is organized. This raises the question of whether or not organization is information, a point we will return to later in this paper once we have dealt with the nature of information in biotic systems.

06 信息：产生差异的差异

尽管 Shannon 将信息与意义分离的观点成为了信息理论的核心，但 MacKay 的反对意见也产生了一定的影响，导致人们对信息的看法有所改变。相信读者们对 Gregory Bateson [4] 著名的信息定义「引起差异的差异」已经耳熟能详。这句话其实暗示了信息的意义才是引起差异的关键。虽然 Bateson 因此得到了认可，但他很可能受到了 MacKay [3] 的影响。MacKay 在其书《信息、机制和意义》中写道：「信息是引起差异的区分。」这本书出版时间比 Bateson 的定义早了四年。Bateson、MacKay 和 Shannon 都参加了 Macy 会议，因此 Bateson 对 MacKay 的思想非常熟悉。相比之下，MacKay 使用的「区分」一词比「差异」更能体现与「意义」的联系。讽刺的是，MacKay 是指出 Shannon 信息理论缺陷的第一个人，也是最早使用「信息理论」一词的人，并强调信息的重要性在于其意义和影响。MacKay 确实是一位对信息理论有重要贡献的学者，他应当得到更多的认可和赞誉。Bateson 和 MacKay 都提醒我们，信息的意义至关重要，Shannon 的信息概念并不完整。

Ed Fredkin 也给出了另一种包含信息意义的定义，他说：「信息的意义由解释它的过程决定。」这个定义非常有见地，因为它明确指出了信息依赖于上下文这一概念。

如果信息是 McKay 所说的区分，或者 Bateson 所说的差异，那么没有区分或差异就不会有信息。这意味着混乱或随机数字不包含信息，因为数字流的某一部分与另一部分之间没有组织上的差异。这与 Shannon 的结论相反，他认为随机数字流包含最大的量的信息。虽然每个元素都是不同的并且是一个完全的惊喜，但整体的混乱和随机模式是相同的，因此在随机数字流中没有区分或差异。对于一个在相同温度、压力和体积下保持均匀的气体来说，尽管气体在不断变化，但人们无法区分某一时刻的气体与另一时刻的气体。在这些不同的时刻，气体的行为没有差异。人们只能辨别气体的体积、压力和温度，这些是不会变化的。这个气体无法做功。然而，如果这个气体体积中存在温度差异，那么可以从气体中提取功，并且气体中存在信息，因为温度差异的组织方式。这引出了一个问题：组织是否就是信息，我们将在本文后面对生物系统中的信息性质进行讨论时回到这个问题。

### 07. Information in Biotic Systems

We have seen that as early as 1925 the notion of information as an abstraction was first introduced by Fisher [7] and formalized by Shannon [2]. It was not long after this development that biologists also began to talk about information. The OED cites the first uses of the term in biology in 1953:

1953 J. C. ECCLES Neurophysiol. Basis Mind i. 1 We may say that all ‘information' is conveyed in the nervous system in the form of coded arrangements of nerve impulses.

1953 WATSON and CRICK in Nature 30 May 965/2 In a long molecule many different permutations are possible, and it therefore seems likely that the precise sequence of the bases is the code which carries the genetical information.

The use of information in this context was not the mathematicization of information as was done by Fisher and Shannon but rather information was thought of qualitatively as something capable of being transferred or communicated to or through a living organism or stored in a living organism in the form of nucleic acid.

07 生物系统中的信息

我们已经看到，早在 1925 年，Fisher [7] 首次引入了信息作为抽象概念，并由 Shannon [2] 进行了形式化。不久之后，生物学家也开始谈论信息。OED 引用了该术语在生物学中首次使用是在 1953 年：

1953 年，J. C. ECCLES 在《Neurophysiol. Basis Mind》一书中提到，我们可以说，所有的「信息」都是以神经脉冲的编码形式在神经系统中传递的。

1953 年，WATSON 和 CRICK 在 5 月 30 日的《Nature》杂志上发表文章指出，在长分子中存在许多不同的排列组合，因此精确的碱基序列很可能是携带遗传信息的代码。

在这里，「信息」并不像 Fisher 和 Shannon 那样被数学化，而是被认为可以传递或转移到生物体内，或者以核酸形式存储在生物体内的东西。

### 08. Life as Propagating Organization

Stuart Kauffman [15] defined an autonomous agent (or living organism) acting on its own behalf and propagating its organization as an autocatalytic system carrying out at least one thermodynamic work cycle. The relationship of the information found in living organisms to the kind of information treated in Shannon information theory was not clear even though a lot of attention has been given in recent times to the notion of information in biotic systems by those pursuing systems biology and bioinformatics. It was to examine this relationship that a group of us undertook a study to understand the nature and flow of information in biotic systems. This led to an article entitled Propagating Organization: An Enquiry (POE) authored by Kauffman, Logan, Este, Goebel, Hobill and Shmulevich [1] in which we demonstrated that Shannon information could not be used to describe information contained in a biotic system. We also showed that information is not an invariant independent of its frame of reference.

In POE we argued that Shannon's [2] classical definition of information as the measure of the decrease of uncertainty was not valid for a biotic system that propagates its organization. The core argument of POE was that Shannon information "does not apply to the evolution of the biosphere" because Darwinian preadaptations cannot be predicted and as a consequence "the ensemble of possibilities and their entropy cannot be calculated [1]." Therefore a definition of information as reducing uncertainty does not make sense since no matter how much one learns from the information in a biotic system the uncertainty remains infinite because the number of possibilities of what can evolve is infinitely non-denumerable. I remind the reader that in making his definition that Shannon specified that the number of possible messages was finite.

Instead of Shannon information we defined a new form of information, which we called instructional or biotic information, not with Shannon, but with constraints or boundary conditions. The amount of information will be related to the diversity of constraints and the diversity of processes that they can partially cause to occur. By taking this step, we embed the concept of information in the ongoing processes of the biosphere, for they are causally relevant to that which happens in the unfolding of the biosphere.

We therefore conclude that constraints are information and… information is constraints… We use the term "instructional information" because of the instructional function this information performs and we sometimes call it "biotic information" because this is the domain it acts in, as opposed to human telecommunication or computer information systems where Shannon information operates [1].

A living organism is an open system, which von Bertalanffy [16] "defined as a system in exchange of matter with its environment, presenting import and export, building-up and breaking-down of its material components." Instructional or biotic information may therefore be defined as the organization of that exchange of energy and matter. The fact that a biotic system is an open system can be used to argue against the association of instructional or biotic information with cybernetics because cybernetics focuses strictly on the flow of information and does not deal with the flow of energy and matter.

In POE [1] we argued that constraints acting as instructional information are essential to the operation of a cell and the propagation of its organization.

The working of a cell is, in part, a complex web of constraints, or boundary conditions, which partially direct or cause the events which happen. Importantly, the propagating organization in the cell is the structural union of constraints as instructional information, the constrained release of energy as work, the use of work in the construction of copies of information, the use of work in the construction of other structures, and the construction of further constraints as instructional information. This instructional information further constrains the further release of energy in diverse specific ways, all of which propagates organization of process that completes a closure of tasks whereby the cell reproduces [1].

In POE [1] we associated biotic or instructional information with the organization that a biotic agent is able to propagate. This contradicts Shannon's definition of information and the notion that a random set or soup of organic chemicals has more Shannon information than a structured and organized set of organic chemicals found in a living organism.

The biotic agent has more meaning than the soup, however. The living organism with more structure and more organization has less Shannon information. This is counterintuitive to a biologist's understanding of a living organism. We therefore conclude that the use of Shannon information to describe a biotic system would not be valid. Shannon information for a biotic system is simply a category error. A living organism has meaning because it is an autonomous agent acting on its own behalf. A random soup of organic chemicals has no meaning and no organization [1].

The key point that was uncovered in the POE analysis was the fact that Shannon information could be defined independent of meaning whereas biotic or instructional was intimately connected to the meaning of the organism's information, namely the propagation of its organization. Thus we see organization within a system as a form of information, which is a much more dynamic notion of information than Shannon information which is merely a string of symbols or bits.

According to Shannon's definition of information a set of random numbers transmitted over a telephone line would have more information than the set of even numbers transmitted over the same line. Once 2, 4, 6, 8, 10, 12 was received the receiver, who is assumed to be a clever person, would be able to correctly guess that the rest of the numbers to follow the sequence would be the set of even numbers. The random numbers have no organization but the even numbers are organized so the mystery of the relevance of Shannon information deepens as one must counter-intuitively conclude that information and organization can be at cross-purposes in Shannon's scheme of things.

This argument completely contradicts the notion of information of a system biologist who would argue that a biological organism contains information. It is by virtue of this propagating organization that an organism is able to grow and replicate, as pointed out by Kauffman [15] in Investigations. From the contradiction between Shannon and biotic information we already have a hint that there is possibly more than one type of information and that information is not an invariant like the speed of light in relativity theory, which is independent of its frame of reference. We also see that perhaps Shannon's definition of information might have limitations and might not represent an universal notion of information. After all Shannon formulated his concept of information as information entropy to solve a specific problem namely increasing the efficiency or the signal to noise ratio in the transmission of signals over telecommunication lines.

08 作为传播组织的生命

Stuart Kauffman [15] 将自主智能体（Autonomous Agent）或生物体定义为一种自催化系统，它能够自发地进行至少一个热力学工作循环并传播其组织。尽管近年来系统生物学和生物信息学的研究人员对生物系统中的信息概念给予了极大关注，但生物体中的信息与 Shannon 信息理论中的信息之间的关系仍不清楚。为此，我们团队进行了研究，旨在理解生物系统中的信息的本质和流动。这项研究导致了一篇题为《传播组织：一个探讨》（POE）的文章，由 Kauffman、Logan、Este、Goebel、Hobill 和 Shmulevich [1] 撰写。在这篇文章中，我们证明了 Shannon 信息理论无法描述生物系统中的信息。我们还指出，信息并不是一个独立于其参考框架的恒定值。

在 POE 中，我们指出 Shannon [2] 经典的信息定义，即信息是减少不确定性的度量，对于传播其组织的生物系统并不适用。POE 的核心论点是 Shannon 信息 "不适用于生物圈的进化"，因为达尔文的预适应是无法预测的，因此 "无法计算可能性的集合及其熵 [1]。" 所以，将信息定义为减少不确定性并没有意义，因为无论一个人从生物系统中的信息中学到多少，不确定性仍然是无限的，因为进化的可能性是无限且不可数的。需要提醒读者的是，Shannon 在定义信息时，假设可能的消息数量是有限的。

相对于 Shannon 信息，我们定义了一种新的信息形式，称为指导信息或生命信息。这种信息形式不是通过 Shannon 的方法来定义，而是通过约束或边界条件来定义。信息量将与约束的多样性及其部分引发的过程多样性相关。通过这种方式，我们将信息的概念嵌入到生物圈的持续过程之中，因为这些过程对生物圈的发展具有因果关系。

因此，我们得出结论，约束即信息，信息即约束。我们使用 "指导信息" 这个术语，是因为这种信息具有指导功能，有时我们也称其为 "生命信息"，因为它在生物领域中起作用，而不是在人类电信或计算机信息系统中发挥作用的 Shannon 信息 [1]。

活的有机体是开放系统，von Bertalanffy [16] 将其定义为「一个与其环境进行物质交换的系统，即物质的输入输出和物质成分的建立与分解。」因此，教学信息或生物信息可以定义为这种能量和物质交换的组织方式。由于生物系统是开放系统，这一事实可以用于反对将教学信息或生物信息与控制论联系起来，因为控制论严格关注信息流动，而不处理能量和物质的流动。

我们在 POE [1] 中认为，作为教学信息的约束对细胞的运作及其组织的传播至关重要。

细胞的运作部分依赖于一个复杂的约束网络或边界条件，这些约束部分地指导或引发了细胞内发生的事件。重要的是，细胞中的传播性组织是约束作为教学信息的结构联合、受约束的能量释放作为工作、工作在信息副本构建中的使用、工作在其他结构构建中的使用以及进一步作为教学信息的约束构建。这种教学信息进一步以多种特定方式约束能量的进一步释放，所有这些过程都传播了一种完成任务闭合的组织，从而使细胞得以复制 [1]。

我们在 POE [1] 中将生物信息或教学信息与生物智能体能够传播的组织联系起来。这与 Shannon 的信息定义和认为随机有机化学物质集合或汤比活有机体中结构化和组织化的有机化学物质集合具有更多 Shannon 信息的概念相矛盾。

生物体比有机化学物质「汤」更有意义。然而，具有更多结构和组织的活体生物却包含更少的香农信息。这与生物学家对活体生物的理解是相违背的。因此，我们得出结论，使用香农信息来描述生物系统是不合适的。对于生物系统来说，香农信息的使用存在类别错误。活体生物具有意义，因为它是一个自主智能体，为自身利益而行动。而随机的有机化学物质「汤」既没有意义也没有组织 [1]。

POE 分析揭示的关键点是，香农信息可以独立于意义来定义，而生物信息或指令信息则与生物体信息的意义密切相关，即其结构的传递。因此，我们将系统内的组织视为一种信息，这是一种比香农信息更为动态的信息概念，而香农信息仅仅是一串符号或比特。

根据香农对信息的定义，一组随机数字通过电话线传输比同一线路上传输的一组偶数具有更多的信息。假设接收者是一个聪明的人，一旦接收到 2、4、6、8、10、12 之后，他们将能够正确猜测接下来的一组数字将是偶数。虽然随机数字没有组织，但偶数是有组织的。因此，香农信息的相关性变得更加复杂，因为在香农的体系中，我们必须得出一个反直觉的结论，即信息和组织在某些情况下可能是相互对立的。

这个观点完全与系统生物学家的信息概念相矛盾，他们认为生物体内包含信息。正如 Kauffman 在《Investigations》中指出的那样 [15]，正是这种信息的传播使得生物体能够生长和复制。从 Shannon 信息和生物信息的矛盾中，我们可以推测可能存在多种信息类型，并且信息不像相对论中的光速那样是一个不变的常数，不依赖于其参考系。我们还发现 Shannon 对信息的定义可能有其局限性，未必能代表普遍的信息概念。毕竟，Shannon 提出信息熵的概念是为了解决特定问题，即提高电信线路上信号传输的效率或信噪比。

### 09. The Relativity of Information

Robert M. Losee [17] in an article entitled A Discipline Independent Definition of Information published in the Journal of the American Society for Information Science defines information as follows:

Information may be defined as the characteristics of the output of a process, these being informative about the process and the input. This discipline independent definition may be applied to all domains, from physics to epistemology.

The term information, as the above definition seems to suggest, is generally regarded as some uniform quantity or quality, which is the same for all the domains and phenomena it describes. In other words information is an invariant like the speed of light, the same in all frames of reference. The origin of the term information or the actual meaning of the concept is all taken for granted. If ever pressed on the issue, most contemporary IT experts or philosophers will revert back to Shannon's definition of information. Some might also come up with Bateson definition that information is the difference that makes a difference. Most would not be aware that the Shannon and Bateson definitions of information are at odds with each other. Shannon information does not make a difference because it has nothing to do with meaning; it is merely a string of symbols or bits. On the other hand, Bateson information, which as we discovered should more accurately be called MacKay information, is all about meaning. And thus we arrive at our second surprise, namely the relativity of information. Information is not an invariant like the speed of light, but depends on the frame of reference or context in which it is used.

We discovered in our review of POE that Shannon information and biotic or instructional information are quite different. Information is not an absolute but depends on the context in which it is being used. So Shannon information is a perfectly useful tool for telecommunication channel engineering. Kolmogorov [18] information, defined as the minimum computational resources needed to describe a program or a text and is related to Shannon information, is useful for the study of information compression with respect to Turing machines. Biotic or instructional information, on the other hand, is not equivalent to Shannon or Kolmogorov information and as has been shown in POE is the only way to describe the interaction and evolution of biological systems and the propagation of their organization.

Information is a tool and as such it comes in different forms just as screwdrivers are not all the same. They come in different forms, slot, square, and Philips—depending in what screw environment they are to operate. The same may be said of information. MacKay identified two main categories of information: selective information not necessarily linked to meaning and structural information specifically linked to meaning. Shannon information was formulated to deal with the signal to noise ratio in telecommunications and Kolmogorov complexity was intended to measure information content as the complexity of an algorithm on a Turing Machine. Shannon and Kolmogorov information are what MacKay termed selective information. Biotic or instructional information, on the other hand, is a form of structural information. The information of DNA is not fixed like Shannon selective information but depends on context like MacKay structural information so that identical genotypes can give rise to different phenotypes depending on the environment or context.

Although we introduced the notion of the relativity of information in POE we were unaware at the time of the formulation of a similar idea long ago by Nicholas Tzannes [6] on page 56. He "wanted to define information so that its meaning varied with context… [and] pointed out that whereas Shannon and Wiener define information in terms of what it is, MacKay defines it in terms of what it does [6]." Both Shannon and Wiener's form of information is a noun or a thing and MacKay's form of information is a verb or process. We associate instructional or biotic information with MacKay as it is a process and not with Shannon because DNA, RNA and proteins are not informational "things" as such but rather they catalyze "processes" and actions that give rise to the propagation of organization and hence the transmission of information—information with meaning at that. Put simply instructional information is structural information as the root of the word instructional reveals.

In addition to the Tzannes' notion of the relativity of information we were also unaware that Mark Burgin also developed the notion even earlier in 1994 [19] and wrote of it again in 2010 [20].

Another distinction between Shannon information and biotic or instructional information as defined in POE is that with Shannon there is no explanation as to where information comes from and how it came into being. Information in Shannon's theory arrives deus ex machina, whereas biotic information as described in POE arises from the constraints that allow a living organism to harness free energy and turn it into work so that it can carry out its metabolism and replicate its organization. Kauffman [15] has described how this organization emerges through autocatalysis as an emergent phenomenon with properties that cannot be derived from, predicted from or reduced to the properties of the biomolecules of which the living organism is composed and hence provides an explanation of where biotic information comes from.

09 信息的相对性

Robert M. Losee [17] 在《美国信息科学学会杂志》发表的文章《信息的跨学科定义》中，将信息定义如下：

信息可以被定义为一个过程输出的特征，这些特征能反映该过程及其输入。这种跨学科定义可适用于所有领域，从物理学到认识论。

如上所述，信息通常被视为一种统一的量或质，对所有领域和现象都是一致的。换句话说，信息像光速一样是不变的，在所有参考系中都相同。人们往往默认信息的起源或实际含义。如果深入探讨这个问题，大多数现代 IT 专家或哲学家会回到 Shannon 的信息定义。有些人可能还会提到 Bateson 的定义，即信息是「产生差异的差异」。但大多数人并不知道 Shannon 和 Bateson 的信息定义是互相矛盾的。Shannon 信息不会产生差异，因为它与意义无关，只是符号或比特的串。而 Bateson 信息（更准确地说是 MacKay 信息）则完全是关于意义的。因此，我们得出了第二个结论，即信息的相对性。信息不像光速那样是不变的，而是依赖于使用它的参考系或上下文。

我们在审查 POE 时发现，Shannon 信息与生物信息或指令信息有很大不同。信息不是绝对的概念，而是取决于使用它的上下文。因此，Shannon 信息在电信通道工程中是一个非常有用的工具。Kolmogorov [18] 信息被定义为描述程序或文本所需的最小计算资源，并且与 Shannon 信息相关，这对于研究图灵机上的信息压缩非常有用。而生物或指令信息则不同于 Shannon 或 Kolmogorov 信息，并且如 POE 所示，是描述生物系统的相互作用及其组织传播的唯一方法。

信息是一种工具，因此它有不同的形式，就像螺丝刀有多种形状。它们有槽型、方型和菲利普型等多种形式 —— 取决于所操作的螺丝环境。信息也是如此。MacKay 识别出两种主要的信息类别：选择性信息和专门与意义相关的结构性信息。Shannon 信息是为处理电信中的信噪比而制定的，Kolmogorov 复杂度旨在衡量图灵机上算法的复杂性。Shannon 和 Kolmogorov 信息是 MacKay 所谓的选择性信息。另一方面，生物或指令信息是一种结构性信息。DNA 的信息不像 Shannon 选择性信息那样固定，而是像 MacKay 结构性信息那样依赖于上下文。因此，相同的基因型可以根据环境或上下文产生不同的表型。

虽然我们在 POE 中引入了信息相对性的概念，但当时并不知道 Nicholas Tzannes 早在很久以前的第 56 页中提出了类似的想法。他「希望定义信息，使其含义随上下文变化... 并指出 Shannon 和 Wiener 是根据信息的本质定义信息，而 MacKay 则是根据信息的作用定义信息 [6]。」Shannon 和 Wiener 的信息形式是名词，也就是一种事物，而 MacKay 的信息形式则是动词，即一种过程。我们将指令性或生物信息与 MacKay 关联，因为它是一个过程；而不是与 Shannon 关联，因为 DNA、RNA 和蛋白质本质上不是信息「事物」，而是催化「过程」和行动，从而促成组织的传播和信息的传递 —— 这些信息是有意义的。简单来说，指令信息就是结构信息，正如「指令」一词的词根所揭示的那样。

除了 Tzannes 的信息相对性概念外，我们还不了解 Mark Burgin 早在 1994 年 [19] 就已经发展了这一概念，并在 2010 年 [20] 再次写到它。

Shannon 信息与 POE 中定义的生物或指令信息之间的另一个区别在于，Shannon 没有解释信息的来源及其形成方式。在 Shannon 的理论中，信息是凭空出现的；而在 POE 中描述的生物信息则源于生物体利用自由能并将其转化为工作，以执行其代谢和复制其组织的约束。Kauffman [15] 描述了这种组织如何通过自催化作为一种涌现现象出现，这种现象具有无法从组成生物体的生物分子的属性中推导、预测或简化的特性，从而解释了生物信息的来源。

### 10. Information and Its Relationship to Materiality and Meaning

O, that this too too solid flesh would melt—Shakespeare's Hamlet (Act 1, Scene 2)

Where is the wisdom we have lost in knowledge?

Where is the knowledge we have lost in information?—TS Eliot

Where is the meaning we have lost in information?—RKL

To drive home the point that information is not an invariant but rather a quantity that is relative to the environment in which it operates we will now examine the relationship of information to materiality and meaning drawing on the work and insights of Katherine Hayles [6]. She points out that although information is used to describe material things and furthermore is instantiated in material things information is not itself material. "Shannon's theory defines information as a probability function with no dimension, no materiality, and no necessary connection with meaning. It is a pattern not a presence [6]".

The lack of a necessary connection to meaning of Shannon information is what distinguishes it from biotic information. Biotic information obviously has meaning, which is the propagation of the organism's organization. Information is an abstraction we use to describe the behavior of material things and often is sometimes thought of as something that controls, in the cybernetic sense, material things.

Hayles [6] traces the origin of information theory to cyberneticians like Wiener, von Forester and von Bertalanffy and telecommunication engineers like Shannon and Weaver. She points out that they regarded information as having a more primal existence than matter. Referring to the information theory they developed she wrote: "It (information theory) constructs information as the site of mastery and control over the material world".

She further claims, and I concur, that Shannon and cybernetic information is treated as separate from the material base in which it is instantiated. Wiener [2], for example, wrote in his book Cybernetics, or Control and Communication in the Animal and the Machine that "information is information, not matter or energy". The question that arises is whether or not there is something intrinsic about information or is it merely a description of or a metaphor for the complex patterns of behavior of material things. Does information really control matter or is information purely a mental construct based on the notion of human communication through symbolic language, which in turn is a product of conceptual thought as described in Logan [21]?

While it is true that the notion of information as used by the cyberneticians like Wiener, von Forester and von Bertalanffy and that used by Shannon and Weaver influenced each other and in the minds of many were the same they are actually quite different from each other. The notion of information as the master or controller of the material world is the view of the cyberneticians beginning with Wiener [3]: "To live effectively is to live with adequate information. Thus, communication and control belong to the essence of man's inner life, even as they belong to his life in society".

For communication engineers information is just a string of symbols that must be accurately transmitted from one location, the sender, to another location, the receiver. Their only concern is the accuracy of the transmission with the relationship to the meaning of the information being meaningless to their concerns. If we consider the relationship of information and meaning for the moment then there is a sense in which the cybernetician's notion of information has meaning as a controller of the material realm whereas Shannon information has no relationship as such to meaning. In fact one can question if Shannon's used the correct term "information" when he described H = pi logpi as the measure of "information". The quantity H he defined is clearly a useful measure for engineering in that it is related to the probability of the transmission of a signal—a signal that might or might not contain meaning. It is my contention that a signal without meaning is not truly information. I agree with MacKay and Bateson that to qualify as information the signal must make a difference, as is also the case with the way Wiener defines information in the context of cybernetics. Sveiby reports that Shannon himself had some second thoughts about the accuracy of his use of the term ‘information':

Shannon is said to have been unhappy with the word "information" in his theory. He was advised to use the word "entropy" instead, but entropy was a concept too difficult to communicate so he remained with the word. Since his theory concerns only transmission of signals, Langefors [22] suggested that a better term for Shannon's information theory would therefore perhaps be "signal transmission theory" (from the following Web site visited on 9/9/07: http://sveiby.com/portals/0/articles/Information.html#Cybernetics).

I find myself in agreement with Langefors that what Shannon is analyzing in his so-called information theory is the transmission of signals or data. It is consistent with some of my earlier work in the field of knowledge management and collaboration theory, in part inspired by the work of Karl Erik Sveiby, where Louis Stokes and I developed the following definitions of data, information, knowledge and wisdom:

- Data are the pure and simple facts without any particular structure or organization, the basic atoms of information,

- Information is structured data, which adds more meaning to the data and gives them greater context and significance,

- Knowledge is the ability to use information strategically to achieve one's objectives, and

- Wisdom is the capacity to choose objectives consistent with one's values and within a larger social context [23].

I also found the following description of the relationship of data and information that I accessed on Wikipedia on September 12, 2007 particularly illuminating:

Even though information and data are often used interchangeably, they are actually very different. Data is a set of unrelated information, and as such is of no use until it is properly evaluated. Upon evaluation, once there is some significant relation between data, and they show some relevance, then they are converted into information. Now this same data can be used for different purposes. Thus, till the data convey some information, they are not useful.

I would interpret the signals transmitted between Shannon's sender and receiver as data. Consistent with MacKay and Bateson's position information makes a difference when it is contextualized and significant. Knowledge and wisdom represent higher order applications of information beyond the scope of this study. The contextualization of data so that it has meaning and significance and hence operates as information is an emergent phenomenon. The communication of information cannot be explained solely in terms of the components of the Shannon system consisting of the sender, the receiver and the signal or message. It is a much more complex process than the simplified system that Shannon considered for the purposes of mathematicizing and engineering the transmission of signals. First of all it entails the knowledge of the sender and the receiver, the intentions or objectives of the sender and the receiver in participating in the process and finally the effects of the channel of communication itself as in McLuhan's [24] observation that "the medium is the message". The knowledge and intention of the sender and the receiver as well as the effects of the channel all affect the meaning of the message that is transmitted by the signal in addition to its content.

10 信息及其与物质性和意义的关系

哦，但愿这坚硬的肉体能够融化 —— 莎士比亚《哈姆雷特》（第一幕，第二场）

我们丢失在知识中的智慧何在？

我们丢失在信息中的知识何在？——TS Eliot

我们丢失在信息中的意义何在？——RKL

为了说明信息并不是一个固定不变的量，而是一个依赖于其所处环境的量，我们将借助 Katherine Hayles 的研究和见解来探讨信息与物质性和意义的关系 [6]。她指出，虽然我们用信息来描述物质事物，并且信息也体现于物质事物中，但信息本身并不是物质。「香农的理论将信息定义为一个没有维度、没有物质性、且不必然与意义相关的概率函数。信息是一种模式，而不是一种存在 [6]」。

香农信息与意义之间缺乏必然联系，这使得它与生物信息有所不同。生物信息显然是有意义的，因为它代表了生物体组织的传播。信息是我们用来描述物质事物行为的抽象概念，通常在控制论的意义上被认为是控制物质事物的手段。

Hayles [6] 追溯了信息理论的起源，涉及到控制论学者如 Wiener、von Forester 和 von Bertalanffy，以及电信工程师如 Shannon 和 Weaver。她指出，他们认为信息的存在比物质更加原始。她在描述他们开发的信息理论时写道：「信息理论将信息构建为对物质世界进行掌控和控制的关键」。

她进一步声称（我同意这一点），香农和控制论学者所说的信息被视为与其存在的物质基础分离。例如，Wiener [2] 在他的书《控制论：动物与机器中的控制与通信》中写道：「信息是信息，不是物质或能量」。这引出了一个问题：信息是否有其固有的特性，还是它仅仅是对物质事物复杂行为模式的描述或比喻？信息真的能够控制物质，还是信息只是基于人类通过符号语言进行交流这种观念的心智构建，如 Logan [21] 描述的那样？

虽然控制论学者如 Wiener、von Forester 和 von Bertalanffy 使用的信息概念与香农和 Weaver 的信息概念相互影响，并且在许多人看来它们是相同的，但实际上它们是截然不同的。控制论学者，如 Wiener [3]，认为信息是物质世界的主控或控制者：「有效地生活就是拥有足够的信息。因此，交流与控制是人类内心生活的本质，正如它们是人类社会生活的本质」。

对于通信工程师来说，信息只是需要准确传输的一串符号，从发送方到接收方。他们只关心传输的准确性，而不在乎信息的意义。如果我们暂时考虑信息与意义的关系，从某种角度看，控制论者认为信息在物质领域中具有控制作用，而香农信息则与意义无关。实际上，香农在用 H = p_i log p_i 来度量「信息」时，是否使用了正确的术语是可以质疑的。他定义的量 H 对工程师来说确实有用，因为它与信号传输的概率有关 —— 信号可能有意义，也可能没有意义。我认为，没有意义的信号不是真正的信息。我同意 MacKay 和 Bateson 的观点，要成为信息，信号必须产生影响，这也是 Wiener 在控制论中对信息的定义。Sveiby 报告说，香农自己对使用「信息」一词的准确性也有些顾虑：

据说，香农对其理论中的「信息」一词不太满意。他被建议使用「熵」一词，但「熵」这个概念太难理解，所以他保留了「信息」这个词。由于他的理论只涉及信号的传输，Langefors [22] 建议，香农的信息理论或许更适合称为「信号传输理论」（访问日期：2007 年 9 月 9 日：http://sveiby.com/portals/0/articles/Information.html#Cybernetics）。

我同意 Langefors 的观点，即 Shannon 在他的信息理论中主要分析的是信号或数据的传输。这与我在知识管理和协作理论领域的一些早期工作相契合。这些工作部分受到 Karl Erik Sveiby 的启发。在此基础上，我和 Louis Stokes 制定了以下关于数据、信息、知识和智慧的定义：

- 数据是纯粹的简单事实，没有特定的结构或组织，是信息的基本单位，
- 信息是结构化的数据，增加了数据的意义和背景，
- 知识是能够战略性地使用信息来实现目标的能力，
- 智慧是选择与自身价值观一致并在更大社会背景下的目标的能力 [23]。

我还发现了以下关于数据和信息关系的描述，特别是在 2007 年 9 月 12 日访问维基百科时看到的：

尽管信息和数据经常被混用，但它们实际上是不同的。数据是一组未关联的信息，未经评估前没有任何用处。一旦数据被评估并显示出显著的关系和相关性，它们就转化为信息。这时，这些数据可以用于不同的目的。因此，数据只有在传达信息时才有用。

我将 Shannon 的发送者和接收者之间传输的信号视为数据。根据 MacKay 和 Bateson 的观点，信息在被情境化并具有重要意义时才会产生不同的影响。知识和智慧代表了信息的更高层次应用，但这不在本研究的范围内。将数据情境化，使其具有意义和重要性，从而成为信息，是一种涌现现象。信息的传递不能仅靠 Shannon 系统中的发送者、接收者和信号或消息来解释。这一过程远比 Shannon 为数学化和工程化信号传输所设计的简化系统复杂得多。首先，这需要发送者和接收者的知识，其次是发送者和接收者参与过程的意图或目标，最后是通信渠道本身的影响，就像 McLuhan [24] 所说的「媒介即信息」。发送者和接收者的知识和意图以及通信渠道的影响，都会影响信号传递的消息的意义，而不仅仅是它的内容。

### 11. The Meaning of Information in Biotic Systems

Biotic or instructional information, defined in POE as the constraints that allow an autonomous agent, i.e., a living organism, to convert free energy into work so that the living organism is able to propagate its organization through growth and replication, is intimately connected with meaning. "For Shannon the semantics or meaning of the message does not matter, whereas in biology the opposite is true. Biotic agents have purpose and hence meaning [1]". One can therefore argue that since the meaning of instructional information is propagating organization that we finally understand the meaning of life–the "meaning of life" is propagating organization. This remark is not meant to trivialize the great philosophical quest for the meaning of life from a human perspective but there is a sense in which the meaning of life including human life is indeed the propagation of organization. The purpose of life is the creation or propagation of more life.

In addition to the fact that Shannon information does not necessarily entail meaning whereas biotic or instructional information always entails meaning there is one other essential difference between the two. Shannon information is defined independent of the medium of its instantiation whereas biotic information is very much tied to its material instantiation in the nucleic acids and proteins of which it is composed. The independence of Shannon and cybernetic information from the medium of its instantiation is what gives rise to the notion of strong artificial intelligence and claims like those of Moravic, Minsky, and to a certain extent Wiener, that human intelligence and the human mind can somehow be transferred to a silicon-based computer and does not require the wet computer of the human brain. Shannon and cybernetic information can be transferred from one material environment to another, from one computer to another or in the case of Shannon information from one telephone to another or from a computer to a hard copy of ink on paper. This is not the case with living organisms in the biosphere where information is stored in DNA, RNA and proteins.

Shannon information whether on paper, a computer, a DVD or a telecommunication device, because it is symbolic, can slide from one medium or technology to another and not really change, McLuhan's [24] "the medium is the message" aside. This is not true of living things. Identical genotypes can produce very different phenotypes depending on the physical and chemical environment in which they operate. Consider the fact that identical twins are not "identical". The reason identical twins are not "identical" is that the environment in which the biochemical interactions between biomolecules takes place alters the outcome.

11 生物系统中信息的意义

生物或指令信息，在 POE 中被定义为对自主智能体（如活的有机体）的约束，这些约束使其能够将自由能转化为工作，从而通过生长和复制传播其自身的结构，这与「意义」密切相关。「对于 Shannon 来说，消息的语义或意义并不重要，而在生物学中，情况正好相反。生物智能体有其目的，因此也有其意义 [1]」。因此，可以说，指令信息的意义在于传播结构，我们最终理解了生命的意义 ——「生命的意义」就是传播结构。这并不是要淡化从人类角度对生命意义的伟大哲学探索，但可以说，包括人类生命在内的生命意义的确是结构的传播。生命的目的是创造或传播更多的生命。

相比于香农信息不一定包含意义，生物或指令性信息总是包含意义，这两者之间还有一个本质的区别。香农信息的定义不依赖于其载体，而生物信息则与其在核酸和蛋白质中的物质载体密切相关。香农和控制论信息可以独立于其载体，这使得强人工智能（Strong AI）的概念得以提出。像 Moravic、Minsky 和 Wiener 等学者声称，人类智能和思维可以转移到基于硅的计算机上，而不需要依赖于人类大脑的「湿计算机」。香农和控制论信息可以从一个物质环境转移到另一个，从一台计算机转移到另一台，或者在香农信息的情况下，从一部电话转移到另一部电话，甚至从计算机转移到纸上的硬拷贝。然而，在生物圈中的生物体中，信息是储存在 DNA、RNA 和蛋白质中的，这种转移是不可行的。

香农信息无论是在纸上、计算机上、DVD 上还是在电信设备上，都因为它是符号性的，可以在不同媒介或技术之间转换而不发生实质性变化，撇开 McLuhan 的「媒介即信息」(The medium is the message）[24] 不谈。这在生物体中则不成立。相同的基因型在不同的物理和化学环境中可以产生非常不同的表型。比如说，虽然双胞胎有相同的基因，但他们并不完全「相同」。这是因为生物分子之间的生化相互作用在不同的环境中会产生不同的结果。

### 12. The Materiality of Information in Biotic Systems

Information is information, not matter or energy. No materialism which does not admit this can survive at the present day.

– Norbert Wiener [2] 

Shannon's theory defines information as a probability function with no dimension, no materiality, and no necessary connection with meaning. It is a pattern not a presence [6].

Shannon information cannot be, nor was it meant to be, naively applied to complete living organisms, because the information in a biotic system like DNA is more than a pattern—it is also a presence. A receptor for food or toxins is not just a pattern—it is also a presence. A biological system is both an information pattern and a material object or more accurately information patterns instantiated in a material presence. Schrödinger [13] long ago before the discovery of DNA described this dual aspect of chromosomal material metaphorically. "The chromosome structures are at the same time instrumental in bringing about the development they foreshadow. They are law-code and executive power–or, to use another simile, they are architect's plan and builder's craft–in one." It is the dynamic of the interaction between the patterns of information and the material composition of the biotic agents that determines their behavior.

As previously discussed, the issue hinges on the degree to which one can regard a biotic agent as a fully physical computational system. It is clear that a biotic system cannot be described only by Shannon information for which the information is abstracted from it material instantiation and is independent of the medium. The same argument can be made for the inappropriateness of Kolmogorov complexity for biotic systems. Kolmogorov complexity, which is defined with respect to Turing machines, is another case where the information pattern is separated from its material instantiation. Biology is about material things not just mathematical patterns. As Kubie once warned at one of the Macy conferences, "we are constantly in danger of oversimplifying the problem so as to scale it down for mathematical treatment [6]". As noted above the physical environment changes the meaning of the information embedded in the DNA of the genome.

Another way to distinguish the difference between biotic or instructional information and either Shannon or Kolmogorov complexity is that the latter are symbolic which is not the case for biotic or instructional information. The information coded in the chemical alphabet of biomolecules that make up living organisms acts through the chemical interactions of those biomolecules. "DNA is a molecule interacting with other molecules through a complex set of mechanisms. DNA is not just some text to be interpreted, and to regard it as such is an inaccurate simplification [25]". It is not the symbolic nature of DNA that gives rise to messenger RNA and it is not the symbolic nature of RNA that gives rise to proteins but rather the chemical properties of DNA that produce or catalyze the production of RNA and the chemical properties of RNA that produce or catalyze proteins and the chemical properties of proteins that carry out the protein's various functions such as:

1. Serving as enzymes to catalyze biochemical reactions vital to metabolism;

2. Providing structural or mechanical functions, such as building the cell's cytoskeleton;

3. Playing a role in cell signaling, immune responses, cell adhesion and the cell cycle.

DNA, RNA and proteins are both the medium and the content, the message and the messenger. Not so for Shannon and Kolmogorov information where one can distinguish between the medium and the message, the message and the messenger. The message is the information, which operates independent of the medium in which it is instantiated, McLuhan [24] aside. For biotic information, on the other hand, the medium and the message are the same—they cannot be separated. For biotic information the medium is the message in the McLuhan sense and it is also the content. For human symbolic information described by Shannon information, the information or content and the medium are quite separate. For biotic systems not only is the medium the message in the McLuhan sense that a medium has an effect independent of its content but the medium is also the content because it is the chemical properties of the medium that affects the organism. In fact the medium is the message because it is literally the content and the content of the message is unique to that medium and is instantiated in it and it cannot be transferred to another medium. To repeat it is not possible to transfer the content or the message of the medium to another medium. There is an isomorphism between the medium and its content. The medium is the content and hence also the message. The medium is both the message and the content for a biotic system because information in a biological system is not symbolic but rather chemical. It is for this reason that the notion of transferring the contents of the human brain to a computer is pure nonsense.

To conclude we have argued that information is not an invariant independent of the frame of reference in which it operates. In the biotic frame of reference information is always associated with meaning, which is not necessarily the case with Shannon or Kolmogorov information. In the biotic frame information cannot be separated from the medium of its instantiation as is the case in the Shannon and Kolmogorov reference frames. In other words the information in DNA, RNA and proteins are embodied. They differ from human symbolic information, which can be disembodied and moved from one medium to another. Each generation makes a god of their latest technological or scientific achievement or breakthrough. For the Hebrews it was the written word and the law "written with the finger of God". For the Greeks it was their deductive logic and rational thought disembodied from practical experience and empirical evidence of the physical world. For the Enlightenment it was Newtonian mechanics and God, the clock maker, where things were explained in terms of mechanical models. In the Information Age the god is disembodied information, information without context where everything is explained in terms of the transfer of information, and sometimes it is information without meaning.

12 生物系统中信息的物质性

信息就是信息，不是物质或能量。任何不承认这一点的唯物主义在当今都无法存活。

– Norbert Wiener [2]

香农的理论将信息定义为一种概率函数，它没有维度、没有物质性，也不一定与意义相关。信息是一种模式，而不是一种存在 [6]。

香农的信息理论不能也不应简单地应用于完整的生命体，因为在像 DNA 这样的生物系统中，信息不仅仅是模式 —— 它也是一种存在。对于食物或毒素的受体来说，它们不仅仅是模式，还是一种存在。生物系统既包含信息模式，也具有物质特性，更准确地说，是信息模式在物质中的体现。早在 DNA 被发现之前，Schrödinger [13] 就以隐喻的方式描述了染色体物质的这种双重特性。他说：「染色体结构在实现它们所预示的发展方面起到了工具性的作用。它们既是法律代码也是执行的权力 —— 或者换个比喻，它们既是建筑师的计划也是建筑工匠的技艺。」正是信息模式与生物智能体物质组成之间的动态相互作用决定了它们的行为。

如前所述，问题的关键在于我们是否可以完全将生物智能体视为物理计算系统。显然，生物系统不能仅用 Shannon 信息来描述，因为 Shannon 信息是将信息从其物质形式中抽象出来，并且不依赖于具体介质。同样的道理也适用于 Kolmogorov 复杂性。Kolmogorov 复杂性是基于图灵机定义的，这也是一种将信息模式与其物质形式分离的方式。生物学研究的核心是物质，而不仅仅是数学模式。正如 Kubie 在 Macy 会议上警告的那样，「我们总是有简化问题的危险，以便将其缩小为数学处理 [6]」。如上所述，物理环境会改变嵌入基因组 DNA 中的信息的意义。

另一种区分生物信息或指令性信息与 Shannon 信息或 Kolmogorov 复杂性的方法是，后者是符号性的，而生物信息或指令性信息则不是。生物体中的信息是通过生物分子的化学相互作用来发挥作用的。「DNA 是一种通过复杂机制与其他分子相互作用的分子。将 DNA 仅仅看作是需要解释的文本是不准确的简化 [25]」。不是 DNA 的符号性质产生了信使 RNA，也不是 RNA 的符号性质产生了蛋白质，而是 DNA 的化学性质催化了 RNA 的生成，RNA 的化学性质催化了蛋白质的生成，蛋白质的化学性质则执行其各种功能，例如：

1. 作为酶催化对新陈代谢至关重要的生化反应；

2. 提供结构或机械功能，例如构建细胞的细胞骨架；

3. 在细胞信号传导、免疫反应、细胞粘附和细胞周期中发挥作用。

DNA、RNA 和蛋白质既是载体也是内容，既是信息也是信使。而在 Shannon 和 Kolmogorov 信息论中，可以区分载体和信息、信息和信使。信息是独立于其载体存在的，McLuhan [24] 也不例外。而对于生物信息而言，载体和信息是不可分割的 —— 它们是同一个东西。对生物信息来说，载体即是信息（在 McLuhan 的意义上）也即是内容。而在人类符号信息中，根据 Shannon 信息论，信息或内容和载体是分开的。对于生物系统，不仅在 McLuhan 的意义上载体就是信息，即载体对其内容产生独立影响，而且载体也是内容，因为是载体的化学性质影响了生物体。实际上，载体即是信息，因为它就是内容，信息的内容是该载体独有的，并且在该载体中被实例化，不能转移到其他载体中。换句话说，不可能将载体的内容或信息转移到其他载体中。载体与其内容之间存在同构关系。载体就是内容，因此也是信息。对于生物系统来说，载体既是信息也是内容，因为生物系统中的信息不是符号性的，而是化学性质的。这就是将人类大脑的内容转移到计算机上的想法纯属无稽之谈的原因。

总结来说，我们认为信息并不是独立于其参考系而存在的。在生物参考系中，信息总是与意义相关联，而这在 Shannon 或 Kolmogorov 信息中并非必然。在生物参考系中，信息无法像在 Shannon 和 Kolmogorov 参考系中那样与其载体分开。换句话说，DNA、RNA 和蛋白质中的信息是具象的。它们不同于人类的符号信息，后者可以脱离载体并在不同介质之间转移。每一代人都会将其最新的技术或科学成就奉为神明。对于希伯来人来说，这些是「上帝之手」写成的文字和法律。对于希腊人来说，这是他们抽象的演绎逻辑和理性思维，与实际经验和物理证据无关。对于启蒙运动时期的人来说，这是牛顿力学和钟表匠上帝的机械模型。在信息时代，神明变成了脱离语境的信息，一切都被解释为信息的传递，有时这些信息甚至没有实际意义。

### 13. Organization as Information

What is the relationship of organization and information? What we discovered in POE was that the autocatalysis of biomolecules led to the organization of a biological living organism whose organization of constraints allowed it to convert free energy into work that sustained growth and permitted replication. We identified the constraints as instructional or biotic information, which loops back into the organization of the organism. This model of information holds for biotic systems where autocatalysis is the organization and the components are the individual biomolecules.

The argument seems circular only because a living organism represents a self-organizing system. This is still another way that biotic information differs from Shannon information which is defined independent of meaning or organization. In fact organized information has less Shannon information because it does not reduce as much uncertainty as disorganized information. It is also the case as we mention above that this model provides a mechanism for the creation of information which in not the case with the Shannon model of information.

I believe that Hayles [6] has come to a similar conclusion regarding the relationship of information and organization when she wrote about the paradigm of autopoiesis or self-organization:

Information does not exist in this paradigm or that it has sunk so deeply into the system as to become indistinguishable from the organizational properties defining the system as such.

It is the latter half of her statement that is congruent with our notion that the set of constraints or organization that give rise to an autonomous self-organizing system is a form of information.

Wiener like Shannon related information to entropy but, unlike Shannon, Wiener [3] saw a connection between organization and information, "The notion of the amount of information attaches itself very naturally to a classical notion in statistical mechanics: that of entropy. Just as the amount of information in a system is a measure of its degree of organization, so the entropy of a system is a measure of its degree of disorganization".

We make a similar claim in POE [1] when we assert that the constraints that allow the propagation of organization in a living organism represents the information content of that organism. In other words the propagating organization of a living organism is its information content. Our position in a certain sense recapitulates similar sentiments expressed by Norbert Wiener [3] when he wrote "We are not stuff that abides but patterns that perpetuate themselves".

However where I differ from Weiner is that while we are patterns that abide I also believe that we are patterns that are uniquely instantiated in flesh. I therefore believe that human intelligence cannot be transferred from a human brain onto a silicon-based computer as is claimed by some advocates of strong AI. The point that I would make is that the pattern cannot be separated from the medium in which it is instantiated as was argued above. The medium of flesh and its organization are what is critical. It is the pattern instantiated in the flesh and not just the pattern by itself that makes life. The information in a biological system is not symbolic but rather chemical. As we have already asserted the medium of the flesh is both the message and the content of a biotic system.

13 组织即信息

组织和信息之间是什么关系？在 POE 中，我们发现生物分子的自催化作用会形成一个生物体的组织，这种组织通过特定的约束使其能够将自由能转化为维持生长和复制的能量。我们将这些约束称为指令性或生物性信息，这些信息又反过来影响生物体的组织。这种信息模型适用于那些自催化是组织方式，而成分是个别生物分子的生物系统。

这个观点看起来是循环的，因为活的有机体是一个自组织系统。这是生物信息与香农信息不同的另一个方面，香农信息是独立于意义或组织定义的。实际上，有组织的信息比无组织的信息包含的香农信息更少，因为它减少的不确定性较少。正如我们之前提到的，这个模型提供了一种创建信息的机制，而香农信息模型并没有这一点。

我相信 Hayles [6] 在讨论自创生或自组织的范式时，已经得出了关于信息与组织关系的类似结论：

在这个范式中，信息不存在，或者它已经深深融入系统，以至于与定义系统的组织属性无法区分。

她声明的后半部分与我们的观点一致，即产生自主自组织系统的约束或组织形式本身就是信息的一种形式。

Wiener 像香农一样，将信息与熵联系起来，但与香农不同的是，Wiener [3] 看到了组织与信息之间的联系，「信息量的概念非常自然地与统计力学中的经典概念 —— 熵相联系。正如系统中的信息量是其组织程度的衡量标准，系统的熵是其无序程度的衡量标准」。

我们在 POE [1] 中也提出了类似的观点，认为生物体中组织传播的约束条件代表了该生物体的信息内容。换句话说，一个生物体的组织传播过程，就是它的信息内容。从某种角度来看，我们的立场与 Norbert Wiener [3] 的观点相似，他说过 "我们不是永恒存在的物质，而是自我延续的模式"。

然而，我与 Wiener 的不同之处在于，虽然我们确实是持续存在的模式，但我认为我们是以肉体为载体的独特模式。因此，我认为人类智能不能像某些强 AI 的支持者所说的那样，从人类大脑转移到基于硅的计算机上。我的观点是，模式无法脱离其载体存在，正如前文所述，肉体及其组织方式才是关键。正是肉体中的模式，而不仅仅是模式本身，使得生命存在。生物系统中的信息不是符号化的，而是化学性的。正如我们已经说过的那样，肉体既是信息的载体，也是生物系统的内容。

### 14. Who Are We? What Are We, Information or Flesh?

Information in the form of words or language is symbolic. The word cat is a symbol that represents a class of living breathing creatures made of flesh. An actual cat is not a symbol of something else but an organization of organic chemicals that can propagate its organization through its metabolism and its ability to replicate.

The organic chemicals of which we are composed are continually replaced so that after seven years there is a completely new set of molecules. So we are not flesh or a particular set of molecules but the organization of the molecules of which we are composed or more accurately we are a process and not a thing that can be duplicated.

One cannot make a replica of a person. Even twins that originated from the same fertilized egg are never exactly the same. But a text can be replicated or duplicated exactly. A text can also be transmitted and reformatted from one medium to another, for example from a computer file to a text printed on paper or from a live performance to a podcast.

I believe that the proponents of strong artificial intelligence (AI) and strong artificial life (AL) make the mistake of considering intelligence or life as merely reified information. They do not take into account that it is the interaction or organization of flesh-based matter that makes intelligence and life. The pattern of that interaction or organization that we identify as information cannot be abstracted away from the physical medium in which it is instantiated and remain unchanged or, even more importantly, continue as the process that gave rise to that intelligence or life in the first place.

A feature of both intelligence and life is that it is autonomous. A living organism is an autonomous agent that has the capacity to exploit free energy from its environment and use that energy in the form of work to carry out its metabolism, to replicate and to make use of its intelligence. The proponents of strong AI and AL overlook this important factor when they claim that intelligence and life is nothing more than information or a pattern that is independent of its physical instantiation. At best artificial life forms may be regarded as obligate symbionts with humans but not as independent living organisms as they are not autonomous.



### 15. Human Language, Culture, Technology, Science, Economics and Governance as Forms of Propagating Organization

"I take informatics to mean the technologies of information as well as the biological, social, linguistic and cultural changes that initiate, accompany, and complicate their development [6]".

Katherine Hayles' quote indicates that there is a link between biological, cultural and linguistic information. It was also noted in POE that language and culture like living organisms also propagate their organization and hence their information. This also includes science, technology, economics and governance which are part of culture and will be treated separately because they provide vivid examples of propagating organization. The information that language and culture represent like biotic information is not Shannon or selective information but rather information with meaning, namely MacKay structural information.

Cultural and linguistic information is not fixed but depends on the context–as conditions change so do languages and cultures. This statement applies to the various sub-division of culture that we have explicitly identified, namely, science, technology, economics and governance. These forms of information do not represent Shannon selective information but rather MacKay structural information because of their dependence on context. Each one is more than a string of alphanumeric symbols or a string of 0 s and 1 s.

Let me provide an example of how linguistic meaning depends on context based on my experience of being the father of four children who in turn have provided me so far with four grandchildren. The meaning of the term Dad has changed for me over my lifetime. Dad used to be my father and then when I had children it meant me and then when my children had children and I became grandpa and Dad became the father of my grandchildren.

The point is that the meaning of words are context dependent. This is why I [21] identified words as strange attractors. They are strange attractors because the meaning of a word is never exactly the same as its meaning changes ever so slightly each time it is used because the context in which it is used is never the same. To illustrate the idea let us consider the word water which represents the water we drink, wash with, cook with, swim in, and that falls as rain, melts from snow, constitutes rivers, lakes, ponds and oceans, etc. The meaning of water in each of these contexts is slightly different but there is a common thread and hence the claim that the word "water" acts as a strange attractor for a diverse set of contexts involving water.

A language is an organization of a set of symbols whose semantics and syntax is a form of information. A similar claim can be made for a culture which Geertz [26] defined as "an historically transmitted pattern of meanings embodied in symbols, a system of inherited conceptions expressed in symbolic forms by means of which men communicate, perpetuate and develop their knowledge about and attitudes towards life." He goes on to add, that "culture is patterns for behavior not patterns of behavior".

Information as a form of organization for either language or culture, although it is symbolic like Shannon information, still cannot be associated with Shannon information because linguistic and cultural information is context dependent and meaningful. It is also the case that language and culture are like living organisms in that they evolve in ways that cannot be predicted. We may therefore use the same core argument we did in POE to rule out the description of language and culture and their evolution with Shannon information. "The ensemble of possibilities and their entropy [for language and/or culture] cannot be calculated [1]". Therefore a definition of information as reducing uncertainty does not make sense since no matter how much one learns from the information in a linguistic or cultural system, as was the case with a biotic system, the uncertainty remains infinite because the number of possibilities of what can evolve is infinitely non-denumerable. Because science, technology, economics and governance are part of culture and it is also true that their evolution cannot be predicted; the argument we just made for language and culture applies to these subsets of culture as well.

At this point it is perhaps useful to define two forms of information micro-information consisting of isolated bits of information, the kind that are transmitted as Shannon information and are also components of a larger information system or organization and macro-information or the organization of a system like a living organism, a language, or a culture. Other forms of macro-information include the specific elements of a culture such as a business, an economic system, a polity, science and the technosphere. Narrative is the organization of a text or an utterance and therefore may be regarded also as a form of macro-information. Micro information is the string of characters and symbols that make up the narrative of a book, an article or a story.

There is still another property that the organizational information of language and culture share with living organisms that distinguishes them from Shannon information. This is the fact that language and culture, like life, are self-organizing phenomena and hence as is the case for biotic information and not the case for Shannon information we have a primitive model for the emergence of this information. Although we do not have a precise theory for how language and culture and the information and organization associated with them emerged we do have a number of proposals and models for how this might have happened through self-organization. Logan [21] contains a review of these models.

The notion of organization as a form of information is based on the notion that the systems we have reviewed consist of components that are organized by some organizing principle. For living systems the components are the biomolecules of which living organisms are composed and the constraints or instructional information that allows the conversion of free energy into work is the organizing principle of these biomolecules, which is propagated as the organism replicates.

This model holds for languages where grammar is the organizing principle and the components are the individual words or semantics. Replication takes place as children learn the language of their parents or care givers.

The model also holds for social systems where the culture as patterns for behavior is the organizing principle and the components are the behaviors and judgments of the individual's of the society. Replication occurs as young people learn the intricacies of their culture from a variety of sources including parents, teachers and peers.

For technology the technosphere is the organization and the components are the individual inventions or artifacts. Replication takes place each time an inventor or innovator makes use of components of the technosphere to create a new artifact or invention.

The model holds for economic-governance systems where the economic model is the organization and the components are the individual business transactions. Examples of different economic models based on the work of Johnson and Earle [27] are:

- Individual families as basic economic unit;

- The big man tribal economic unit where the big man is the coordinator of economic activity and serves at the pleasure of the people;

- The chief dominated tribal economic unit where the chief controls all the means of economic activity but answers to a tribal council;

- The state or manor economy where the monarch or the lord of the manor is the absolute ruler; as was case with Medieval manor system, Czarist Russia and France before the revolution;

- The market driven system which is democratic as in a republic like the USA or constitutional monarchy like the UK;

- The socialist state where private enterprise is controlled; and

- The communist state, which is state capitalism as was case with Soviet Union and Maoist China.

China is now evolving into a mixed communist-socialist state.

The replication of economic-governance systems is through cultural and legal systems.

The model holds for science where the scientific method is the organizing principle and the components are the individual scientific theories. Replication occurs through the publication of scientific results and the education of new scientists.

16. Conclusions

We have demonstrated the relativity of information by showing that information is not a unitary concept independent of the phenomena it is describing or the frame of reference with respect to which it is defined. In particular we have shown that Shannon information cannot properly describe living organisms, language, culture and the various components of culture such as technology, science, economics and governance. We have examined the relationship of information to materiality, meaning and organization and showed that Shannon information is independent of meaning, organization and its material instantiation, which is just the opposite for biotic information, and the information associated with language and culture. We have also shown that that there exists an intimate relationship between information and organization for biotic systems and the elements of human culture including language, technology, science, economics and governance.

Acknowledgement

This paper draws heavily on two sources other than my earlier work, namely the paper Propagating Organization: An Enquiry [1] and the book How We Became Posthuman [6]. In a certain sense this paper is a remix of these two sources with help from the references cited below.

### References

1. Kauffman, S.; Logan, R.K.; Este, R.; Goebel, R.; Hobill, D.; Shmulevich, I. Propagating organization: An enquiry. Biol. Philos. 2007, 23, 27–45.

2. Shannon, C.E. A mathematical theory of communication. Bell Syst. Techn. J. 1948, 27, 379–423, 623–656.

3. Weiner, N. The Human Use of Human Beings; Houghton Mifflin: Boston, MA, USA, 1950.

4. MacKay, D. Information, Mechanism and Meaning; MIT Press: Cambridge, MA, USA, 1969.

5. Bateson, G. Steps to an Ecology of Mind; Paladin: Frogmore, St. Albans, UK, 1973.

6. Hayles, K. How We Became Posthuman; University of Chicago Press: Chicago, IL, USA, 1999.

7. Fisher, R.A. Theory of statistical estimation. Proc. Camb. Philos. Soc. 1925, 22, 700–725.

8. Hartley, R.V.L. Transmission of Information. Bell Syst. Techn. J. 1928, VII, 535–563.

9. Campbell, J. Grammatical Man: Information, Entropy, Language, and Life; Simon and Schuster: New York, NY, USA, 1982; p. 32.

10. Wicken, J.S. Entropy and information: Suggestion for a common language. Philos. Sci. 1987, 54, 176–193.

11. Schneider, E.D.; Sagan, D. Into the Cool: Energy, Flow, Thermodynamics and Life; University of Chicago Press: Chicago, IL, USA, 2005.

12. Lewis, G.N. The symmetry of time in physics. Science 1930, 71, 569–576.

13. Schrödinger, E. What is Life? Cambridge University Press: Cambridge, UK, 1992.

14. Brillouin, L. Science and Information Theory; Dover: Mineola, NY, USA, 2004.

15. Kauffman, S. Investigations; Oxford University Press: Oxford, UK, 2000.

16. von Bertalanffy, L. General Systems Theory: Foundations, Development, Applications; George Braziller: New York, NY, USA, 1968.

17. Losee, R.M. A discipline independent definition of information. J. Am. Soc. Inf. Sci. 1997, 48, 254–269.

18. Shiryayev, A.N. Selected Works of A.N. Kolmogorov: Volume III: Information Theory and the Theory of Algorithms (Mathematics and its Applications); Kluwer Academic Publishing: New York, NY, USA, 1993.

19. Burgin, M. Evaluation of scientific activity in the dynamic theory of information. Sci. Sci. Sci. 1994, 1, 124–131.

20. Burgin, M. Theory of Information: Fundamentality, Diversity and Unification; World Scientific: Singapore, 2010.

21. Logan, R.K. The Extended Mind: The Origin of Language and Culture; University of Toronto Press: Toronto, ON, Canada, 2007.

22. Langefors, B. System för Företagsstyrning; Studentlitteratur: Lund, Sweden, 1968.

23. Logan, R.K.; Louis, S. Collaborate to Compete; Wiley: Toronto, ON, Canada, 2004.

24. McLuhan, M. Understanding Media: Extensions of Man; MacGraw Hill: New York, NY, USA, 1964.

25. Sarkar, S. Decoding "coding"—information and DNA. Bioscience 1996, 46, 857–864.

26. Geertz, C. The Interpretation of Culture; Basic: New York, NY, USA, 1973.

27. Johnson, A.W.; Earle, T. The Evolution of Human Societies: From Foraging Group to Agrarian State; Stanford University Press: Stanford, CA, USA, 1987.