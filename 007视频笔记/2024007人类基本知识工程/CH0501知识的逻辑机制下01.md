## CH0501知识的逻辑机制下01

各位同学，现在我们正式开始今天的课程。今天的内容较多，吸取上一次讲解演绎逻辑时录制的视频中速度较快的经验，今天我们将会稍微讲得慢一点。有些知识点可能会快速过。今天上午是从 9 点半讲到 11 点半左右，留出一些时间给大家交流和提问；晚上则是从 7 点讲到 9 点左右，同样会留出时间供大家交流和提问。

现在我们正式开始今天的课程。前面的内容已经进展到了第五部分的知识逻辑机制，在此之前，我们将对第四部分的知识逻辑机制上做一个简单的回顾。为什么我们在学习了多年的逻辑后，还是记不住用不上，日常生活中实际使用到的逻辑规则并不需要那么多呢？这主要是从人类作为求知者的角度出发，围绕第三种知识的常见形态进行探讨，将不同的处理机制最终归结为几个重要的方面，即一些基本的逻辑规则。

我们的分类方法如下：第一类是从逻辑规则的角度来看，即使是一个盲人也能感知到的一些逻辑规则。这类规则与语言表达无关，我们称之为知觉类逻辑规则，它是相对底层的一种。例如，列举对比模拟以及在此基础上的组合和递归等。

接着是第二类，我们称之为语言符号类的逻辑规则。在日常生活中，大多数情况下进行的是符号处理，即任何一个语言系统本身都是由符号组成的，我们需要定义所使用的语言中每个字母或符号代表的意义。

接着是语法问题，即主语在前还是谓语在前。接下来是语义问题，A 指向的是什么？推导过程是从 A 到 B 再到 C 的逻辑关系是如何建立的。那么从这个角度来看，我们总结了逻辑这一基础性的重要内容，称之为「逻辑九则」。这一点大家已经比较清楚了。

今天我们要总结的是最重要的逻辑九则。接着，我们将开始沿着四大逻辑系统进行：演绎、归结、因果和高阶分别介绍了这些内容。这里重点介绍演绎逻辑，它作为逻辑的起点，被总结为四个子逻辑系统：词相逻辑、命题逻辑、关系逻辑、模态逻辑。

其中最经典的是三段论，属于词相逻辑；而处理符合命题则需要命题逻辑。接着是关系逻辑，它处理的是关系命题；最后比较重要的是模态逻辑，涉及可能世界的演绎逻辑，涵盖了所有可能事件的演绎过程，作为整个逻辑系统的起点。在今天的讲述过程中，大家会发现演绎逻辑中的命题逻辑和模态逻辑将反复出现。

今天我们要开始介绍的内容包括归纳逻辑、因果逻辑以及高阶逻辑。我们先从归纳逻辑讲起。归纳逻辑又称为经验概括，在日常生活中无论是小孩、青少年还是成年人都非常依赖习惯，而习惯本身就是一种典型的归纳逻辑。接下来我们将对归纳逻辑进行抽象的图示化，例如：观察一，铁加热后会膨胀；观察二，铜加热之后也会膨；观察三，铝加热之后也会膨；观察 n，其他金属加热之后也会膨。

首先，我们观察到三种具体的金属在加热之后都会膨胀；进一步地，其他已知的金属亲属在加热后也表现出相同的特性。基于这些观察，我们可以进行一次归纳推理，并概括出一个普遍结论：所有已知的主要金属在加热后均会出现膨胀现象。这是基于当前世界上发现的大多数金属所得出的结论。但是我们也应该意识到，世界上可能还有一些新近发现的或未被完全研究的金属，它们是否同样遵循这一规律尚未可知。因此，在归纳推理中还包含了一个预测部分：即第 N+1 种金属在加热后也会膨胀。

这个图示非常重要，因为它清晰地展示了整个归纳过程可以分为两个阶段：第一是归纳概括，第二是归纳预测。我们从具体的观察入手，逐步形成系统的归纳逻辑。这一贡献主要归功于弗朗西斯·培根，他在 1620 年出版了《新工具论》一书，在书中提出了系统化的归纳逻辑理论，即著名的「培根三表法」。

培根的三表法简单来说分为三个步骤：第一，确定具备某一特性的对象（存在与具有表）；第二，排除不具有该特性的对象（差异表）；第三，分析这些特性在不同对象间的表现程度（程度表）。以加热现象为例，在《新工具论》中，培根列举了太阳光、火以及受压水和空气等 28 种不同的例子，发现它们共同的特点在于内部存在运动或摩擦。例如，光线的传播伴随着其内部的摩擦运动，而受压缩的空气则因为内部粒子的碰撞而产生热量。

接着，培根又观察到月光、灵光以及未受压水和空气等不具备热的现象，这些例子中没有发现内部摩擦与运动的存在。进一步地，他还注意到一些生物如鱼、野兽、神鸟等体内的温差变化，这种温度的变化是逐步进行的。例如，在对比不同星球的热能时，培根指出太阳显然比月亮更热；而在地球上的生物中，鸟类的体温波动范围通常大于鱼类，而蛇类的体温变化则又小于鸟类。

通过这些观察与分析，培根为归纳逻辑提供了一套系统的方法论，对后世科学研究方法的发展产生了深远的影响。

培根通过大量的观察，最终归纳出一个结论：热的本质是物体内部微小粒子的运动。大家注意一下，培根在这里运用的是最原始的归纳法。体现了一种精神，在 17 世纪这个年代，人们开始对世界上各种各样的现象进行观察，尝试总结出一类事物的本质。因此，这种追求在 17 世纪形成了所谓的本质主义。

前面我们讲到不同的逻辑系统及其历史背景时，已经提到数学和物理学等领域，尤其是以数学的公理系统为例，在古希腊时期已初步形成。到了 16、17 世纪，从这些公理系统中发现的世界规律被用来改造世界的已经差不多了。这个过程中，有一批智者，特别是培根为代表的人物，他们对这种现象不再满足。数学部分在很大程度上与神学捆绑在一起，最终形成了经验哲学。这一时期，人们更加重视对自然界的观察，并总结各种本质。因此，在 16、17 世纪诞生了无数类似的著作，比如探讨热的本质等课题。

这类思潮在科学史上被称为本质主义。随着逻辑学的进步，现代的归纳逻辑系统逐步形成，我们可以将其总结为三个子系统：枚举归纳、类比归纳和因果归纳。其中最特殊的是因果归纳，它既属于归纳逻辑部分，也属于因果逻辑部分。为什么需要有因果归纳以及因果逻辑系统，我们将在后面解释。

接下来，我们将详细探讨这三个逻辑系统的处理对象的区别。例如，枚举归纳通常处理的是比较具体的问题，像培根的这个例子就是一例。一个很典型的就是枚举归纳。什么是枚举？就是举例，这是比较直接的方法，但它容易导致过度概括。第二种是类比归纳，处理的对象具有相似性，启发性强，但可能容易误导人。第三种是因果归纳法（即穆勒方法），它基于世界的关系和数据，强调的是因果，复杂性比较高。

我们先来看一下枚举归纳。什么是枚举归纳？它以列举为核心，是所有归纳法的起点之一。今天讨论的内容在中小学时学习的演绎与归纳中一般指的是这个方法 —— 即通过列举一系列具体的实例或案例，最终从中得出一个普遍性的结论。这就是我们常说的从具体到一般的归纳过程，也就是枚举归纳。

它的数学表达可以用一个简单的公式来表示。我们可以举一个小孩子的例子来看：比如观察到所有的小狗都会叫，从而得出「所有的狗都会叫」这样的结论。这样就是通过具体的实例推导出普遍性的规律。

枚举归纳与我们的认知系统和信息加工机制有密切的关系，即列举，覆盖全么（这里指的是列举的范围和代表性）。从这个角度出发，我们将枚举归纳一般分为两类：一类是完全枚举归纳，即你的结论涉及到了所有可能的实例都已经被观察到。例如，你下了一个结论认为某个班上的同学全部是男生，这意味着该班上所有的人都已经由你统计并观察过。这种情况下就是所谓的完全枚举归纳。

另一种情况则是不完全枚举归纳，即只观察了一部分实例就得出结论，这具有一定的不确定性。比如，这个班上有 100 人，但有 20 人没有到现场，而这 20 人的性别你不认识也不确定是男生还是女生。像这种情况下就是所谓的不完全枚举归纳。在我们的日常生活中，绝大多数的归纳推理都是不完全的。因此，这导致了无数的认知偏差。

接下来，我们来看一下枚举归纳的一个逻辑规则总结：S1 是 P，S2 是 P……Sn 也是 P，所以所有的 S 都是 P。这一过程会导致许多复杂的问题，我们在后面会详细展开讨论。

通过这次介绍，希望大家能够了解枚举归纳法的优点和缺点。首先，它比较容易理解；然而它的缺点在于其不确定性、易受到偏差影响以及容易出现一些典型的反例。因此，在枚举归纳的基础上发展出了类比归纳法。与前者相比，类比归纳的推理机制有所不同。

在中小学阶段学习逻辑学时，我们通常只接触到枚举归纳这一层面。实际上，基于相似性的类比归纳在日常生活中更为广泛地使用。这是它的数学表达式：\[示例公式\] 下面我给大家举一个具体的例子：设 A 和 B 都是鸟，它们都有羽毛、都能下蛋。但我们知道，A 不能飞。通过这个前提 P（即 A 的特征）得出结论 Q（即 A 不能飞）。接着我们进行类比归纳推理，因为 A 和 B 在羽毛、鸟类特性及下蛋等方面相似，所以推测如果 A 不能飞，则 B 也不能飞。这是一个基于相似性的典型类比归纳例子。

侯世达对类比归纳极为推崇，他认为人类的思维基本上是通过类比的方式来进行的。然而，阳老师对此持有不同的观点。随着讨论的深入，我们将明白侯世达的问题所在 —— 他在这一推理模式中犯了一些相对复杂的错误。从这个角度看，虽然类比归纳在逻辑上强调比较差异，但关键在于如何区分简单与复杂的类比。

所谓简单的类比，是指我们在第三讲和第四讲中多次提到的一种新的知识论。这种知识论认为，底层受到人类大脑信息架构能力的限制。而人类的信息架构能力依然是受到七加减二的制约，也就是说我们在处理相似性的时候通常是在四至九个要素之内。所以，在这里侯士达有一个错误，他没有全面考察简单类比与复杂类比的事物，而是更多地将类比的流动替代了这两者之间的区别。这引出了一个非常有趣的话题。比如我们之前给大家画了一张图，展示了 AB 的特征。在什么情况下我们会产生所谓的「类比归纳」呢？请大家注意 P1、P2 和 P3。

我们的大脑接触某个事物时，假设只是初次接触，并且这次接触到的特征值非常多，比如说达到了 30-40 个这样的特征值。在这种情况下，产生类比归纳的难度其实比大家想象的要高得多，特别不容易进行有效的类比归纳。然而，在日常生活中，我们为什么能够如此频繁地进行类比归纳呢？这里涉及到另一句话：假设我们遇到一个非常复杂的失误，它包含 30 个特征值。

我们会逐级快速处理这些信息。具体来说，当面对一个复杂的事物时，比如由 100 个特征值构成的（为了便于说明，假设是 100 个），我们会迅速将其分为两类，例如 80 个和 20 个。这 80 个表示较强或显著的部分，而那 20 个则表示较少或较弱的部分。

当我们对比这些相似性时，如果它们之间存在差异，最初可能是 80 个部分相同没有差异，那么我们就会放弃这个对比。我们的大脑会迅速转向那些具有明显差异的 20 个部分进行比较。

接下来我们继续细分 20。我们将 20 细分为几个部分：例如，这一个是分针，这是 16，这边的分针是 4。这时我们发现，在 16 个特征上，AB 两个项目是相同的。此时，我们放弃进一步对比，转而关注四个特征，大家听懂了吗？这里面涉及到一个非常复杂的认知科学原理，这是我未来将要写的一本书的主题《认知原理》。请注意，当我们的大脑进行这种快速对比时，有第一层和第二层的处理。然而，我们的大脑并不是无限层级的机器，并不会无限制地增加层级到 N 层。大家听懂了吗？许多认知科学家以及经济学家都犯了一个严重的错误，实际上我们只能进行有限次的比较：一次、两次、三次、四次、五次左右，这是我们所有能力的极限。大家明白了吗？即使是最复杂的事物，比如我们在日常生活中遇到的一些复杂的决策 —— 例如现在的男朋友或者女朋友问题，是否要买房，你的大脑实际上会采用所谓的「适当对比原则」。这时，你往下对比到第五层就会停止了。这个原则会产生什么样的现象呢？就像再复杂的系统，在进行类比归纳时，我们基于相似性的假设。例如，你现在决定买房子，明明是在经历一个严格的计算过程，但当你去公司吃饭，和同事一起吃午饭时，同事说他最近买了一个不错的小区，你会发现你一下子跳过了这些对比。大家发现了吗？因为这时出现了非常强烈的一个相似性，我们的大脑就放弃了这个严格计算的对比过程，这就是所谓的类比归纳。

在日常生活中，这给我们带来了大量的走捷径的情况。所以，只要是在任何一次对比时，如果你身边恰好有一个极其鲜明的例子，那么这个例子需要具备什么样的特征呢？各位同学可以思考一下，显然一个很理性的人给你讲述时，这个购房决定是如何做出的呢？他说得对。虽然这样的感染很少见，但你会发现有两个同事给你这样讲：一个同事说他现在买房已经节约了一百个计算指标，并进行了五轮计算，最终选择了这套房子。你觉得这位同事能成功说服你吗？另一位同事则认为这房子交通方便，值得购买；再有一位同事称赞这房的环境好，劝你也买下来；还有的同事告诉你这是学区房，建议你买下它。大家发现了吗？这就是我们人类在进行类比归纳时一个突出的规律：当你在做复杂的类比归纳时，身边接触到简单的类比归纳，大多数情况下这些简单类比归纳中的实体数量不会超过三个，最多谈 ABC 三点，即 AB 比较、BC 比较和 AC 比较就差不多了。这样的简单类比一旦完成，会对你正在进行的复杂类比产生严重影响。

这种类比在我们日常生活中其实非常普遍且重要，但往往被忽略。这是一种图示性的归纳，其优点在于人类的大脑直觉是如何产生的 —— 因为它具备很强的灵活性和启发性，能够帮助我们找到捷径，就像跨越了五轮计算一样，简单而鲜明的例子更容易吸引成功。这种确定性依赖于相似性的度量，但很多时候我们的相似性度量往往是错误的。例如，你可能认为 80 和 20 这两个数字中，80 是相同的，20 是不同的；但实际上，在现实世界中可能是 20 相同，80 不同。因此，在进行类比归纳时，我们容易出现一些问题。

第三种归纳方法被称为因果归纳。它比前面介绍的规则稍微复杂一些。在这个方法中，通常会分为直接因果规律和间接因果规律。我们可以看一下它的逻辑规则：前面提到的方法是从一般到具体的，而这一种则是从具体到一般的归纳。这里所指的不是简单的观察一个事件，而是要同时观察两个事件，并且在时间上有先后顺序 —— 先发生的是前一个事件，后发生的是另一个事件。在这种情况下，人类大脑会开始形成一种归纳，认为按一下开关会导致灯亮。接下来，我们进一步归纳出结论：当第 N+1 次按下开关时，灯依然会亮。这就是因果归纳的一个非常典型的过程。

因果归纳最重要的贡献者是约翰·穆勒，他在 1843 年出版了《逻辑体系》一书，这是该书的早期版本。现在这本书通常被翻译为《密尔逻辑》，也被称为密尔法则。在 1863 年，大约培根提出归纳法后的 200 年，密尔对培根的方法进行了扩展。培根最初的三表法更多地涉及到我们今天所说的枚举归纳，而不太涉及因果归纳的部分。此时，穆勒对其进行了扩充。

他在书中写道：「归纳法的主要目标是确定每一个原因的结果。」那么，在培根的三表法基础上，他将其扩展为五种方法：一致法、差异法、共变法、共用法和剩余法。这些方法对于具备大学文化基础的同学来说，基本都是容易理解的。例如：

一致法

所有中毒的顾客都吃了沙拉，而其他菜品各不相同。

结论：沙拉可能是导致中毒的原因。

差异法

有一位顾客没吃沙拉且没有中毒，而其他顾客吃了沙拉并中毒。

结论：沙拉是导致中毒的可能原因。

共变法

中毒的严重程度与顾客吃的沙拉量成正相关。

结论：沙拉的摄入量与中毒程度相关，进一步支持沙拉是原因。

共用法

所有中毒的顾客都吃了沙拉，没吃沙拉的顾客没有中毒。

结论：沙拉很可能是中毒的原因。

剩余法

假设其他食物（如牛排、冰淇淋）不会导致中毒，剩下的轻微中毒可能由沙拉之外的其他食物引起。

结论：沙拉是主要嫌疑，但其他食物可能也有影响。

穆勒的这套古典归纳逻辑基本上涵盖了归纳推理能覆盖的所有原则。在这个基础之上，20 世纪我们开始发展出更多的归纳统计技术。这些统计技术是我们今天讨论归纳逻辑的起点，其中最重要的是干预和假设检验。我们今天绝大多数科学家所使用的科学知识和科学诚信，其实是建立在 20 世纪这两个核心归纳方法的基础之上。

然而，到了 21 世纪，我们 20 年前在大学学习的这套思维方法已经开始过时了。尤其是在统计学、经济学以及人工智能领域，这些方法变得非常落后。这就是为什么我们需要引入第三套逻辑系统的原因。

在大学高等教育阶段学到的知识往往已经过时了。这些过时的知识成为我们讨论的起点。现在，让我们来了解一下什么是概率和假设检验。

前面我们可以发现，穆勒的说法实际上还是一种定性的文字描述。我们需要将这种定性描述转换为定量描述，这就引出了概率的概念。

最早提出概率相关表述的是 17 世纪的数学家伯努利。他的研究成果综合了当时的知识。伯努利认为，概率可以用 0 到 1 之间的数值表示，1 代表最大可能性，0 代表最小可能性。这是一种相对客观的概率表示方法。

什么是客观概率？客观概率是指我们观察 100 件事情，然后发现某一类事件在这 100 件事情中出现的频率。这就是最早的概率概念。然而，我们会发现这种客观概率对解释日常生活中的大量现象其实有一定局限性。因为我们日常生活中的许多现象都与我们的信心、主观认知以及已有的知识背景密切相关。

基于这一认识，贝叶斯注意到了客观概率的缺点。有趣的是，贝叶斯在世时并没有发表关于贝叶斯定理的论文。他去世两年后，他的一位朋友将这篇论文发表在英国皇家科学院的院刊上，这就是著名的贝叶斯定理。

贝叶斯定理也可以称为贝叶斯概率。它指的是一种什么样的概率呢？贝叶斯概率是指人们会根据已有的信息和个人信念来估计概率。这与伯努利提出的概率不同，后者是通过实际观察 100 件事情，然后计算某类事件在这 100 件事情中出现的频率。

实际上，某一类事情发生的频率，比如说它发生了 80 次，那么我们认为它的概率是 0.8。贝叶斯定理对此进行了扩充。例如，有些人不相信这个概率，比如陈启腾，他不仅观察了 100 次，而是观察了 1000 次，他认为这个事情发生的概率很小。

这就意味着，不同人拥有的先验概率可能是不同的。另一个人可能观察了 1000 次，发现发生了 800 次；而陈启腾观察了 1000 次，只发现发生了 100 次。那么，陈启腾对这个事情再次发生的信心是不足的，他的信心程度可能在 0.1 左右，而另一个人对这个事情的信心程度可能在 0.8 左右。

贝叶斯定理的意思是什么呢？就是相信陈启腾和另一位卢晓彤这两位同学，一个 0.1，一个 0.8，他们之前的观察会影响到他们在这次事件上的判断。这就是最著名的贝叶斯定理。

贝叶斯定理是很多事情讨论的起点，今天我们也会反复提到它。很多同学都明白了，我就不再详细解释了。

接下来我们说一下另一个重要的概念：先验知识中的假设。假设可以理解为一种预测或声明。这里面最重要的是零假设和备择假设。零假设是指没有效应、无差异、状态不变的情况；备择假设则是与之相反的情况。

在进行假设检验时，我们会遇到两类典型的错误：第一类错误和第二类错误。第一类错误一般称之为「错杀好人」，比如我们对一位男生说「你怀孕了」。第二类错误则是「放走坏人」，比如我们对一位明明怀孕的女生说「你没有怀孕」。

假设检验的早期发展与著名统计学家威廉·戈塞特（William Gosset）有关。他在研究如何改进家族啤酒厂的啤酒质量时，发明了这个方法。

这是最重要的一种假设检验的统计技术，称为独立样本 T 检验。它的作用是评估不同种类的谷物对啤酒质量的影响。这里涉及两个重要的术语：T 检验和独立样本。

我们前面已经明白，我们现在要比较两种小麦对啤酒质量的影响。假设它们都是正态分布，我们是否能够抽取一些具有代表性的数据，如平均值等？然而，在日常生活中，获得正态分布的数据有一定难度，通常需要较大的数据量。对于这种大量数据，我们在日常研究中相对难以获得。





因此，这就形成了这个最重要的统计技术 ——T 检验。它不再需要正态分布那么大的样本量，而是使用较小的样本量就能满足 T 分布的要求。大家可以将 T 分布理解为一个尾部较厚的分布。当数据量增加到一定程度时，它就会变成正态分布。

威廉·戈塞特（William Sealy Gosset）在这项研究技术上提出了一种新的分布，最终形成了这个最重要的假设检验统计技术 —— 独立样本 T 检验。

我可以给大家举一个通俗的例子：假设现在我们是一位教师，要比较自己教的两个班（A 班和 B 班）学生的学习成绩。我们可能无法获得这两个班上所有同学的成绩，因此我们从 A 班和 B 班各抽取一部分同学的成绩，然后对这部分同学的每个分数进行比较。假设这个班的人数...

在统计学中，我们经常需要比较两组数据。假设我们有 A 班和 B 班，每班各有 1000 人。由于收集全部 1000 人的数据比较困难，我们可以从每个班级中抽取 100 人作为样本进行比较。

我们可以将 A 班和 B 班的学生一一对比，例如 A 班的第一个学生与 B 班的第一个学生，第二个学生与第二个学生，以此类推。当 A 班学生的分数高于 B 班学生时，我们记录一次。这样，通过 100 对学生的比较，我们可以大致了解 A 班和 B 班学生的成绩情况。假设在多数情况下，A 班的成绩都优于 B 班，那么我们就可以说明两个班级之间存在显著差异。

这个例子是一个简化版的独立样本 t 检验。实际上，在日常生活中，独立样本 t 检验通过 20 世纪统计学的发展已经变得非常复杂。我们刚才提到的 1000 人样本可能符合正态分布，但 100 人的样本可能更接近于 t 分布。无论是 t 分布还是正态分布，它们都遵循某种分布规律。然而，在日常生活中，我们遇到的大量数据可能并不遵循特定的分布规律，而是具有更强的随机性。

因此，统计学的技术发展将数据处理分为两类：一类是遵循分布规律的，称为参数检验；另一类是不遵循特定分布规律的，称为非参数检验。统计学已经发展出了无数方法来处理这两类数据。

例如，当我们进行独立样本 t 检验时，如果涉及更多的变量，我们就可能需要使用多元变量方差分析。此外，还有相应的显著性检验方法。这些参数检验方法在非参数统计中也有相对应的三类方法。

在我们的日常生活和科研工作中，非参数统计方法的使用频率非常低。翻阅科学论文，大约一万篇中才能找到十篇使用非参数统计的文章。绝大多数情况下，我们都是以数据的正态分布、T 分布等参数统计方法为主。这类统计方法是我们统计学的主流，当今大多数重要的实验科学发现都建立在这个基础之上。

然而，这一基础现在面临着很强的挑战，这一点我们后面会讲到。目前大家应该已经明白了因果归纳的优点和缺点。它能够初步揭示变量间的关系，应用范围广泛。我们今天绝大多数的实验科学知识其实都来自于以参数假设检验为主流的方法。但它也有明显的缺点，最严重的是相关性不等于因果关系等问题。

这里我要提醒大家，因果归纳和我们后面要讲的因果逻辑有很多相似之处，所以我们后面还会详细讲解。前面我们评判演绎推理的标准是有效性，而现在评判归纳推理的标准是可靠性。归纳的可靠性指的是归纳能否作为一种可靠的推理方法。一般来说，我们可以从几个角度来评判归纳偏差。

接下来，我们对前面讲的三种归纳进行一个详细的总结。大家要注意，我们最熟悉的是内比归纳。这种归纳方法我们很容易忽视，但在日常生活中我们使用得最多的恰恰是内比归纳。因果归纳则与我们的整个科学知识体系强烈相关，它强调相关性的归纳逻辑。

这种方法的优点我们大家也已经明白了。在演绎逻辑的时期，数学、物理、化学都是从概率系统出发的，尤其是数学。当时在 16 世纪、17 世纪，这些学科的发展几乎是同步的。

在这个时期，归纳逻辑逐渐形成。数学转变为实验数学、统计数学和应用数学；物理学分化为实验物理和统计物理；化学也发展出实验化学和统计化学。这些学科不再仅仅从演绎角度出发，而是更加注重实践和应用。因此，21 世纪绝大多数人类文明的成果都建立在归纳逻辑的基础之上。

另一个重要意义是促进了科学家群体的职业化。1660 年，伦敦皇家学会成立，我们前面提到的贝叶斯定理就发表在该学会的刊物上。同样，1666 年巴黎科学院成立。这个时期逐步出现了今天我们所说的科学家这一群体。

接下来，我们要讨论归纳逻辑的局限性，这是一个非常重要且有趣的问题。只有理解了归纳逻辑的局限，我们才能真正让自己的知识体系迈入 21 世纪。归纳逻辑最重要的局限是所谓的「归纳之谜」。我们在前面的课程中已经介绍了休谟问题，这个问题在哲学史上还有另一种称呼，叫做「旧归纳之谜」。

休谟问题源自他的《人性论》（这本书后来被《人性研究》取代）。休谟指出，我们的因果判断来自习惯和经验。他对归纳推理提出了强烈质疑，主要质疑两点：

第一，归纳概括是非必然的。例如，我们每天都看到太阳升起，但这并不意味着明天一定能看到太阳升起。假设宇宙即将毁灭，那么第 n+1 天太阳可能就不会升起了。

第二，休谟认为归纳概括的过程注定是非必然的，即得出的结论不一定正确。即使是像太阳每天升起这样我们已经观察了无数次的现象，也不能保证它永远如此。

我们永远不能把「明天太阳还能升起」这样的预测视为绝对。这就引出了第二个关于归纳问题的质疑：归纳预测是循环的。我们如何证明归纳推理的有效性呢？我们依赖的是过去的成功经验。例如，我们过去每天都看到太阳升起，所以我们认为未来太阳每天都会升起。但是，这本身又是一种归纳推理，因此形成了一种循环论证：用归纳来证明归纳。

让我们回到图表来看，会更清楚。大家可以发现，我们看到的是太阳在过去的每一天都升起了。然而，当我们说第 N+1 天太阳一定会升起时，永远会存在一个新的可能性。在进行规则概括时，我们实际上把它变成了一种全集，即「所有天太阳都必然升起」。当我们进行推理时，就意味着明天太阳还会正常升起。

我们用金属膨胀的例子来说明这个问题。当我们归纳出「所有金属加热后都会膨胀」这个结论后，如果发现一种新的金属，我们就会预测它加热后也会膨胀。这个论证循环是如何出现的呢？假设我们发现了一种新的金属，但它不会膨胀，这时科学家可能就不会将它归类为金属了。大家明白了吗？这就是归纳逻辑的天然弊端。

这些天然的弊端在 21 世纪成了很多更经历用的点。大家听懂了吗？这就是休谟问题。休谟认为，人类的归纳推理注定是无效的。他比较反对培根的观点。那么，他是如何解决归纳存在的两个逻辑漏洞的呢？他认为归纳概括是非必然的。虽然归纳概括我们这个...

休谟认为归纳推理不是必然的，但在日常生活中我们仍然大量使用这种推理方式。例如，我们认为明天太阳会升起，金属加热会膨胀。然而，当面对更复杂的情况时，这种逻辑变得十分复杂。

休谟解释说，归纳概括的非必然性源于人类心理的习惯。我们习惯性地认为明天太阳会升起，所有金属都会因加热而膨胀。同样，对于归纳预测的循环论证，休谟也认为是一种经验习惯。我们基于过去的经验，认为所有金属加热都会膨胀，并将这种属性归因于金属本身。

因此，休谟提出了他最著名的观点："习惯是人生的最大指导。」这是因为休谟认识到，尽管人类日常使用的归纳推理如此不可靠，存在着归纳概括不完全和归纳预测循环论证的深层问题，但我们仍然每天都在使用这种推理方式。这既是我们的心理习惯，也是我们的经验习惯。

休谟的重要贡献在于他敏锐地意识到所有规则在形式上都有局限性。这种局限性在哲学上被称为「不对称」，并在 21 世纪形成了一个重要流派 —— 不对称哲学。休谟在解决这个问题时，将焦点完全集中在习惯上，但并没有进行更深入的剖析。

需要注意的是，这个话题涉及复杂的哲学和逻辑难题，需要更多的解释和讨论才能全面理解。

为了使解释更加通俗易懂，我们可以画一些图。在日常对话中，我们通常会遇到两类归纳推理。第一类是典型的归纳推理，例如「太阳会升起」、「金属加热会膨胀」等。第二类是反向归纳推理，即质疑既有结论，如「太阳明天不会升起」或「金属加热不会膨胀」。显然，在我们的日常生活中存在这两类推理，一种是好的，另一种是不好的。

休谟从形式上指出了所有归纳推理在形式上都是有局限性的。因此，我们需要继续思考在内容上哪些归纳是可能是好的，哪些可能是坏的。这是休谟的一个重要贡献。

20 世纪一位伟大的哲学家古德曼在他的著作《事实、虚构与预测》中做出了重要贡献。这本书于 1954 年出版，我在 2014 年读过，并在许多著作中提到过。直到最近准备课程时，我才真正理解了这本书的精髓。

古德曼的贡献主要体现在「新归纳问题」上。他构建了一个非常巧妙的思想实验。我在 2013 年曾用文学的方式描述过这个实验，当时的描述可能比原文更加优美。现在我会慢慢讲解这个例子，他是如何巧妙地构建的。

首先，请大家想象自己有一颗宝石。你每天都在观察它，发现第一天、第二天、第三天、第四天它都是绿色的。这时，你可能会形成一个归纳概括：这颗宝石永远是绿色的。大家明白吗？

古德曼非常聪明，他发明了一个新的英文词「grue」（绿蓝）。这个词之前在英语中并不存在，是由「green」（绿）和「blue」（蓝）组合而成。他给「grue」下了一个定义：在 2024 年 8 月 25 日之前被观察到的物体是绿色的，但在这个日期之后变成蓝色。我们把这种宝石称为「绿蓝」。

现在，让我们回到最初的宝石例子。显然，我们观察的这颗宝石...

首先，我们给出了一个定义，大家注意，这是一个相对关系。只要在今天之前观察到它全是绿色的，我们就可以将它定义为绿色。但是，如果从明天开始观察到它是蓝色的，我们就可以将它定义为绿蓝色。这构建的是一个相对的关系，而非绝对的关系。

显然，我们之前提到的那个宝石也是绿色的。这时，我们就形成了两个结论：第一个结论是我们的宝石是绿色的；第二个结论是我们的宝石是绿蓝色的。在今天之前，我观察到它全是绿色的，符合「绿蓝」这个词的定义。

此时，当我们进行归纳概括、归纳推理或归纳预测时，就会形成一个矛盾的结论。大家发现了吗？这个矛盾的结论是如何形成的呢？第一个结论是宝石永远是绿色的，因为它一直是绿色的，我们归纳概括时得出的结论是绿色的。第二个结论来自于我们引入的「绿蓝」这个词，它有一个特点：过了某个时间点，到明天就开始变成蓝色。这个说法归纳预测的结果是，过了一个时间点后它变成蓝色了。这两个结论显然是相互矛盾的。

大家理解了吗？这就是古德曼的《事实、虚构与预测》这本书所提出的问题。这本书在 20 世纪和 21 世纪的哲学著作中排名大约第三或第四，仅次于维特根斯坦的《哲学研究》。

你会发现，这是一个很典型的哲学家的思维方式。我们谈维特根斯坦的时候，讨论的是家族相似性的思想实验；而在讨论古德曼时，我们谈的是「绿蓝」悖论。这显然与我们的生活常识完全冲突。为什么会形成这么强烈的反直觉结果呢？这在哲学史上被称为「新归纳之谜」，与休谟问题相对应。它实际上是将休谟问题进一步深化了，提出了一个新的问题：我们究竟如何确定哪些归纳是合理的？显然，像这个归纳，为什么引入一个词之后，整个逻辑就发生了变化？

顾雷马对这个问题的解答方式很有趣。你会发现，这些哲学家在提出问题的同时，也在尝试解答这些问题。顾雷马认为这个预设是基于习惯的，他遵循了休谟的传统。我们习惯了使用「绿」这个词，但像「绿蓝」这样的词是人造的新词，它并没有得到我们语言习惯和生活经验的支持。

接下来，古德曼提出了一个新的哲学体系，称之为「投射哲学」。他认为在日常生活中广泛使用的归纳推理是有效的，并将其称为「投射」。"投射」的意思是表示「绿色」这个词能够从过去投射到今天，再投射到未来。而我们制造的新词「绿蓝」不具备这种投射能力，既不能从过去投射到今天，也不能投射到未来。因此，它会被时间的河流阻挡，必然产生悖论。

通过格鲁悖论，我们发现归纳推理深受我们使用的词汇和概念的影响。当我们把「预设」这个词换成「预览」时，整个推理就完全行不通了。在这个基础上，古德曼形成了更复杂的哲学体系。

古德曼的新归纳主义促进了 20 世纪哲学家对归纳推理合理性的深入探讨。目前，对这个问题解答相对成功的是一位在 1988 年发表论文的哲学家。我们前面介绍的贝叶斯干预作为一种主观干预，实际上能够较好地解答这一问题。

新归纳之名首先是人类构造的，它没有受到传统支持，因此其先验干预强度较低。相比之下，日常使用的词语已经广泛应用，其先验干预强度相对较高。

在计算时，我们需要对未来进行预测，得出市场值。然后，结合先验干预和市场值，计算后验干预，最终得出结论。显然，所有干预都是绿色的情况比所有干预都是绿蓝的情况更有可能，其可能性干预会更高。大家理解吗？这是从贝叶斯的角度来解答的。

苏博的方案其实有一定创新。通过苏博的解答，我想让大家意识到一个很重要的问题：我们前面谈到的新归纳之名，不仅让我们从形式上意识到归纳推理存在先天局限，同样也让我们从内容上意识到存在好的归纳和坏的归纳。当遇到一个坏词时，整个归纳推理可能变得太过荒谬。

那么，我们该如何解决这个问题呢？举个例子，假设遇到一个顽固者，他拿着一个说明称明年太阳不会升起，所以金属碰到加热不会膨胀。大家明白了吗？对付顽固者最好的方法其实就是贝叶斯理论。我们可以告诉他，过去数千万年来太阳每天都会升起，这个先验概率是非常高的。而我们碰到一个罕见事件的先验概率是非常低的。大家听懂了吗？这是对付顽固者相对较好的一种方法。

接下来，我们再看一下归纳推理的第二个较大局限。在古希腊时期，所有学者一生下来就注定是博学者，不受限于特定学科。但在现代科学体系的发展下，我们每个人都被无穷无尽的学科所牵制。最终，我们需要掌握大量的实验性知识，如实验心理学、实验物理学、实验医学等。你会发现我们被大量无穷无尽的碎片化知识所束缚。

人工智能在构建人类知识全貌方面存在较为严重的局限性。除此之外，还有第三个重要的局限，即归纳推理，特别是因果归纳。在这方面，人工智能面临的实验往往是受限的。我们有大量的实验无法进行，尤其是涉及伦理道德和人类相关的实验。

在处理因果关系时，因果归纳存在一个显著的特点：它往往是单线程的，即 A 导致 B 或 A 导致 C。然而，我们的日常生活经验告诉我们，事物发展的结果通常是多因多果的。在处理这种复杂的因果关系时，因果归纳变得非常无力。

这就是为什么我反复强调，对于从事科研的同学来说，今天的实验科学这种认知方式所能带来的红利已经变得越来越小。在学术界，仅靠实验科学很难获得显著成果。从 17 世纪到 21 世纪，我们已经有整整 300 年的实验科学历史，最容易产出成果的时期已经过去了。

对于 21 世纪见多识广的人来说，实验科学中那种单一原因导致单一结果的结论越来越难以具备说服力。因此，我们需要开发新的逻辑处理系统。

对于前面提到的演绎与归纳这两个经典的逻辑系统，我们可以进行一个简单的总结。演绎逻辑面临的问题是：演绎的普遍前提来自哪里？例如，数学推理中的公理来自何处？而归纳逻辑面临的问题则是...

从具体事例中总结出一般规律的能力是目前两个逻辑系统的核心问题。围绕这两个逻辑系统的问题，最终形成了三个不同的哲学分支。

第一个分支是演绎逻辑，其结论包含在前提中。演绎的普遍前提来自于先验论，最典型的代表是康德。康德的先验论是最容易被人接受的，他将人的普遍先验论系统化。我们前面也讲过，康德的先验论，尤其是关于范畴的论述，已经得到认知科学的实证支持。例如，人一出生就具备对空间和时间的感受能力。

第二个分支是经验论，第三个分支是介于两者之间的折中派。这些分支都涉及归纳逻辑的问题，也就是大家现在所熟知的休谟问题，即归纳之谜和新归纳之谜。归纳逻辑实际上有一个默认的前提，那就是自然的齐一性。这意味着未来的事实必然来自于过去，比如未来太阳升起这一事实与过去太阳升起是具有齐一性的。

这种自然的齐一性表明未来受制于过去，未来的事实类似于过去，无论是相似性还是绝对性，这些只是不同的表达方式。这个概念非常类似于一个公理，它特别像是一个演绎逻辑。因此，我们最终构建了两大逻辑的一种新的解法，这实际上变成了一个类似于「先有鸡还是先有蛋」的问题。

目前，我们有一个相对较好的认识：演绎逻辑的普遍前提来自于归纳逻辑，而归纳逻辑的前提又来自于演绎逻辑。这样，它们就形成了一个相互依存的循环。通过这种方式，我们能够解决许多问题。

我们已经攀登到了逻辑的高阶，终于到达了英国逻辑这一层次。

部分内容涉及相对熟悉的演绎逻辑和归纳逻辑，而高级逻辑则是大家最不熟悉的。既然我们已经有了归纳逻辑，为什么还要引入演绎逻辑？特别是在我们已经有了演绎归纳的情况下，为什么还要引入演绎逻辑系统？这个问题在哲学史上一直存在强烈争议。不少哲学家认为人类只有两大逻辑系统：演绎和归纳，不存在其他逻辑系统。但现在，尤其是 21 世纪以来，越来越多的哲学家不同意这个观点。

让我们回顾一下 21 世纪的观点。前面我们对因果规则这个逻辑规则进行了总结：A1 导致 B1，A2 导致 B2，A3 导致 B3，所以 An+1 会导致 Bn+1。这里会出现一个非常有趣的问题：如果我们将「导致」这个典型的因果关系词换成「伴随」，即 A 伴随 B，会不会显得更像一个典型的因果归纳？

这时我们会发现，传统的因果归纳似乎变成了一种文字游戏。你用「导致」，我用「伴随」，本质上都是在描述相关性。无论是相关、伴随还是因果，都是在描述某种关系或直接影响。

统计学之父皮尔逊，作为现代统计学的创始人和奠基者，就是从这个角度出发的。他强烈反对我们应该研究任何因果关系，认为因果关系是不可证实的。他发明了一个最重要的概念 —— 相关系数。所有接受过高等教育的人都接触过相关系数这个概念。

我们可以用一个简单的例子来说明：假设我们有 100 个 A 主题的数据和 100 个 B 主题的数据，我们之前是通过比较大小来分析它们之间的关系。现在，我们可以用更精确的方法来分析这种关系。

皮尔逊相关系数是一种计算两个变量之间关系的统计方法。它通过累积相关系数，最终用一个整体数字来代表变量间的关系程度。

皮尔逊作为现代统计学的奠基者之一，他从多个方面反对英国经验主义。首先，他认为英国经验主义所追求的因果关系是不可验证的。这让我们想起前面讨论过的「本质」这个概念。从培根最初探求热的本质，到后来经过三百年的发展，科学界形成了一种称为「本质主义」的思潮。这种思潮导致研究者们试图探索情绪、动机、认知等各种现象的本质。

然而，皮尔逊认为这种做法是毫无意义的，并且会阻碍科学的进步。他认为因果关系是不可验证的，这一观点可以从两个角度来理解：从形式上看，休谟已经证明了因果关系的不可验证性；从内容上看，因果关系的描述受制于所使用的语言和概念，稍微改变用词，其含义就可能完全不同。

基于这些观点，皮尔逊成为了操作主义的代表人物之一。操作主义放弃了追求事物本质的做法，而是转向建立可操作的概念。例如，在研究幸福感时，我们首先需要给出幸福感的操作性定义，然后才能进行相关研究。

皮尔逊之所以要放弃因果关系而转向相关关系，是因为相关关系更加客观，并且可以通过数据进行验证。在皮尔逊的时代，通过数学方法验证因果关系是非常困难的。当时验证因果关系的唯一方法是随机对照组实验，这种方法在许多情况下难以实施。

因此，皮尔逊提出了相关系数这一概念，为统计学的发展做出了重要贡献。

从 20 世纪皮尔逊所处的时代出发，他认为人类注定无法以合理的成本验证因果关系。我们最终只能在医学领域通过发明药物，采取昂贵且难度很高的随机对照组试验来验证因果关系。

皮尔逊认为应该放弃因果关系研究，主要是因为这种研究很容易导致错误和偏见。通过我们前面对因果归纳的介绍，可以发现确实如此，因果推断很容易受到人的主观影响。在皮尔逊强大的影响力下，整个统计学界差不多有 30 年时间放弃了对因果关系的探讨。在这 30 年里，统计学界达成了共识。

如今 21 世纪，我们在统计学教材中学到的内容，无论是心理学、经济学还是其他学科的统计教育，都不会涉及因果关系的探讨，也不会有其定义。唯一涉及因果关系的就是随机对照组实验。可以看出，皮尔逊的影响一直延续到 21 世纪的今天。

皮尔逊的观点之所以如此有影响力，是因为它确实有一定道理。我们可以用一个问题来看看当今教科书上学到的心理统计、教育统计、经济学统计存在什么根本性的局限，以及为什么皮尔逊的批评是成立的。

这是一个常见的关于因果效应的探讨：我们如何确定 X 和 Y 之间的因果关系？通常的做法是让人服用某种药物，然后观察是否能改善健康状况。这时会出现一个非常经典的问题，称为辛普森悖论。它指的是：一类患者服用某种药物后身体状况改善了，但没有考虑到另一种情况，即这批患者不服用这种药物，身体状况是否也会改善。这一点非常关键。

这是整个统计学的一个真正突破，非常关键，我在这里要放慢一点解释。传统统计学是如何处理的呢？它是这样处理的：把其他因素当作混杂变量，比如人口学变量（如性别、年龄等），这些都称为混杂变量。然后通过消除混杂变量后再来寻找因果效应。

接下来就开始做实验，找一批病人来吃药。我们一般将参与者分为处理组和对照组，也称为吃药组和没吃药组。这时，吃药组就会开始出现各种各样的情况，没吃药组也可能出现各种各样的情况。这个图非常关键，大家会发现这里漏掉了一类很重要的情况，就是没有考虑如果不吃药会怎么样。

在传统统计学中，对这类情况的处理更多是把它当作一个群体层面的因果效应。比如说，我们举个例子：假设有一篇论文是关于治疗抑郁症的某种疗法。通过传统统计学的处理方式，得出的结论其实是一个群体层面的有效性，即这个疗法可能从群体层面确实对抑郁症治疗是有效的。

但是具体到个体上，如果没有接受这个疗法，他的抑郁症会不会变好？对于这部分问题，传统统计学的处理是非常无力的。所以说，当年我在心理学系就读时学习心理测量和心理统计学，接受到的教育都是这样的：任何心理学结论都是概率性的，有一个置信区间，比如 0.95 的置信区间或 0.99 的置信区间。

当时统计学老师似乎在告诉我们，要相信任何心理学的结论都是概率性的，也就是说在群体层面有效，但在个体层面一定要谨慎处理。这种思维方式会导致一些问题...

在现实生活中，我们无法让一个人同时服药和不服药。这是一个无法在现实中实现的复杂问题。由此可见，群体有效和个体有效是两个不同的概念。

在心理学科普文章的评论区，经常会有人说："你们讨论的是群体有效性，但我个人的经历证明这种方法对我是有效的。」传统上，我们很难处理这种情况。面对这些质疑，我们不应该简单地认为对方缺乏心理学或科学训练。实际上，21 世纪的科学已经能更好地解决这个问题了。

皮尔森认为只有相关没有因果，这种观点有一定道理。但这种反对意见也引发了一些问题：难道我们真的无法发现因果关系吗？难道因果关系仅仅是一种语言游戏吗？如果是这样，我们今天介绍的所有统计学教育最终都变成了这样的认识，我们是不是只能从群体层面讨论因果效应？

例如，为了证明一种药物有效，使其达到上市标准，我们会发现说明书中会留出一些容错空间。这时，似乎就变成了群体层面的因果效应证明。

让我们看看同时代的研究者是如何逐步迭代的。首先，他们引入了随机化的概念，形成了随机对照组实验（通常称为 RCT）。这已成为当前新药研发的标准，所有药物要达到上市标准都必须通过随机对照组实验。随机对照组实验主要处理的是这一部分问题。

然而，随机对照组实验也存在一些典型问题，比如无法完全控制人与人之间的大量行为互动等因素。

在这个基础上形成了双盲实验。我们可以发现，通过二十世纪的进展，实验方法从对照组实验发展到随机对照组实验，再到双盲实验。双盲实验可以理解为随机对照组实验的迭代版本，即让参加实验的人和主持实验的人都不知道自己属于哪个组。

这种实验方法存在几个很大的问题：首先是成本很高。例如，药物研发动辄需要花费十亿美元，难以进行。随机对照组实验一般只能用于非常昂贵的药物研发部分。在经济学、心理学领域，真正的随机对照组实验其实并不多见。其次是伦理道德问题，比如为什么让这批人吃药，而不让另一批人吃药。

因此，历史上出现了一个较大的问题：除了随机对照组实验之外，其他因果效应难道真的无法验证吗？事实上，很多哲学家和统计学家对这个问题提出了质疑。

皮尔森先生是古典英国逻辑的提出者。1878 年，他总结了三大逻辑系统：演绎逻辑、归纳逻辑和溯因逻辑。他将归纳逻辑和溯因逻辑视为可以扩展的逻辑。这也是皮尔森最著名的演讲，收录在他的著作中。在皮尔森的基础上，以及更多哲学家的努力下，形成了真正的因果逻辑。我们可以用一个简单的图示来总结这一发展过程。

逻辑推理可以分为几种主要类型：从特定结论到普遍性命题的推理，即从具体到一般的归纳逻辑；以及从一般到具体的演绎逻辑。而溯因逻辑与前两者有所不同，它是一种从结果推导原因的逻辑方法。

最早研究溯因逻辑的是皮尔斯（Pierce）。他最初主要从逻辑学和哲学的角度进行研究，并未对其进行深入的形式化工作。皮尔斯的溯因逻辑也被称为溯因推理或最佳解释推理。这种推理方法是指当一个结果发生后，我们寻找多种可能的解释，并从中找出最能解释该结果的原因。这种方法也被认为是溯因逻辑的一种表现形式。

与皮尔斯偏重哲学和逻辑学的研究不同，另一个流派对皮尔斯的相关理论进行了强烈的批评。他们最终从数学、统计学和哲学等多个角度成功论证，我们确实能够在非随机对照组实验之外验证因果关系，并进行数学计算。这个流派通常被称为溯因推理或溯因推断。

在本书中，我们将这两个流派统称为溯因逻辑。皮尔斯的方法可以被视为寻找最佳解释的一个例子。

回到我们前面讨论的逻辑系统，可以发现溯因逻辑更多地涉及到「如果... 那么...」的推理结构。理解了这一点，我们就能更容易地理解皮尔斯等人观点中的不足之处。例如，他们努力寻找最佳解释，但这种解释通常是在事情已经发生之后才得出的。实际上，比最佳解释更重要的是预测和预防。

在现代英国逻辑争论中，人们发现比事实更重要的是反事实。这意味着我们需要考虑「如果」的情况，例如一个人如果不吃药，他的抑郁症能否好转。这种思考方式被称为反事实思维，在心理学的不同领域有着不同的称呼。在儿童心理学中，我们称之为「反事实思维"；在心理理论中，我们也使用这个术语；在行动心理学中，我们称之为「执行功能」。由此可见，反事实思维是人类的一种关键能力。

现代英国逻辑体系就建立在反事实思维的基础之上。反事实思维的最早描述可以追溯到休谟质疑归纳法时的论述，但真正的突破发生在 1973 年。这一年，哲学家大卫·刘易斯（与模态逻辑创始人同名）在 32 岁时出版了《反事实条件句》一书。

反事实条件句的一个简单例子是："如果昨天我买了那张彩票，今天我就是百万富翁了。」显然，说这句话的人并没有买彩票。这种表达方式被称为反事实条件句。

值得注意的是，这里提到的两位刘易斯虽然同名，但并无血缘关系。前面提到的刘易斯在模态逻辑中提出了一个重要概念 —— 可能世界。这个概念是在我们之前讨论演绎逻辑发展过程中提到的。

总的来说，反事实思维为我们提供了一种新的认识世界的方式，让我们能够思考在现实世界中可能性不大的情况。

人既吃药又不吃药，那么我们分析让他不吃药这个事情发生在可能世界。大家听懂了吗？这就形成了刘易斯的反事实框架可能世界。刘易斯认为存在无数个可能世界，每个世界呈现不同的情况。

我们可以将这些世界理解为平行世界或可能世界。显然，有些可能世界距离我们的现实世界比较远，有些则比较近。距离我们比较近的被称为最接近的可能世界。

第三个概念是反事实的真假。它实际上指的是假设语句的真假取决于在最接近的可能世界中结果是否真的发生。

让我们举一个通俗易懂的例子来解释这个概念。刘易斯规则考虑没有某个条件时结果会如何变化。比如说，在现实世界中，我设置了闹钟，最终上班没有迟到。可能事件一是我没有设置闹钟，然而我也没有迟到。可能事件二是我没有设置闹钟，我迟到了。

在这种情况下，迟到又可以分为 15 分钟、30 分钟、一个小时、两个小时等。显然，最不太可能的情况是没有设置闹钟但迟到两个小时。相对而言，更可能发生的是没有设置闹钟迟到 30 分钟。所以，在众多可能世界中，最有可能发生的那个就是最接近的可能世界。

从这个讲解出发，我们来看个体在 21 世纪英国逻辑中的四个要素。第一个要素一般被翻译为「单元」。但为了保持我们对基本知识体系的统一，我们在前面全部使用「实体」这个词。大家需要注意，这只是我们使用的词汇，在英国逻辑中，绝大多数时候是把它翻译成「单元」。它的含义是指我们究竟是处理一个班、一个年级还是一个学校这样的实体。

在因果推断中，有几个重要的术语需要理解。首先是「单元」，它在潜在结果框架中代表不同的实体对象。其次是「干预」，通常用字母 T 表示。干预可以理解为一种处理或行动，比如设置闹钟或吃药等。

干预通常分为两种类型：人工干预和自然干预。人工干预是指我们主动进行的处理，如在新药研发中让一组人吃药，另一组不吃药。自然干预则是由大自然形成的，如某地水质较好、山势特殊、气候炎热或寒冷等。

另一种常见的分类方法是将干预分为二值干预和多值干预。在日常生活中，我们接触最多的是二值干预，如吃药与否。而在科学研究中，我们还会遇到多值干预，例如让不同的人学习 30 分钟、1 小时或 2 小时等不同时长。

第三个重要概念是「变量」。在随机对照试验中，我们经常使用一些客观指标作为变量，如小麦产量。然而，在因果关系研究中，很多变量并没有客观指标，这些变量被称为「潜变量」。潜变量无法直接观察，比如一个人的幸福感或主观感受。事实上，绝大多数心理学变量都是潜变量。

潜变量上定义的另一个重要概念是潜在结果和事实结果。这个概念的含义是，每个人在接受每种处理后可能获得的所有可能结果。在现实世界中实际进行的处理所获得的结果通常称为事实结果。而在可能世界中进行处理所获得的结果则称为反事实结果。无论是事实结果还是反事实结果，都构成了一个结果空间，包含各种可能获得的结果。

协变量指的是与我们研究相关的一些属性，例如人口学变量、性别变量等。这些变量不是我们的干预处理，但会影响到研究结果。这就是反事实框架中的第三个重要概念。

第四个重要概念是效应。我们前面提到，绝大多数心理学研究处理的是平均因果效应，它是基于群体的推理。因此，这种方法不一定能获得个体的因果效应。从 20 世纪逻辑学和统计学的发展到 21 世纪，我们现在已经能够更好地获得个体层面的因果效应了。

通过这些概念，我们可以重新思考随机对照组实验。你会发现这种实验方法存在一个较大的问题：我们最终获得的只是事实结果，而无法获得反事实结果。我们无法知道那些被安排服药的实验组成员如果不服药，他们的病情会如何发展。这是传统随机对照组实验的一个主要问题。

第二个较大的问题是，在无干预的观察环境下，我们的实验往往不是随机分配的。这意味着实验样本可能存在偏差，而且这种偏差可能比我们预期的要大得多。

使用反事实框架来审视我们今天的统计学教材，可能会发现一些问题。

因果推理现在能够很好地解决这个问题。大家可以看到，他们重点是如何基于反事实来进行因果推断。这一解决思路形成了两个大的流派：第一个流派称为潜在结果框架，第二个流派称为结构方程模型。这两个大的流派各有优缺点。相对而言，潜在结果框架出现得更早一些，而结构方程模型则是在其基础上解决了一些前者无法解决的问题。

这两个流派的根本区别在于：潜在结果框架是把因果关系表达为结构方程。结构方程是什么意思呢？大家可以简单地理解为我们在初中、高中、大学都学过的回归方程。实际上，你可以把结构方程理解为比回归方程包含更多类型变量以及更复杂的一种方程。应该这么说，我们学过的那种线性回归方程一般都会被认为是结构方程中的一种特例。

而结构方程模型则是把因果关系表示为因果图，而不是结构方程。

接下来我们来看一下两个流派是如何进行因果推断的。首先看一下潜在结果框架，这个模型是由学者鲁宾在 1974 年首次提出的，另一个重要成果是在 1983 年发表的。这两个成果杨晓飞讲到过。鲁宾实际上是从反事实的思路出发，针对观察数据提出了各种各样的方法。

我们可以看一下鲁宾的这篇论文。这里有传统的一元回归方程和多元回归方程，他用这些来表示效应。那么我们重点来看一下鲁宾他们这批人是怎么来估算因果效应的，他们进行了一些重要的创新。总共这个流派最终有五个技术。这五个技术具体是如何做的，各位同学今天可能难以完全理解。

各位同学一定要明白这些方法各自解决的是什么问题。你要清楚在这个研究方向上它能帮你解决什么问题，未来还可以邀请更多的人来协作。接下来我会介绍一些统计方法。

首先我们来看第一类，叫做自然实验。自然实验是由 1989 年诺贝尔经济学奖得主提出的。我们都知道随机对照组实验，但它的要求太高了。在医学新药研发领域，可能需要投资 10 亿美元才能完成。但在经济学、心理学研究中，我们很难做到随机对照实验。那么该怎么办呢？

这位诺贝尔经济学奖得主发现，在经济学领域，尤其是政策研究方面，当政策实施后可能会产生某些影响，这时很难进行随机对照实验设计。传统上，经济学与实验科学的关系相对较远。但这位得主提出了一种新的实验技术，他认为世界本身就已经包含了真正的随机性。那么，我们为什么不从这种自然存在的随机性出发，来获得因果效应呢？

举个例子，比如人的基因天然具有随机性。他就是从这个角度出发，最终形成了一个很大的研究流派，叫做自然实验。自然实验指的是利用自然界中随机发生或随机分配的事件作为研究因果效应的工具。

我们可以对比一下几种研究方法。第一类是民科们进行的自然观察研究，这种偏向自然观察，统计学基础较差。第二类是新药研发中的随机对照实验。除了这两类实验之外，还有第三类，就是我们刚才讨论的自然实验。它既不同于民科学者简单地收集数据，也不同于严格的随机对照实验，而是利用自然界中已存在的随机性来进行研究。

已经具备随机性的数据可以用来进行研究分析。让我们来看一些具体的例子。前面我们谈到的基因就是一个例子，兄弟姐妹之间的基因差异就是一种天然具备随机性的现象。这是一种自然实验的设计。

自然实验的设计在我们现在的研究中是一个较新的概念，大约在最近十年才开始兴起。需要注意的是，我们今天讨论的所有因果推断的新知识都是最近十年才开始流行的。如果你在学术圈发表论文，使用这些方法基本上现在是比较容易的，因为这个领域的研究竞争还不太激烈。相比之下，实验研究的竞争则比较激烈。

自然实验对我们的一个启发是什么呢？我们发现它的逻辑规则实际上是去寻找自然界中天然具备的随机性。这意味着我们在日常生活中需要思考哪些现象具有天然的随机性，并利用它们来进行研究。

接下来我们来看第二个技术，叫做倾向得分匹配。我们刚才谈到匹配的时候，你会发现在进行随机对照组实验时，我们实际上就是在进行匹配，尽量减少干扰变量的影响。例如，在进行实验设计时，我们会自然地排除一些干扰变量，比如年龄过大或有严重基础疾病的人可能不会被选入某些研究中。

这些是传统的匹配方法，其中还包括一些典型的方法，如分层匹配。然而，这些传统匹配方法面临的最严重问题是所谓的「维度诅咒」。比如说，我们可能保证被试的性别相近、年龄相近、家庭收入相近等。通常来说，我们会考虑三四个人口学变量，再加上一个在研究中通常称为社会经济地位（SES）的变量。SES 通常包括三到七个变量，结合人口学变量和其他相关变量。

在研究设计中，社会经济地位、家庭经济状况和年龄、性别、家庭收入等通常是最常见的匹配考虑变量，一般会选取三到七个变量。然而，这种做法会导致一个严重问题：当我们按照不同变量进行分层时，比如先按照年龄将 1000 人分成 500 人一组，再按性别分成 250 人一组，随着分层的继续，样本量会越来越小。这不仅超出了人类的认知负荷，也使得可用于分析的数据量大幅减少，进行匹配变得非常困难。

以股票投资为例，研究一个公司的竞争力可能就涉及 30 多个变量。在这种情况下，要保证比较对象在所有维度上都相似几乎是不可能的。因此，实际操作中，我们通常只选取三到七个变量进行匹配，这种做法虽然简单粗暴，但也是无奈之举。

这种困境被称为「维度诅咒」。为了解决这个问题，倾向得分匹配方法应运而生。这个方法于 1983 年被发明，其核心思想是通过数学方法（如协方差分析）将多维度的匹配变量进行降维，形成一个综合的分数，即倾向得分。

例如，在比较一个新的量化交易模型对股市获利的效益时，需要考虑的变量可能包括公司规模、员工人数、社会口碑等众多因素。倾向得分匹配方法可以将这些繁多的变量综合成一个分数，从而简化匹配过程，使得比较更加可行和有效。

假设中国股市上有 5000 家公司，我们通过倾向得分为每家公司计算一个分数。举例来说，我们可以用 1、2、3、4 来表示 A、B、C、D 四家公司，它们分别获得的倾向得分可能是 0.9、0.8、0.7 等。

当我们要进行某类实验研究时，比如探讨一种算法，我们应该选择倾向得分接近的公司作为 A 组和 B 组。这种方法解决了许多传统实验研究中难以处理的问题。

这种方法是由罗森鲍姆（Rosenbaum）和鲁宾（Rubin）在 1983 年共同发明的。它使得在传统实验研究中难以处理的因果效应变得容易验证，同时也简化了控制干扰变量的过程。通过使用倾向得分来分配研究对象，我们可以确保不同组别之间的倾向得分相近，从而更容易验证因果效应。

这篇 1983 年的论文中提出了倾向得分的具体计算公式。本质上，它是将多维变量转化为一个倾向得分。计算步骤通常涉及使用逻辑回归等统计技术。获得分数后，开始进行匹配。常见的匹配方法有三种：最近邻匹配、卡尺匹配等统计方法。

在进行倾向得分匹配后，我们不需要过多关注具体的匹配过程，而是使用统计方法进行验证。这相当于检验匹配是否成功，然后进行匹配后的计算。这个阶段的计算与传统假设检验的计算方法类似，主要是在某个置信区间（如 0.95 或 0.99）内检验差异是否显著。整个过程主要包括前面三个步骤。

接下来，我将用一篇具体的心理学论文作为例子来说明。这是一篇发表在中国的心理学论文，研究的问题是独生子女和非独生子女在情绪适应方面是否存在差异。我们可以发现，这个研究涉及许多干扰变量，如性别、城乡差异、家庭类型、父母教育水平、是否留守儿童以及父母职业等。在心理学研究中，处理超过三个以上的干扰变量通常非常困难。

那么，我们如何通过倾向得分来处理这些变量呢？首先，我们对所有这些变量进行计算，得到一个倾向得分值。计算完成后，我们获得了这个值。接下来，我们看一下这个例子中的下一步是什么。

在获得倾向得分后，我们开始对独生子女和非独生子女样本进行匹配。匹配前和匹配后，我们使用类似于独立样本 T 检验的方法来检验它们之间是否存在显著差异。这就是所谓的匹配后分析。

在匹配后分析中，我们对匹配前和匹配后的样本进行比较。这里使用的是独立样本 T 检验。从结果中我们可以看到，在生活满意度等方面，计算出来的结果显示没有显著差异。

让我们回顾一下整个过程：首先，我们计算各个变量的倾向得分。计算出来后，接下来我们进行样本匹配和后续分析。

断点回归是一种统计方法，用于估算样本差异。它将样本分为两组，使其成为没有差异的两组。在这个过程中，两组被拆分出来后，就相当于匹配成功了。然后，我们拿着这两组进行检验，看它们在生活满意度、主观幸福感、自我效能感、孤独感、抑郁和教育这七个变量上是否存在显著差异。研究结果发现，这些变量之间没有显著差异。

通过这种思路，我们可以看到许多传统实验科学的研究结论可能并不成立。倾向得分匹配的逻辑规则实际上是尽量找到与处理组相似的对照组。传统的寻找方法效率不够高，而且人为随机分组往往效果不佳，所以现在出现了这种新的方法。

接下来，我们来看第三种方法：断点回归。断点回归与自然实验密切相关，它是由一位经济学家在 1960 年左右提出的。我们来看一个经典研究，以理解断点回归的关键点。

在很多情况下，我们会遇到类似这样的情况：比如高考录取分数线设在 550 分。那么，550 分就成为一个界限，549 分和 551 分的考生实际上没有本质区别。如果我们要研究某个问题，当发现这两组人之间存在一定的因果效应时，就可以用这个方法来证明因果关系。这就是断点回归发现因果效应的原理。

断点回归还有一些技术细节，如找到断点，将变量进一步细分等。这些都是断点回归方法中的重要组成部分。

一个经典研究基本上是断点回归。这种方法从 1990 年代开始兴起，是近 20 年来经济学最著名的研究方法之一。从图表可以看出，这种研究方法的增长曲线在 2000 年左右开始呈现急剧上升的趋势。

接下来我们快速回顾一下这个经典研究。该研究结合了自然实验的方法。具体来说，研究对象是以色列的一所学校，它与其他地方有所不同。该校人为设置了一个规定：所有中小学班级的人数上限为 40 人。如果超过 40 人，就必须拆分成两个班级。这个 40 人的规定就成为了断点回归分析的断点。

让我们看一下这个研究的结论。研究的核心原理是这样的：我们找到某个特定的临界点附近的结构变化。比如在这个以色列学校的例子中，当班级人数从 39 人变为 41 人时，我们要观察是否发生了任何变化。如果 39 人和 41 人之间没有发生任何变化，那么我们就要质疑干预效应是否存在。但是，如果在这个断点处发生了明显的变化，那就意味着这个变量（班级人数）非常重要。

这个道理对大家来说其实很容易理解。比如高考，如果你差一分没有达到一本线，那就意味着你无法进入一本院校。我们可以拿 985 高校的分数线来举例，甚至可以更极端一点，以北大清华的分数线为例。假设北大清华的分数线是 660 分，那么我们就可以比较低于 660 分和高于 660 分的考生的最终结果。

断点回归是一种统计分析方法，它可以帮助我们计算考虑北大侵犯后人们收入的变化。这种方法的优点在于，它可以揭示一些可能存在的显著断点，同时也能帮助我们发现某些影响并不存在。北大侵犯这个问题就是一个很好的例子，因为它接近于一个极限词，断点往往刚好卡在一个特定位置。这使得断点回归成为一种特别有效的工具，能帮助我们推翻一些不成立的结论。这就是断点回归在逻辑学上的应用。

接下来我们来看第四种技术，叫做双重差分法。这个概念可能有点难理解，让我来解释一下。与教科书上常见的统计处理技术不同，双重差分法是这样的：假设我们让一批人参与实验，A 组吃药，B 组作为对照组不吃药。实验结束后，我们会得到一些结果，然后进行假设检验。

双重差分法的特点在于，它引入了时间维度，对同一批人进行两次测试。比如说，我们先进行一次如上所述的实验，几年后再进行一次相同的实验。这样，我们就获得了两组数据。传统的方法通常只进行一次差分检验，而双重差分法则进行两次。最终，我们会得到一个类似于两次差分检验结果的图表。

这种方法从传统的一次差分检验扩展为双重差分，使我们能够更好地推测反事实情况，比如没有吃药的人在实验期间是否也会有所改善。双重差分法主要解决的就是这个问题。这种方法的提出者是艾斯特·迪弗洛（Esther Duflo），这是世界上最经典的自然实验方法之一。

实验是最经典的双重差分设计。这个设计在自然实验中也是最具代表性的，在科学史上非常有名。感谢同学的提问，我们可以继续看一下这个例子。大家能看到吗？这就是相应的双重差分，1849 年一次，1854 年一次。这是双重差分最原始的一个图。

这是双重差分中英国效应的估计，包括第一次差分和第二次差分。大家看到了吗？最终这个结果就是相应的。再看一下这个图，对于传统的随机对照实验来说，它只能处理群体效应，对个体效应处理得不太到位。大家听懂了吧？双重差分最核心的优势是更有说服力，因为它比较了干预前后的变化。

这也是一个经典研究，是 2018 年诺贝尔经济学奖得主的研究，非常有名。这些都是一些经典研究，我们快速略过，估计大家都已经脑子转不动了。

对我们来说，双重差分的核心意义是要明白，我们要发现因果效应，推断一个事情是否确实存在因果关系。我们尽量要引入一个时间变量。

接下来是合成控制法。这部分相对来说不像前面几个方法那么重要。大家会发现，双重差分更多是从时间的角度出发，而合成控制法更多是从其他角度出发。大家可以看一下，它是从这个角度触发的。

这是合成控制法的一个经典研究，发表于 2003 年。好的，那么我们快速略过这部分。

最后，我想说的是，这些方法都有各自常见的应用场景。大家会发现，到了因果推断这部分，和前面的逻辑就非常不一样了。它特别注重的不仅仅是这一个方面。

计量经济学中的自然实验方法主要包括符号语法语义、推导、断点回归、双重差分和合成控制等。其中，推导部分更为重要，主要是寻找自然存在的随机分布数据。经济学家通过倾向得分匹配等方法提高样本的相似性。断点回归则侧重于寻找临界值，双重差分关注实验组和控制组在不同时期的变化情况，而合成控制法则是在控制组合成后分析各自的适用情况。

潜在结果框架相对易于理解，且这些方法巧妙且成本低廉。经济学、心理学、社会学和政治学等领域都存在大量自然实验数据，如基因数据、智商测试结果、高考分数、分班人数和政策规定等。这些方法在近几十年来对计量经济学产生了重大影响，形成了专门的研究流派。

然而，这些方法也存在一些缺点。首先，它们需要事先明确所有变量和因果关系，这严重依赖研究者的理论知识和学科背景。其次，这种方法难以应用于人工智能领域。最后，当涉及到大量变量时，比如 30 个以上，这些方法的应用会变得非常复杂。

在研究中，如果涉及的变量超过 100 个，计算就会变得异常复杂。20 年前，我曾为一家世界 500 强公司研究顾客满意度，当时涉及的变量约有 30 个。即便如此，计算协方差已经变得非常复杂，最终的结果难以直观理解。

目前，绝大多数心理学研究论文的模式是：几位教授或学者假设一个方程，推测 A 变量对 B 变量可能有影响，或者假设没有影响。最终，他们往往会得出支持自己假设的结论。这种研究方法存在很大问题，因为它无法真实反映现实世界中复杂的关系。

鉴于这些技术的局限性，出现了第二类方法，即结构方程模型。这是由著名科学家、图灵奖得主珀尔提出的。2018 年出版的《为什么》和《因果推断》等书都涉及了这个主题。这方面的内容比统计方法更难理解，但非常重要，因为当前人工智能的最新进展都受到珀尔思想的影响。

我们已经讲了 246 周，总共计划 350 周，还剩约 130 周。虽然我们无法详细讲解每一项技术，但最重要的是进行全局扫描，让大家意识到这些领域已经取得了突破。这样才能避免我们的知识停留在独立样本 t 检验、方差分析等基础统计方法的层次。

值得注意的是，五年前贝叶斯推断的计算还非常困难，但现在技术难度已经大大降低。五年前无法实现的事情，现在已经变得可行。

开源远见真正成熟了，非常容易计算。这意味着我们可以利用自然界和社会中存在的大量数据进行计算。在这种情况下，发表论文变得相对容易。但是，真正的人工智能突破并不在结构方程那部分。结构方程那一块是用来发表经济学、理学论文的。真正的人工智能突破是在布尔代数这一块。今天晚上我们暂时不讲这个话题，先让大家提问。

有一位听众问道："我平时从事实验工作。我想知道，随着这些理论和模型的进展，未来是否有可能替代我们现在进行的随机双盲实验？"

对此，我要特别提醒大家一点，这也是我另一个演讲《占卜 3000 年》中提到的起点。传统的随机对照试验（RCT）成本太昂贵了。目前，人工智能正在逐步取代全范围的研发，这个趋势已经变得非常明朗，特别是在技术上有了很多突破。

举个例子，我们可能需要花费十亿美元在日常生活中进行 30 到 100 次随机对照组实验，而且可能还需要 3 到 10 年的时间。但是在计算机模拟中，可能一年时间就能运行几十万次实验。所以这绝对是未来的趋势。

因此，我特别提醒所有从事科研和高端研究的同学们，你们都要投身于贝叶斯推理这种认知方式。贝叶斯推理直接涉及到因果推断、因果逻辑等这些新技术。大家要明白，这些新技术真的都是最近五年才成功普及的。

十年或十五年前，我第一次接触贝叶斯理论时，麻省理工的一个好朋友向我推荐了这方面的论文。那时我问他如何计算，大家都说非常复杂，说不明白。但最近五年，这些计算变得特别简单，特别容易，就像我们使用计算器一样容易。

所以说，现在是一个红利时期。但如果你这个时候还在做传统的随机对照实验，时间成本都很昂贵。大家听明白了吗？谢谢。

火焰先生提到，现在人工智能的比例已经可以称为可检视的人工智能，这与 OLED 技术非常相似。目前，将可检视的人工智能与 OLED 技术结合的研究已经取得了突破性进展。

对于文字类工作者来说，如何训练自己的思维模式是一个颇具挑战性的问题。我们传统的两大逻辑系统 —— 演绎法和归纳法，经过 5000 年的科普，已经成为人类社会的共识。然而，英国逻辑和高级逻辑等新兴逻辑体系尚未成为人类社会的共识，这意味着我们正处于一个巨大的机遇期。

对于普通大众来说，最重要的是要学会反思。我们应该思考：如果现实世界是这样的，那么可能的世界会是怎样的？可能的世界有多少种？通过这种思考方式，我们的认知水平会得到提升，思维也会变得更加灵活，不易受到传统演绎推理规则的局限。

在技术方面，我们现在应该是处于红利期，而不是说已经进入普通文明位。因为这些技术太新了，随便发表、随便写出、随便做成果都是令人兴奋的东西。这些新兴的内容甚至还来不及写入教材。一般来说，本科教材和博士生教材会提到的内容包括俄罗斯、欧巴菲等。

关于大模型能力测量的问题，特别是如何确定实体、如何定义性速度，以及大模型与人类的类比问题 —— 人类有各种群体，但大模型在这方面似乎不适应整体的范畴和属性。其实这个问题并不复杂，因为现在大模型领域已经发展出了强大的新分支，包括大模型评测。在评测过程中，我们可以借鉴语言学的方法，虽然这种方法很难解释，但只要有超越性的表现就足够了。

多性定义已经足够了。无论是学术界还是人类社会，都有一些普遍的习惯。比如说，人类社会拥有的一些词语，像智商、自立等，其实也是一种习惯。颜色的概念同样是一种习惯。在大模型领域也是如此，这个新兴领域最终会形成一些被广泛接受的术语。

例如，最近有一位百讲主提出了一个新概念，他认为在人工智能史上，一个人的可学习性是最重要的指标。然而，这个指标显然是不太全面的。首先，这个指标很难成为人类社会普遍承认的一种习惯。大家要明白，这里面有很微妙的地方。我们人类对于绿色、蓝色等颜色的认知，其实也是一种学习得来的社会建构。事实上，整个世界都是如此，不仅仅是这些事情。

因此，我们早期就主张要采用实用主义和操作主义的方法，不要过分纠结于这些细节。让那些哲学家去讨论这些问题吧，他们可能几千年都无法达成共识。重要的是要解决实际问题。今天我们讨论的内容，可以看作是对我们之前所说的「被实际干预」这一概念的扩充。

这些内容听起来可能比较容易理解，因为我讲得比较通俗。但实际上，真正理解这些概念是非常困难的，特别是对于外行人来说。

如果目前没有其他问题，今天的讲解就到这里。我们下次网上再见，还有很多内容要继续讨论。大家现在可以去吃饭了。



---

### 01

然而，时至今日，在 21 世纪的大学里，我们所学的一些思维方式已经过时了。特别是在科技、管理以及人工智能领域中，这种过时更为明显。因此，引入第三种逻辑系统变得尤为重要，即现代归纳逻辑。我们需要认识到，高等教育阶段学到的知识很多都已不再适用，这些知识虽然作为讨论的起点，但现在已经不足以应对新的挑战。

干预和假设检验是前面讨论的基础。孟诺的方法实际上是一种定性的描述，而将这种定性描述转化为定量分析，则标志着干预概念的发展。最早提出这一概念的是 17 世纪的数学家伯努利。他认为，通过数值来表示可能性的大小 ——「1」代表最大的可能性，「0」代表最小的可能性 —— 这样可以更加客观地进行干预分析。

这就是所谓的客观性的含义。

这称之为客观的干预。所谓客观的干预是指我们去观察某一事件的发生频率。比如，我们观察一百件事情，发现其中某一类事件出现了多少次，这就是所谓的频率，即客观的干预。这种干预方式在最早期被提出时是最重要的概念之一。然而，我们会发现，这种客观干预对于日常生活中大量现象的解释其实存在一定的局限性，因为日常生活中的许多现象与人的信心、主观感受以及个人的知识背景密切相关。

基于这一点，贝叶斯最早注意到客观干预的缺点。尽管他在世时并未发表关于贝耶斯定理的相关论文，但在他去世两年后，他的朋友将这篇论文发表在英国皇家学会的院刊上，这就是著名的贝叶斯定理。贝叶斯定理也可称为贝叶斯的主观干预，它指的是人们会根据已有的信息和个人信念来进行判断。

前面提到过，伯努利提出的干预方法是指实际去观察某一事件发生的频率。例如，我们实际观察一百件事情中某一类事情发生了 80 次，因此认为其概率为 0.8。贝叶斯定理对此进行了扩展，提出个人的先验知识和信念会影响对同一事件发生概率的不同估计。

比如陈启腾，他不仅观察了 100 次，而是观察了 1000 次，并且认为这一事件发生的概率很小。他的这种低概率判断与另一个人不同。另一个人也观察了 1000 次，但发现该事件发生了 800 次。因此，陈启腾对这一事件再次发生的信心程度可能只有 0.1，而另一个人则有 0.8 的信心水平。

贝叶斯定理的意义在于承认不同的个体（如陈启腾和卢晓彤）基于各自的观察会有不同的先验概率估计，这些差异会影响他们对同一事件未来发生的信心程度。

这次事件的判断受到了著名的贝叶斯定理的影响。贝叶斯定理是很多问题讨论的起点，今天我们也会反复提及。这个定理许多同学已经了解了，所以我就不再赘述。

接下来我们谈谈另一个重要的概念 —— 先验知识。先验知识也被称为假设。假设可以理解为一种预测或者声明。其中最重要的是零假设和备择假设。零假设指的是没有效应、无差异或状态不变的情况；而备择假设则是与零假设相反的情形，即存在某种效应或变化。在进行假设检验时，我们可能会遇到两种典型的错误：第一类错误和第二类错误。

第一类错误通常被称为「错杀好人」，比如我们对一位男性说他怀孕了。第二类错误则可以理解为「放走坏人」，比如说对于一个确实怀孕的女性，我们却告诉她没有怀孕。

假设检验这一概念早期是如何产生的呢？它是由著名的统计学家威廉·戈塞特在研究啤酒酿造时提出的。他在一家啤酒厂工作，致力于改进啤酒的质量。正是在这个过程中，他发明了一种重要的假设检验技术 —— 独立样本 T 检验。这项技术用于评估不同种类麦芽对啤酒质量的影响。

这里有两个中文术语需要解释：第一个是 T 检验。我们前面已经讨论过，现在我们要对比的是两种不同类型的小麦对啤酒质量的影响。在假设它们都呈正态分布的情况下，我们可以通过抽取一些样本的平均值来代表整体情况。然而，在日常生活中，想要获得完全符合正态分布的数据有一定的难度，通常需要较大的数据量。由于获取大规模数据较为困难，这就促使了这种重要的统计技术的发展。

一个叫做 T 检验的方法不再需要正态分布那么大的样本量，即使较小的样本量也可以满足 T 分布。可以将 T 分布理解为一种尾部较厚的分布形式。当数据量大到一定程度时，T 分布就会变成正态分布。Villon 在这个研究技术基础上提出了一种新的分布方法，并最终形成了假设检验这一重要的统计技术 —— 独立样本 T 检验。

这里我可以给大家举一个通俗的例子：假设现在我们是一名老师，需要比较自己所教的两个班级的学生学习成绩，比如 A 班和 B 班。由于无法获得每个班级所有同学的成绩，这时我们可以从 A 班和 B 班中分别抽取一部分学生的成绩进行对比。以这两个班级的人数为例，如果 A 班有一百人，B 班也有一百人，那么直接从一千人的班级（假设 A 班有一千人，B 班同样有一千人）中收集所有数据是比较费劲的。因此，我们现在从每个班级中各抽取一百名学生的成绩进行对比。

具体来说，我们将 A 班的第一个学生与 B 班的第一个学生的成绩进行比较，然后是第二个、第三个、第四个…… 以此类推。当某个学生的分数高于另一个学生时，我们记一个正子。这一百名学生全部对比完毕后，我们可以大致了解 A 班和 B 班同学成绩的情况。假设每次对比结果都是 A 班显著优于 B 班，那么这时我们就能够说明两个班级之间确实存在显著的差异。这个例子虽然简单，但希望能帮助大家理解。

在实际的日常生活中，我们进行的是独立样本检验。这一方法是在 21 世纪随着计算机科学的发展变得非常复杂的。例如，当我们提到一组有 1000 人和另一组也有 1000 人的数据时，它们可能符合正态分布；但当每组人数减少到 100 人时，这种分布可能会变成 T 分布。无论是 T 分布还是正态分布，这些分布都遵循一定的规律。然而，在日常生活中，我们接触到的大量数据往往不遵循任何特定的分布规律，其随机性更高。

统计学技术的发展最终将这些数据处理分为两类：一类存在明确的分布规律，被称为参数检验；另一类则称为非参数检验。为了应对这两类不同的情况，统计学已经发展出了一系列的方法。例如，在对比独立样本时，我们使用的是独立样本 t 检验；当涉及到更多的变量时，则会采用多元方差分析等方法来评估这些变量之间的显著关系。

这三种统计方法对应于非参数检验的三类情况。相对来说，在我们的日常生活中和科研工作中，使用的非参数检验非常少。可以说，在一万篇科学论文中，可能只有十篇左右会用到非参数检验，这一比例是非常低的。绝大多数情况下，我们使用的数据都是基于正态分布或 T 分布的数据。

因此，这种基于参数假设检验的方法成为了统计学中的主流，并且几乎所有重要的实验科学研究发现都建立在这个基础上。然而，这个基础目前正面临着强烈的挑战。我将在后面的讲解中提到这一点。

这部分内容已经解释了因果归纳的优势和缺点：它能够初步揭示变量之间的关系，并且应用范围非常广泛。但它的主要缺点是容易混淆相关性和因果性，导致错误的结论。

英国的归纳法和我们后面讲到的逻辑有很多内容是一样的，所以我们在后面的讲解中会详细探讨。现在，让我们先来看一下前面提到的评判英国演绎推理的方法。实际上，这是一个关于有效性的讨论。而现在我们要评判的是归纳法，这里的可靠性是指什么？它指的是作为一种方法，这个归纳是否能够具备所谓的可靠性。一般来说，我们可以从以下几个角度来评估，这是一些常见的归纳偏差。

这部分内容我们已经讲过，现在是对前面提到的三种归纳进行详细的总结。请大家注意，最熟悉的莫过于类比归纳了，这是我们日常生活中应用最多的归纳方式之一，尽管它很容易被忽略。类比归纳与我们的整个科学知识体系是紧密相关的。归纳逻辑的优点在于它强调的是科学知识的相关性，在演绎逻辑时期，数学、物理和化学都是从公理系统出发的。特别是数学，在 16 世纪到 17 世纪这段时间里，其发展形式已经基本定型。这之后，形成了规范逻辑的基础。现在，数学已经发展为实验数学、统计数学和应用数学；物理学也有了实验物理、统计物理等分支；同样地，化学也有实验化学和统计化学等领域，这些学科不再仅从纯演绎的角度出发。

21 世纪的绝大多数人类文明成果都是建立在归纳逻辑基础之上的。另一个重要点是，它促进了科学家群体的职业化。例如，1960 年伦敦皇家学会成立，贝叶斯定理就是在该会刊上发表的；同样地，1666 年巴黎科学院成立，这标志着今天我们所说的科学家这一职业群体逐步形成。

归纳逻辑的局限性是非常重要且有趣的问题。当我们理解了归纳逻辑的局限后，才能真正使我们的知识体系迈入 21 世纪。

逻辑的局限性在于其最重要的一个局限被称为归纳之谜。在前面的课程中，我们已经为大家介绍了休谟问题，这个问题在哲学史上还有另一种称呼，即「旧归纳难题」。它指的是什么？休谟问题在他的著作《人性论》中被提出，《人性论》后来取代了某种关于人类本性的传统理论。休谟的问题在于因果判断来源于习惯和经验，他对所谓的归纳推理进行了强烈的质疑。

他质疑的要点有两个方面：第一，归纳概括是非必然的。这意味着即使我们每天观察到太阳升起，并不能保证明天太阳也会升起。假设宇宙即将终结，在时间序列中的 N+1 时刻，小太阳并不一定再次升起。因此，休谟认为归纳概括的过程注定是非必然的，即结论不一定正确。即使像太阳天天升起这样的现象已被无数次观察，我们也无法绝对确定明天太阳还会升起。

第二，休谟质疑的是归纳预测是循环论证的问题。我们如何证明归纳推理的有效性？我们依赖于过去的成功经验，比如过去每天都能看到太阳升起，因此我们认为未来太阳也会每天升起。然而，这种依赖本身就是一种归纳推理，从而形成了一个循环论证的困境。这相当于用一张纸去证明另一张纸是否准确。

回到图表上来看会更清楚：我们可以看到，在观察到太阳每天都升起的情况下，当时间序列中的 N+1 时刻到来时，并不是一个必然的结果。我们进行规则概括时，实际上是假设了所有情况下太阳都是必然会升起的。但这种假设本身是一种全称命题，而我们在推理过程中实际上就是在用归纳方法去证明归纳方法的有效性。

天依然会照常升起。通过金属的例子，我们可以发现类似的问题：当金属加热后都会膨胀。在我们进行预测时，如果发现一种新的金属，它也会遵循这一规律而膨胀。然而，这种论证的循环是如何形成的呢？假设你发现了一种新的金属，它不会因为温度升高而膨胀，那么科学家就不会将它归类为金属了。大家明白了吗？这就是为什么归纳逻辑存在天然的弊端。这些天然的弊端在 21 世纪成为许多哲学讨论的焦点。这就是休谟问题，即人类像英国注定是一个国家一样，认为归纳这一过程是必然无效的。他反对培根的观点，并试图解决归纳推理中存在的逻辑漏洞。

他认为虽然归纳概括不是必然正确的，但在日常交流中我们依然大量使用归纳推理。例如，当我们没有遇到钢筋时，我们会认为太阳明天还会升起，认为所有金属加热都会膨胀。但是一旦遇到钢筋，这一逻辑就会变得异常复杂。那么为什么这种非必然的归纳概括会存在呢？休谟认为这是因为人类心理的习惯使然。我们都习惯性地认为太阳每天都会升起，所有的金属在受热后都会膨胀。

因此，他在解决这个问题时提出了一个观点：预测未来的能力也是一种基于经验的习惯。我们过去遇到的所有金属加热后都会膨胀，所以我们倾向于认为所有金属加热后都会膨胀。这种习惯是基于经验和观察的积累。休谟在他的这一理论基础上得出了他最著名的一句话：「习惯是人生的最大指导。」大家明白了吗？

休克尔的逻辑是这样的：既然人类每天使用的归纳推理如此不可靠，那么归纳推理就存在两个深层次的问题。一是归纳概括必然是不完全的；二是归纳预测注定是有局限性的。在当时，这种观点被称为「论据循环法」。我们所有人都天天使用归纳推理，我自己也天天这么用，原因是什么？是源自习惯。因此，这既是我们人类心理的习惯，也是我们经验上的习惯。休克尔形成了他最著名的那句话：「习惯就是人生最大的指导。」

现在我们再来看一下休克尔的最重要贡献是什么。他非常敏锐地让我们意识到所有规则在形式上都有局限性，这种局限性在哲学上被称为「不对称」。因此，在 21 世纪也形成了一大流派 —— 不对称哲学。当时，休克尔在破解这个问题时，全部是指向习惯的问题，但他并没有进行更深入的剖析。

在这段学习中，我们遇到了学报派的文章，并且今天写的白板会比较多。今天讲的内容比较复杂，既是哲学上的难题又是统计学上的难题，因此我需要画一些图来讲解得通俗一些。显然，在日常对话中我们会遇到两类归纳推理：一类是典型的，如太阳会升起、金属加热会膨胀；另一类是杠金式的，即有人质疑太阳明天不会升起或金属加热不会膨胀。显然，在我们的日常生活中存在这两类情况：一类是好的归纳，另一类是不好的归纳。

休克尔从形式上指出了所有归纳推理在形式上的局限性，那么内容上哪些归纳可能是好的，哪些可能不好呢？这是他的一个贡献。20 世纪的一位伟大哲学家古德曼对此做出了重要贡献。他在 48 岁时发表了《事实、虚构与预测》这本书。我在 2014 年读过，并在我的许多书中都提到这本书，但直到最近准备课程时才真正搞明白了书中的内容。

古德曼做出了什么样的重要贡献呢？他实际上

贡献者提出了一个概念，称为「绿蓝悖论」，并构建了一个非常巧妙的思想实验。这个思想实验我在 2013 年以文学的方式写过，那时我写的内容比他的更为优美。大家可以看一下他是如何实际构建的。我会讲慢一点，这个例子他采用了一种非常聪明的方法。首先，我们每个人想象一下自己现在有一颗宝石，你每天都在观察它，发现它第一天、第二天、第三天、第四天都是绿色的。这时你会不会形成一个归纳概括：这颗宝石永远是绿色的？大家明白吗？

那么古德诺（Goodman）非常聪明地发明了一个新的英文词：「绿蓝」（grue）。这个词之前在英语中是没有的，是由「green」（绿）和「blue」（蓝）组合而成。他为这个新词下了一个定义：2024 年 8 月 25 日之前观察到它时是绿色的；但到了 2024 年 8 月 25 日之后再观察它，它就变成了蓝色。这种宝石我们称之为「绿蓝」。

大家听懂了吗？好的，那么接下来继续回到我的这颗宝石。显然，我们对这颗宝石做了一个定义：注意这是一个关系性的概念。只要是在今天之前关注到它时是绿色的，我们可以把它定义为「绿蓝」；但从明天开始观察到它是蓝色的，我们也同样可以将其定义为「绿蓝」。这种构建的是一个「或」的关系，而不是「非此即彼」的关系。

显然，我们之前看到的那颗宝石也是「绿蓝」的。这时就形成了一个非常严重的逻辑问题：我们可以形成两个结论。第一个结论是这颗宝石永远是绿色的；第二个结论是这颗宝石是「绿蓝」的。在今天之前我观察到它全是绿色的，符合「绿蓝」这个词的定义。然而，当我们进行归纳概括时，在进行规则推理和预测时，就形成了一个矛盾的结论。

大家发现了吧？这个矛盾的结论是如何形成的呢？第一个结论是我们认为这颗宝石永远是绿色的，因为我们在进行归纳概括时认定它是绿色的。

一个是叫杰现在放了一个词吧，他是这一个叫做绿蓝。绿蓝有一个特点是什么呢？过了这个时间点，到了明天就开始变成蓝色的了。这种说法归到预测是不是过了一段时间后它会变成蓝色？这两个结论是否明显是相互矛盾的呢？大家听懂了吗？

所以这就是相应最终的一本书的内容，这本书来自 20 世纪和 21 世纪，可以说是最终的一本哲学著作，差不多排在第三第四的位置，仅次于维特根斯坦的《哲学研究》。大家听懂了吗？你会发现这是一个很典型的哲学研究。你会发现这代表了一个很典型的哲学家思维方式。当我们谈论维特根斯坦时，我们讨论的是家族相似性的思想实验；同样的，当我们谈到绿蓝这个问题时，明显与我们的生活常识是完全冲突的。

为什么它会形成如此强烈的悖论呢？这时在哲学史上就把它称之为「新归纳之谜」，与休谟的观点相对应。他实际上是将休谟的观点进一步深化了，并提出了一个新的问题：我们究竟是如何确定哪些规则是合理的呢？显然，像这种规划，在一个词被发明之后整个逻辑就出错了。古德曼是如何解答这个问题的呢？你会发现，哲学家在提出一个问题时，他们也会尝试自己去解答这个问题。

古德曼认为这个预设是习惯性的，它遵循了传统因为我们习惯了这个词。例如，「Glu」这个词是一个人为创造的新词，并没有受到我们的语言习惯和生活习惯经验的支持。

好的，接下来古德曼提出了一个新的哲学体系，这个体系被称之为投射哲学。他认为在我们日常生活中广泛使用的、已经被证明有效的归纳推理被称为是「投射」的。大家听懂了吗？所谓的「投射」，意思是指绿色这个词能够从过去投射到今天，再投射到未来。当我们创造了一个新的词 —— 绿蓝的时候，它不具备这种特性。

这有一个投射的能力，所以说它既不能从过去投射到今天也不能投射到未来。所以，在这个时候，它会被时间的河流所阻挡，必然会产生一个悖论。因此他定义了这一点，称之为「投射」。希望大家都听懂了。在此基础上，通过这个日南悖论，我们可以发现归结到这里与之前我们所有人的想法是不同的，实际上是受到了我们使用的词语和概念的影响。当我们把预设这个词换成预览这个词时，这一推理就完全行不通。因此，在此基础上，贵德满形成了更复杂的哲学体系。希望大家都听懂了。

那么，也就是说在这里面，贵德满的新归纳之名促进了类似于前面讲的问题的发展，即促进了 20 世纪的哲学家对贵德满推理的合理性进行更深入的探讨。目前对于这个问题解答相对成功的是哪位哲学家呢？是 1988 年这篇论文中提到的一位哲学家。我们之前介绍了贝耶斯干预作为一种主观干预，实际上贝耶斯干预能够较好地解答这个新归纳之名。首先，日南这个词是人类构造的，没有受到传统支持，因此其先天干预防御度较低。像日常使用的词汇已经广泛使用时，现眼干预的防御度相对较高。

好的，在这个时候我们在计算对于未来的预测市场值。暂时结合现眼干预和市场值计算得出后院干预，最终得出结论：所有干预都是绿色的和所有干预都是绿蓝的，显然前者可能性干预更高。大家听懂了吗？这是从贝叶斯的角度来解答这个归纳之名问题。苏博的方案其实有一定的创新性，通过苏博的解答，我想让大家意识到一个很重要的问题是，新归答之名不仅让我们在形式上认识到归纳推理存在先天局限，同时也让我们从内容上认识到好的归纳和

当一个归纳推理中的某个前提出现错误时，整个论证就显得异常荒谬。这时我们该如何解决这个问题呢？举个例子来说，假设有人提出明年太阳不会升起或金属遇热不会膨胀这样的论点。面对这些极端的反对意见，最好的应对方法其实是贝叶斯定理。我们可以告诉他，在过去数百万年里，太阳每天都会如期升起，这种先验概率非常高；而碰到罕见事件时，我们的先验概率则相对较低。大家理解了吗？因此，运用贝叶斯定理是对付这类问题较为有效的方法。

接下来我们来看归纳推理的第二个重大局限是什么。在古希腊时期，所有的学者从一开始涉足学问就注定成为博学多才之人，不受学科限制。然而，在现代科学体系的发展下，每个人几乎都被无数个专业领域所牵制，需要掌握大量的实验性知识，比如实验心理学、实验物理学和医学等。你会发现我们被大量零散的碎片化知识束缚住了，很难构建出人类知识的完整图景。这是归纳推理一个相对严重的局限。

第三个较大的局限是什么呢？归纳推理，尤其是因果归纳，在面对某些实验时其实是较为受限的。许多涉及伦理道德或个人相关的实验无法进行。此外，我们在处理因果关系时，因果归纳有一个显著的特点：它通常是单线程的，即 A 导致 B 或者 A 导致 C。然而，我们的日常生活经验告诉我们，事物发展的结果往往是多因多果的。

在这个时候，因果归纳法在处理因果关系时，面对众多原因和结果变得非常无力。这是我反复向从事科研的同学强调的一个问题：为什么说今天的实验科学这一认知方式的红利已经变得越来越小了。这是因为在学术界，依靠实验科学基本上很难获得新的成果。从 17 世纪到 21 世纪，我们已经有整整 300 年的实验科学发展史，最易取得成果的时代已经过去。大家听懂了吗？好的。

也就是说，在实验科学中那种将某一原因直接对应为某一结果的结论，在 21 世纪见多识广的人看来越来越缺乏说服力了。大家理解这一点了吗？那么在这个时候，我们就开始需要一个新的逻辑处理系统。对于前面提到的演绎与归纳这两个经典的逻辑系统，我们需要做一个简单的总结。

演绎逻辑的问题在于它面临的普遍前提来自何处。例如，数学推理中的公理从何而来？规纳逻辑（即归纳逻辑）面临的问题是，为何能从具体的事例中总结出一般的规律？这是目前这两个逻辑系统存在的问题。沿着这些问题，最终形成了三个不同的哲学分支：一个是演绎的结论包含在前提中，另一个是关于演绎的普遍前提来自何处的问题，这是我们前面提到过的先验论，最典型的是以康德为代表的先验论。康德的理论将人的普遍先验知识进行了阐述，并且我们之前也讲过，康德的先验论尤其是在范畴上的观点已经得到了认知科学的实证支持，例如人类一出生就具备对空间和时间的感受。

另一个典型的例子是经验论与建议论及其之间的折衷。归纳逻辑的问题现在大家应该也理解了，这相当于休谟问题，即归纳之谜和新归纳之谜。归根结底，这个问题实际上涉及的是...

一个默认的前提是：它已经预示了自然的奇异性，即未来的状态必然源自过去。例如，未来太阳的升起与过去的太阳升起具备相同的奇异性。因此，在这种情况下，你会发现这一自然的奇异性的概念中，未来受制于过去，无论是相似性、绝对性或其他表达方式。这似乎像是一个公理，并且特别像一种演绎逻辑。最终，我们通过这种方法构建了一种新的解法，即将其视为知识或生命的探讨问题。目前，我们的认识是：演绎逻辑的普遍前提源于归纳逻辑，而归纳逻辑的前提又来自演绎逻辑，两者相互交织、相互印证。这使得我们可以解决许多问题。

接下来，我们将讨论这一逻辑阶梯的英国版本。这部分在前面已经相当熟悉了，即英国逻辑与高级逻辑，后者是大家最不熟悉的领域之一。既然我们已经有了归纳逻辑，为什么还要引入英国逻辑？尤其是在我们已经有因果归纳的情况下，为何还需要引入英国逻辑系统呢？这个问题在哲学史上引起了激烈的争议，许多哲学家认为人类只有两种逻辑系统：演绎和归纳，不存在其他类型的逻辑系统。然而，21 世纪以来，越来越多的哲学家开始反对这一观点。

让我们看看 21 世纪的观点是什么。前面我们对因果规则进行了总结：A1 导致 B1，A2 导致 B2，A1 导致 B2，因此可以推断 N+1 会导致 Bn+1。这里出现了一个非常有趣的问题：「导致」这个词代表了一种典型的因果关系，即 A 导致 B。如果我们换一个词

词语 AE 半血 BAI 半血 BN 半血 B，在这种情况下，它显得非常像是一个典型的因果归纳。因此，我们会发现，这时传统的因果归纳变成了某种文字游戏。你只是使用了「导致」这个词，而我用了「伴随」。所以你会发现，无非是相关性和伴随性之间的区别，以及直接的因果关系。

统计学之父卡尔·皮尔逊是现代统计学的创始人和奠基者之一，他从这一角度出发，强烈反对人们去研究任何形式的英国关联。他认为这种英国关联是不可验证的。为此，他发明了一个非常重要的概念，即相关系数。所有接受过高等教育的人几乎都接触过相关系数。我们可以用一个简单的例子来说明这一点。比如，我们前面提到的一百人中，A 组一百人和 B 组一百人之间，我们之前让他们对比谁大谁小。现在，我们将这种比较转化为计算他们之间的相关系数，并将这些数据累积起来，用一个总体的数字表示最终的结果，这就形成了著名的皮尔逊相关系数。

听懂了吗？那么作为现代统计学的重要贡献者，皮尔逊从很多方面反对英国关联的研究，他认为英国关联不可验证。在我们前面讨论中特别提到的一个词是「本质」。你会发现，在早期，弗朗西斯·培根最早探讨的是热的本质。经过三百年的历史发展，最终在整个科学界形成了一种思潮 —— 本质主义。这意味着无论进行什么样的研究，都试图探索情绪、动机和认知等现象的本质。皮尔逊认为这一切都是无稽之谈，他认为这注定会使科学研究停滞不前。因此，他强调英国关联的不可验证性。

刚才从休默的角度看，形式上是不可验证的，内容上则受限于你所使用的语言和概念。稍微再翻译一下这个词吧，在英语中这个关系就完全不同了。显然这样东西就毫无意义了。因此，皮尔逊成为了操作主义的代表人物之一，这意味着我们放弃了本质主义，转而采用操作主义。我们不再追求事物的本质，而是关注第一个操作性的概念。例如，什么是幸福感？当我们定义了幸福感的操作性概念时，就可以对其进行研究了。大家听到了吗？

第二个观点是：为什么要放弃英国呢？因为这个更加客观，并且能够从数据上进行验证。在皮尔逊的时代，从数据上验证这种关系特别困难。后面姚势会讲到当时验证因果关系的方法非常有限，唯一有效的方法就是随机对照实验（RCT）。这是当时唯一能验证因果关系的方法。因此，皮尔逊认为，鉴于他所处的时代背景，即 20 世纪初，人类注定无法通过如此昂贵的成本来验证这种因果关系。最终，这种方法仅在医学领域被用来发明药物时采用，因为在这种情况下，进行高成本的随机对照实验是必要的。

好的，第三个观点是：为什么他认为要放弃英国？皮尔逊认为英国特别容易导致错误和偏见。通过对前面关于因果归纳法的介绍，你会发现的确如此，它很容易受到人的主观影响。在皮尔逊的强大影响力之下，整个统计学界差不多 30 年都放弃了对因果关系的探讨。在这 30 年间，统计学界达成了共识。今天，杨老师在这里教我们的统计学书籍中，无论是 21 世纪任何接受高等教育的人，无论你是心理学、经济学等学科的学生，在所有心理统计学或教育统计学教材中，都不会提到因果关系。

关于因果关系的探讨，并没有唯一确定的定义。例如，皮尔逊的影响一直持续到 21 世纪。为什么他的观点会有如此大的影响？事实上，皮尔逊的观点有一定的道理。我们从一个更宏观的角度来看待这个问题，可以发现今天教科书上所教授的心理统计、教育统计和经济学统计存在一些根本性的局限性。皮尔逊的批评指出了这些统计方法的根本局限性：为什么这些批评是成立的？让我们来看看一个关于英国效应的讨论。

如何确定 X 与 Y 之间的因果关系呢？通常我们会采用一种方法，即给某人服用某种药物，观察这是否能改善他的健康状况。这时会出现一个非常经典的问题 —— 辛普森悖论。这个问题指的是这样一个情况：一类患者服用了某种药物后身体好转了，但没有考虑到另一类未服用该药物的患者的健康状况是否会自然好转。这一点非常重要。

在处理这一问题时，传统统计学会采取以下方法：将其他因素视为混杂变量，例如人口学变量（性别、年龄等）。通过消除这些混杂变量的影响来寻找因果效应。然后开始进行实验，选取一批病人分为两组 —— 一组服用药物（治疗组），另一组不服用药物（对照组）。这时，治疗组可能会出现各种各样的情况，对照组也可能有不同表现。

这一图示非常关键，因为它揭示了一个被忽略的重要情况：假设不吃药的患者会如何？大家是否注意到了这一点？

在传统的统计学中，对这一类情况的处理更多是将其视为群体层面的因果效应。例如，在一篇关于治疗抑郁症的研究论文中，通过这种全面的统计方法得出的结论是：该疗法从群体层面上看是对抑郁症有效的。然而，具体到个体层面时，如果某个人没有接受这种疗法，他的抑郁症是否会改善，这一问题在全统统计学处理上是非常无力的。

当年我在心理学系就读时，学到的心理测量和心理统计学教育都是这样的观点：任何心理学结论都是概率式的，并且有一个置信区间，比如 0.95 的置信区间或 0.99 的置信区间。当时的统计老师教导我们相信：任何心理学结论都是群体层面上有效的，但具体到个体层面则不一定适用。

这种处理方法会导致一个复杂的问题：在现实生活中，我们无法让一个人既同时接受某种治疗又不接受它。因此，从这个角度出发，大家应该明白群体有效与个体有效是两回事。这也是为什么在心理学科普文章的评论区总会出现这样的言论 ——「你们谈论的是群体有效性，但通过我的个案我发现这种方法对我个人是有效的。」

传统上，我们很难处理这种情况。面对这种争论，如果未经受过心理学或科学训练的人可能会觉得无法辨别真伪。然而，在 21 世纪，科学发展已经更好地解决了这个问题。

从这个角度出发，现实世界中确实无法让一个人同时存在接受和不接受治疗的状态。小皮亚生认为只有消防没有英国的说法虽显得极端，但也揭示了一定的道理。这种反对声音实际上也推动了科学的发展。

那么问题难道真的无法解决吗？难道英国关系仅仅是一种语言游戏吗？最终，我们今天所介绍的所有统计学教育，是否都变成了这样的认识：即因果效应的证明只能从群体层面进行。例如，当我们要证明一种药物的有效性并达到上市标准时，会发现说明书中留出了一些容错空间。这似乎表明因果效应是在群体层面上被见证的。

我们来看一下与皮尔森同时代的这批学者是如何逐步迭代这一概念的。首先，他们将实验设计演变为随机版本，逐渐形成了所谓的「随机对照组实验」，也称为 ICT（Individual Comparison Trial）。这是目前新药研发的标准流程：任何药物要达到上市标准都必须通过随机对照组实验。这种实验主要是为了处理群体中的个体差异。

这是一个最古老的随机对照组实验。然而，它存在一些典型的问题，如人与人之间的大量行为互动等。因此，在此基础上形成了所谓的「盲法实验」。我们发现，通过二十世纪的进展，从最初的对照组实验到随机对照组实验再到盲法实验，每一个新的版本都是对前一个版本的改进。在盲法实验中，参与实验的人和主持实验的人都不知道自己属于哪个组。

这一点很重要，希望大家理解了。然而，这里有几个很大的问题：成本非常高，以至于药物研发动辄需要十亿美元；而随机对照组实验通常只能应用于非常昂贵的药物研发项目。在经济学、心理学等领域，真正的随机对照组实验其实并不多见。

关于伦理道德的问题，比如为什么不让这批人吃药，而非让另一批人吃药。这提出了一个明显的问题：大家是否理解了这个在历史上的实验中出现的重大问题？除了 ICT 学籍对照组之外的这些因果效应，真的没有办法验证吗？

对此，许多哲学家和统计学家都表示质疑。皮尔森先生是古典英国逻辑的提出者之一，1878 年他总结出了三大逻辑系统：演绎逻辑、归纳逻辑和英国逻辑。他认为归纳逻辑与英国逻辑是可以扩展的逻辑部分。

这也是皮尔森最著名的演讲内容之一，被收录在一本供同学们参考的书中。基于皮尔森的技术以及更多哲学家的工作，形成了真正的因果逻辑。我们可以通过一个简单的例子来总结这一点：从特定的结论到普遍性的命题，即从个别案例推及一般规律。而归纳逻辑是从具体到一般；英国逻辑则有所不同，它是从结果追溯到原因。

早在研究英国逻辑时，皮尔森更多地是从逻辑学和哲学的角度进行探讨，并未深入形式化的工作。因此，他提出的英国逻辑也被称作树英逻辑或回数推理。此外，还有一种称为「聚加可解释论证」的方法，当一个结果发生时，我们会寻找多种解释（如解释一、解释二、解释三），最终发现其中某个解释能够作为最合理的解释被接受，这被认为是树英逻辑的一种表达形式。

同时，在另一个流派中，他们与皮尔森的观点有所不同。

哲学与逻辑学的不同在于它们分别针对的是皮尔逊的相关性以及因果效应的强烈论证。最终，从数学、统计学及哲学的角度成功证明了即使在非随机对照实验以外，我们也能对因果效应进行验证和数学计算。因此，这一流派通常被称为英国推理或英国推断。在本书中，我们将这两个流派统一称为英国逻辑。皮尔森的例子展示了如何找到最佳解释，即最可能的原因。

当我们回顾前面提到的哥德尔关于逻辑系统的不完全性定理时，会发现它对英国逻辑系统的影响是九折。因为英语中的「nine」（九）与「nearly」（几乎）发音相似，这里用来形象地表达「几乎完全」。这意味着如果理解了这一点，就更容易明白 PS4 等人观点中所谓的瑕疵：尽管他们努力寻找最佳解释，但这些解释往往是在事情发生之后才提出的。实际上，更为重要的是反事实推理 —— 即在特定情况下未发生的情况。

现代英国逻辑争论的最终发现是比事实本身更重要的往往是反事实情况。例如，在考虑一个人是否因为服用药物而缓解了抑郁症时，更重要的是思考如果这个人没有服药会怎样。这一反思过程的重要性不容忽视，它不仅在儿童心理学中被称为「心理理论」，也在行动心理学中被称作「执行功能」。由此可见，人类的这种反思能力至关重要。

整个现代英国逻辑体系都是建立在反思史的基础上。最早的描述可以追溯到休谟对归纳法的质疑。然而，真正的定义和突破发生在 1973 年，这一年刘易斯等哲学家的研究成果标志着一个重要转折点。

接下来我们说的另一位刘一诗与前面提到的不同。这位刘一诗是模型逻辑（又称模态逻辑）的创始人，并且是对该领域最重要的贡献者之一。他在 1973 年，32 岁的时候发表了一本书叫做《反思条件句》。现在让我们来了解一下这本书的内容。其实很简单，《反思条件句》中讨论的是反事实条件句，例如：「如果昨天买了那张彩票，我今天就是百万富翁了。」显然我没有买那张彩票，所以今天我不是百万富翁。这一点大家应该都很明白。

我们前面提到了模态逻辑的另外一位创始人也叫刘一诗。但是大家要注意，这两位刘一诗之间并没有血缘关系。这位在模型逻辑方面做出重要贡献的刘一诗，在其中形成了一个非常重要的观念 —— 可能事件。我们在之前讨论演绎逻辑时提到过可能事件，不过现在大多数人都已经忘记了。我们一层层地介绍了演绎逻辑是如何发展的，并且如何逐步形成模型逻辑中的可能事件的概念。

刘一诗进一步发展了这一理论，探索了演绎逻辑的发展过程，并寻找模态逻辑中所说的「可能世界」。他认为存在无数个可能的世界，每个世界都有不同的情况。这些可以理解为平行世界或者是可能世界。显然，有的可能世界与我们现实世界的距离较远，而有些则更近。刘一诗将那些与我们的现实世界最为接近的称为最接近的可能世界。

接下来我们要讨论的是反事实陈述的真实性问题。也就是说，一个假设语句的真假取决于在最接近的可能世界中结果是否真的发生。例如，考虑没有某个处理结果会如何变化：如果现实中我设置了闹钟，最终上班没有迟到；那么，在可能事件之一中，如果没有设置闹钟的情况下会发生什么。

我没有迟到，可能是因为我未设置闹钟。假设这个情境下，迟到可以分为 15 分钟、30 分钟、1 小时和 2 小时几种情况。显然，最不可能的情况是没有设闹钟却迟到了 2 小时；较为合理的是没有设闹钟而迟到了 30 分钟。大家听懂了吗？因此，从这一角度出发，我们讨论个体的行为时，尤其关注 21 世纪英国逻辑中的四个要素。第一个要素通常被称为「单元」。为了保持对基本知识体系的统一，在之前的讲解中，我们都使用了「实体」一词；在这里，我们同样使用「实体」这个词。但请注意，这只是姚二世（此处可能指代特定学派或理论家）所使用的词汇。在英国逻辑中，绝大多数情况下会将其翻译为「单元」。这意味着什么？即我们在处理的可能是班级、年级或是学校等不同的单位。这些不同的单位，在姚二世的自治体中意味着不同的实体对象；而在洋舍（此处可能指另一个学派或理论体系）自治体中，则意味着不同的实体对象。大家听懂了吗？

接下来，我们看看第二个重要的术语 ——「干预」，通常用字母 T 表示。「干预」指的是我们在讨论反事实情况时提到的情境，例如如果我没有设置闹钟、如果我设置了闹钟、如果我吃了药、如果没有吃药等。这些情境实际上都是一种处理或干预的方式。大家听懂了吗？一般而言，「干预」有两种形式：一种是人为的干预，比如研发新药试验中让一个人服药和不让一个人服药；另一种是自然的干预，例如大自然演化过程中形成的某些影响。比如说地理环境的变化，水

这个地点的优点是有山。这里天气比较热，而另一个地方则相对冷一些，这是自然干预的一种形式。第二类常见的分类是什么呢？是直接干预或者在我们日常生活中最常接触到的二次干预，比如是否服用药物等。此外，在科学研究中还有一类也是二次干预的例子，例如学习时间的不同 —— 有的人学习 30 分钟，有的学习 1 小时，还有的学 2 小时。现在如果让一个人以不同的时间长度进行学习，这种一般称为多级干预。大家听懂了吗？

第三个重要的概念是认知变量。这个变量我们在前面介绍随机对照实验时已经提到过，比如小麦的产量等有客观指标的变量。但是大多数涉及因果关系的变量，并没有这样的客观指标，所以这类变量通常被称为潜在变量（latent variable）。也就是说，它们无法直接观察到，例如一个人的幸福感、一些主观感受以及绝大多数心理学变量实际上都是潜在变量。大家听懂了吗？

潜在变量的另一个定义是什么？它是指当每个人接受每种处理时，就会产生每一种可能的结果。在真实的现实世界中进行某种处理所获得的结果，通常被称为事实结果（factual outcome）。而还有其他一些结果，比如在可能的世界中通过不同的处理方式获得的结果，则称为反事实结果（counterfactual outcome）。无论是事实结果还是反事实结果，都构成了一个结果空间，在这个空间里各种各样的结果都是有可能出现的。这就是潜在结果和事实结果的概念。

我们再来看一下变量是什么意思。它指的是与我们的研究相关的一些属性，例如人口学变量、性别变量等。这些不是我们干预处理的对象，但它们会影响到我们的研究结果。这是反思框架（reflection framework）中的一个部分。表三中的概念是上述内容，而表四中的概念是效应。我们之前提到过，像绝大多数心理学研究所处理的，实际上是一个平均因果效应（average causal effect）。它是指

基于一个群体的推理，所以在这种情况下不一定能够获得个体的因果效应。我们从 20 世纪的逻辑学、统计学发展到 21 世纪，现在实际上已经能够更好地获取个体成绩的影响效应了。这称为通过这些宽教（这里可能是指「干预」或「治疗」的误听），我们可以重新反思一下随机对照组实验。你会发现这里面有一个比较大的问题是：我们最终获得的是一个事实结果而无法获得反事实的结果；我们无法知道这一批人如果不吃药，他们的病情会如何发展。大家听懂了吗？也就是说这些被安排服药的实验组的人的情况。这就是传统随机对照组的一个较大问题。

第二个较大的问题是，在无干预的观察环境下，我们的样本往往不是随机分配的。大家明白了吗？它往往是和我们预期的不同，它的这个样本偏差比我们要小得多。大家听懂了吗？用反事实的框架来看，今天的这些统计学教材中存在的问题就容易显现出来。而因果推理现在能够很好地解决这个问题。大家可以看见它们是如何基于反事实进行因果推断的思路形成了两个大的流派：第一个流派称为潜在结果框架；第二个流派称为结构方程模型。这两个大流派各有优缺点，相对来说后一个流派较早一些，后来的流派在前者的基础上解决了它解决不了的问题。

大家就明白了这两个流派的根本区别是什么了？根本的区别在于这个流派将因果关系表达为这种结构方程。结构方程是什么呢？大家可以简单理解为我们初中、高中、大学都学过的回归方程，实际上可以将其视为结构方程的一种特殊情况。但结构方程能包含更多类型的变量以及更复杂的关系。应该这么说吧，就像我们学过的那种非线性方程一般都会认为是结构方程中的一种特例。这种关系大家听懂了么？那么这个结构因果模型它就是...

什么呢？这是把英国的关系表示为英国图而不是结构方程。我们来看一下两个流派是如何进行因果推断的。首先看一下潜在结果框架，这由学者卢宾在 1974 年发表的模型中提出。另一个重要的成果也是他在 1983 年发表的一个模型。杨潇飞提到的是什么意思呢？他其实是从反思这一思路出发，针对观察数据提出了各种各样的方法。我们可以看一下他的这篇论文。这是这边的部分，快速过一下。好的，这是传统的我们学过的一元回归方程、多元回归方程，他是用这些来表示效应的。那么我们重点来看一下卢宾他们这批人是如何估算因果效应的，他们进行了哪些重要的创新。最终，卢宾提出了这五个技术。这五个技术具体是如何操作的呢？各位同学今天肯定是找不到答案了，但大家一定要明白它们各自解决了什么问题，未来你能在这个方向上解决什么问题，并且可以邀请更多的人进行协作。

接下来我会介绍一些统计方法。首先我们看下第一类是叫自然实验。自然实验是什么意思呢？它是这样的一个概念：由罗伯特·卢宾在 1989 年获得的诺贝尔经济学奖中提出的。我们都熟悉随机对照组试验，这在医学新药研发领域要求非常高，通常需要投资 10 亿美金才能完成。但是像我们在经济学研究、心理学研究等领域很难做到随机对照实验的设计。因此，传统的经济学实际上与实验设计的关系较远。

诺贝尔经济学奖得主提出了一种新的实验技术。他认为，这个世界已经天然包含了真正的随机性。既然如此，我们为什么不从这种真正的随机性出发呢？然后去获得因果效应的证据。举个例子来说，像我们的基因就具备天然的随机性。他从这个角度出发，最终形成了一种重要的研究流派 —— 自然实验。

自然实验指的是利用自然界中会发生的随机分配事件作为研究因果效应的工具。我们可以通过对比图表来更好地理解这一点。第一个图展示的是明科的研究，这种研究偏向于自然观察。与此相对，还有另一种研究方法是统计学较差的新要研发的实验设计。除了这两种实验之外，还有一种对照实验。除此之外，还有一类被称为自然实验的方法，它既不同于学生收集数据的方式，也不同于随机对照实验。它是利用实验本身已经具备的随机性来构建研究。

大家听懂了吗？这里面我们可以看一些具体的例子。刚才提到基因的例子，兄弟姐妹之间的基因就是天然具有这种随机性的实例之一。这是一个自然实验的设计案例。自然实验设计在当前的研究领域中大约是在近十年开始兴起的。请注意，今天我们讲的所有关于英国推断的新知识都是近十年才成为流行假设的。如果你在学术界发表论文时使用这些新思路，现在研究竞争尚不激烈；而竞争更为激烈的仍然是传统的实验方法。

那么，自然实验对我们的一个启发是什么呢？我们发现它的逻辑规则实际上就是寻找自然界中天然具备的随机性。在这种情况下，在日常生活中我们需要思考哪些东西是天然具有随机性的，并利用它们来构建研究。

接下来我们再看第二个技术叫做倾向得分匹配。我们在刚刚谈到匹配的时候，会发现，在进行随机对照组实验时，实际上是在进行匹配，尽量减少干扰变量的影响。例如，在设计实验时，我们会天然地排除一些可能的干扰变量，比如年龄过大的人不会让他参加信息研发；有很严重的某一种基础疾病的人也不会让他参与。大家听懂了吗？这是传统的匹配方法。这些是传统的匹配，那么在传统匹配中比较典型的叫做分层匹配。

这些传统的匹配获得的一个最严重的问题是什么呢？即所谓的维度诅咒。例如，我们现在要保证性别差不多、年龄差不多，还要保证家庭收入差不多。一般来说，三四个变量这类人口学变量加上研究中的社会经济地位（SES），它通常会包含三四到七个变量。在研究设计中，这三四个变量是作为最常见的匹配考虑因素，确保性别、年龄还有家庭收入等大致上相同。

但是大家发现，这会出现一个严重的问题：当我们进行分层时，比如第一层总共有 1000 人，先按某个标准分成两组各 500 人，再按性别继续分每组 250 人，分很多层之后，我们会超出人类认知的负荷。同时，随着层次的增加，数据量却越来越小。例如，从 1000 人开始，最后能用的数据可能会变得非常少。此时进行匹配就会非常困难。

以陈启腾最熟悉的例子 —— 炒股为例，你会发现涉及的变量非常复杂。仅研究一个公司的竞争力，可能就已经涉及到 30 个以上的变量。

在处理变量时，例如进行股票交易的任意一种探讨，需要确保在匹配过程中尽量维持统一类型，这实际上是非常困难的。因此，在实际操作中，我们处理股票交易的方法通常比较直接和粗暴。其中一种方法是三到七个辩论就差不多了。

这一现象被称为「维度的诅咒」。倾向得分匹配法就是为了解决这个问题而设计的。这个方法是什么意思呢？它有一个数学公式表示其含义。该方法发明于 1983 年，例如我现在在比较一个新开发的量化交易模型对股市获利的影响。这些公司的不同情况都成为了干涨变量，而我们却需要进行匹配。比如，公司的规模、员工人数、社会声誉等，这些因素会变得无穷无尽。

倾向得分匹配通过协方处理将多维度问题进行降维，形成一个分数，即倾向得分。假设中国股市上有 5000 家公司，每家公司都通过倾向得分获得一个数值。例如，A 公司（用数字 1 表示）、B 公司（2）、C 公司（3）等分别获得了 0.9、0.8、0.7 等不同的倾向得分值。

如果我要进行某一类研究，比如考察我的算法的某一方面，这应该被称作实验研究。在初始选择时，我会选取 A 组和 B 组，确保它们的倾向得分相近。这就解决了许多其他方法解决不了的问题。

这一方法是由和路宾在 1983 年共同发明的。这种方法自 1983 年发明之后，在传统的实验研究中，处理因果效应中的干扰变量变得非常容易了。通过倾向得分来分配，当最终的倾向得分接近时，即这些个体类型相似时，验证这种因果效应就变得更加容易了。大家听懂了吗？

这里给大家展示的是由德文两位提出者的具体做法。这是他们最重要的论文之一，发表于 1983 年，其中包含了计算倾向得分的一个公式。这个公式的目的是将一个多维效量转化为一个倾向得分。大家听懂了吗？这是他们的计算步骤：匹配是通过统计技术进行的，通常使用的是罗杰斯回归来获得分数，然后开始匹配。常用的匹配方法有三种，如精确匹配、卡斯匹配等统计方法。大家不需要过多关注这些具体的统计方法。匹配完成后，会再次运用这些统计方法检查匹配是否成功，然后再进行匹配后的计算。这时的计算与传统的假设检验计算相似，即在某个置信区间（0.95 或 0.99）内判断差异是否显著。

以上是前面提到的三个步骤。接下来我举一个具体的心理学论文例子。这是一篇中国发表的心理学论文，研究的是独生子女和非独生子女之间的情绪适应是否存在差异的问题。大家会发现，在这个问题中存在很多干扰变量，如性别、城乡背景、家庭类型、教育水平以及父母的职业等。在心理学领域，处理超过三个以上的干扰变量特别困难。那么，通过倾向得分是如何处理这些变量的呢？首先是对所有这些变量进行一一考虑。

我们计算之后，可以获得一个倾向得分的值。这个值大家可以看见是一个具体的数值。估计完毕后获得该值，接下来我们要做什么呢？大家可以看看这个例子。开始时，我们把样本在匹配前和匹配后进行对比，看独生子女和非独生子女之间是否存在差异，这里使用的检验类似于独立样本 T 检验，用来判断二者之间的差异是否显著。

这个时候已经完成了匹配，即匹配前后的情况。接着，我们将对匹配后的数据进行分析。在这个过程中，我们会用到独立样本 t 检验等方法来检查两组在生活满意度、主观幸福感、自我效能感、孤独感、抑郁和教育这七个方面的差异。计算结果显示，在这些方面没有显著的差异。大家听懂了吗？这个步骤我讲得慢一点，大家可以再回顾一下前面的内容。

首先第一步是这样的：我们先通过这部分计算来确定样本的情况。计算完成后，我们会不断地调整样本间的差异，直到两组数据尽可能接近，形成匹配成功的状态。然后，我们会对这两组进行检验，看看它们在上述提到的七个方面是否存在显著差异。结论是没有发现显著性差异。

相信通过这类思路，许多传统实验科学的研究结论并不一定成立。大家应该已经理解了这一点。

倾向得分的基本逻辑是尽量找到与处理组相似的对照组。传统的寻找方法效率不高，且随机分配的方式往往不够理想。因此，现在有了这种方法作为改进。

接下来再看一下第三种方法，称为工具变量回归。工具变量回归实际上与自然实验紧密相关，这一概念是由一位经济学家提出的。

这项研究是由一位科学家提出的，大约是在 1960 年。断点回归分析的关键在于这一数据，这是经典的研究案例。具体来说，这里要考察的是人在特定情境下的反应。例如，当我们将高考成绩设为 550 分作为录取线时，比该分数线稍低的 549 分和稍高的 551 分的学生，实际上在本质上并无明显差异。如果研究发现这些学生之间存在一定的因果效应，则可以通过断点回归分析来验证这种因果关系。

这是一个非常经典的研究案例，虽然我们不展开详细讨论，但了解其技术细节是重要的：找到断点，并将变量进一步细分。从 20 世纪 90 年代开始，尤其是最近 20 年里，断点回归在经济学领域获得了广泛的应用和发展。可以看到，自 2000 年以来，相关研究的数量呈现出显著的增长趋势。

另一个经典的研究案例结合了自然实验的方法。例如，在以色列的一所学校中，学校规定每个班级的人数上限为 40 人。如果超过这个人数，则需要将班级拆分为两个班。在这种情况下，断点回归分析就以 40 人为界限，来探讨这一政策的实际影响。研究的结论可以帮助我们更深刻地理解其核心原理。

在特定日期附近比较结构的变化，例如以色列社会的这个案例中，到底是 40 还是 39 或 41。当观察到这一变化时，发现 39 与 41 之间没有发生任何显著变化。这引发了一个疑问，即所谓的英国效应是否确实存在。当时，39 和 41 之间的变化表明了如果存在干预，这种效应实际上可能被放大。我们需要考察的是这个变量 —— 誉子（或称为关键点）在这些特定值上是否有明显的变化。当一个端点与其相邻数值之间的变化显著时，这意味着该变量非常重要。

以高考为例来说明这一点更为直观：如果你的分数达到了一本线以上，即使只差一分未能达到这一分数线，就意味着你没有考上一本院校。同样的逻辑可以应用于 985 高校乃至更极端的例子 —— 如北京大学和清华大学的录取分数线设为 660 分。低于 660 分的考生与高于 660 分的考生之间存在一个明显的断点。当以这个分数作为断点进行分析时，可以明显看出，达到或超过北大、清华分数线的学生在未来的收入等方面可能会有显著的不同。这种断点回归方法经常被用来检验某些特定效应是否存在，例如，很多时候所谓的英国效应可能并不存在。

断点回归的逻辑在于它能够帮助我们识别在某些情况下，由于达到了一个极限值（如录取分数线），所导致的结果变化是否真正存在。这些结果的变化可能是非常显著的，而这种显著性往往能帮我们验证或推翻一些假设。例如，在考虑北京大学和清华大学的分数线时，断点回归方法可以有效地揭示达到这一分数标准对个人未来收入的影响。

接下来介绍第四种技术 —— 松松擦分（应为「松配分」或具体术语待定）。这项技术在逻辑学上的应用较为复杂，简单来说，它是在统计学中处理数据的一种方法。例如，在之前提到的药物试验中，A 组服用药物而 B 组作为对照组不服用药物。通过这种方法，我们可以观察到服用药物后的效果，即 A 组与 B 组之间可能存在的差异。

在获得一些初步的结果后，最终需要进行假设检验。那么双重差分是什么意思呢？双重差分实际上引入了一种比较方法，即对同一批人在不同时间点进行两次观测。例如，我们现在有一批人服用药物，另一批人不服用药物，在实验结束后会得到一定的结果。几年之后，我们再次进行同样的实验，这时我们会获得另一组数据。传统的假设检验通常是针对一次差分检验的，而双重差分则是对同一群体在不同时间点上的两次差分检验。最终，这些数据会被整理成类似于这样的图表，通过第一次和第二次的数据对比，大家可以理解吗？

在这种情况下，实际上是从传统的单次差分检验扩展到了双重差分检验。这种设计能够隐约揭示出所谓的「英国效应」，即对反事实情况的推测能力得到了提升。例如，在没有服用药物的情况下，实验对象是否会好转？大家听懂了吗？这一问题正是双重差分所解决的主要问题。

双重差分这一概念是由某些学者提出的，这是世界上最经典的自然实验之一，也是最典型的双重差分设计。这个例子在科学史上非常著名，希望大家可以继续查阅相关资料。1849 年和 1854 年的两次实验是最早的双重差分实例，这就是其原始图表。通过这些数据的对比，我们可以更清晰地看到英国效应的估计结果，包括第一次差分和第二次差分。

对于传统的随机对照试验来说，它们通常只能处理群体效应，而对个体效应的处理不够精确。双重差分的优势在于它更具说服力，能够更准确地反映干预前后的变化。这是一个非常经典的研究案例，也是 2018 年诺贝尔经济学奖得主的重要研究之一。这些都是非常著名的研究，希望这些内容大家都能理解。

由于涉及的内容较为复杂，如果有什么不明白的地方，大家可以再仔细思考一下。

好的，那么我们来讨论双重差分和合成控制方法。其实对我们来说最核心的意义在于发现因果效应，我们要推断一个事情是否存在确实的因果关系。为了做到这一点，我们尽量将其引入时间变量。这部分内容与前面提到的内容有所不同，主要是从时间角度出发，触发对因果关系的研究。

合成控制方法大家会发现与双重差分相比不是那么重要。双重差分更多是从时间的角度进行分析，而合成控制则是从这个角度触发的。大家可以看，它是从这一角度进行分析的：虽然它将多个因素合成为一组，但从某个特定的角度出发进行触发。这一点大家可能已经有所了解。

这是一项 2003 年的研究，其中提到的方法各有其常见的应用场景。例如，在英国逻辑部分和前面的内容非常不同，这部分特别注重符号、语法和语义的研究，当然这些也很重要，但更重要的是推理的部分。自然实验的推导实际上是在寻找自然界中随机分布的数据，这是经济学中的一个关键点。

在样本选择时，通过倾向得分匹配的方法使样本之间的相似性更高。断点回归分析更多是关注临界值。而在双重差分分析中，我们主要观察实验组和控制组在两个时间点的变化情况。合成控制方法则是在将多个控制变量合成为一个综合的控制变量后进行研究。

潜在结果框架相对来说比较容易直观理解，这些方法不仅巧妙而且成本低。我们知道，在经济学、心理学、社会学和政治学中存在大量的自然实验数据，这些都是自然界随机分布的数据。

基因数据、人的智商等，以及类似高考分数、分班人数和政策规定中的断点，在经济学中也十分普遍。这种现象在计量经济学领域尤为常见，并且近几十年来逐渐形成了一大研究流派，专门针对此类问题进行探讨。

然而，这一方法论存在几个显著的缺点。首先，它要求我们在分析之前明确所有的变量及其因果关系。这意味着我们需要假设某一个原因和某个结果之间存在确定的联系，而这显然依赖于研究者的理论背景知识。这种高度主观性的处理方式在人工智能领域难以应用，这是其一大缺陷。

其次，当涉及的变量数量达到 30 至 100 个以上时，计算复杂度会急剧增加。以我 20 年前为一家世界 500 强公司研究客户满意度为例，当时项目中涉及的变量大约有 30 个左右。计算协方差矩阵已经变得异常复杂，最终结果难以直观理解。这种情况导致很多研究成果无法通过编程形式发表论文，而未复验证的研究又依赖于基金支持，目前绝大多数心理学研究的论文都是这样：几个教授基于假设构建模型，声称 A 变量可能对 B 变量有影响，或在假设计中认为没有影响，但最终这些研究往往缺乏实际意义。

这表明，在现实世界中的复杂关系面前，传统计量经济学的方法实际上难以建立有效的模型。因此，技术上发展出了第二种方法，即权重逻辑。

结构方程模型是由著名的科学家珀尔提出的。珀尔也是一位图灵奖得主。2018 年出版的这本书中提到的结构方程模型，以及相关的英国理论等书籍我都曾推荐过。这部分内容较为复杂，比杰克逊的方法更加难以理解，因此在这里不展开讨论。但非常关键的一点是，当前人工智能领域的最新进展几乎都受到了珀尔思想的影响和制约。所以今天晚上留出一些时间与大家交流一下。现在已经快 11 点 50 了，我们还剩大约 13 个月的时间，总共有 358 个月。在这段时间里，如果能对基本知识有一个全局的了解，则能够更好地意识到技术的发展趋势，不至于被局限在某个特定领域。

提到独立样本检验和方差分析这一层次的内容非常重要。至于如何进行贝叶斯推断计算，目前的技术难度已经大大降低。五年前杨拉斯团队进行贝叶斯效应的研究还非常困难，几乎是无法实现的。但五年后，随着技术的发展，现在这种开源工具已经相当成熟，使得相关计算变得十分容易。因此，利用自然界和社会存在的大量数据可以轻松撰写论文。然而，真正的人工智能突破并不在于结构方程模型那部分，尽管这部分内容目前在经济学和理学领域中占据重要地位。真正的人工智能突破是在布尔逻辑这一块。今天晚上先不讨论这个话题了，大家可以提问。

有位听众问到：平时我在工作中经常使用随机对照实验的方法，想了解未来的理论和模型进展是否可以替代这种方法。这个问题实际上我另一个演讲中有提及，可以看作是未来 30 年的起点。特别提醒大家的是，进行随机对照试验的成本非常高昂。因此，目前 AI 的研究趋势是在于取代传统的全要素研发投入，这一趋势已经变得非常明确。技术上的突破也非常多。希望大家今晚听完后能有更深入的理解。

应是我们用十亿美金可能在日常生活中才能进行的实验，在相应的随机对照组中，可能是 30 次到 100 次。这需要大约 3 年到 10 年的时间，但在计算机计算中，一年内就能完成几十万次这样的模拟实验。因此，这是一个明显的趋势，我特别提醒所有从事科研和高端研究的同学，你们都应该投身于这种基于认知方式的研究。基障摩尼的认知方式直接定义了英国推理与逻辑等新技术。这些技术大家听起来可能都很新，但实际上是最近五年内的成果。普尔在十年前、十五年前就提出了相关观点。当时麻省理工的一位好朋友推荐我阅读这方面的论文，那时计算非常复杂，几乎没有人能解释清楚。然而，在过去的五年里，这一切变得特别简单和容易了。这正是红利时期，但现在如果再去尝试这些方法，时间和成本都很高。因此，随机对照实验的时间成本也相对较高。

现在看这个领域的发展趋势，80 秒内就能完成一个复杂的计算任务，这是个很好的例子。不提那个人了，他说他是对的，这个时候到现在，人工智能的比例甚至是可检测的人工智能，像 OLED 一样已经有所突破。如何将这种可检测的人工智能与 OLED 技术结合起来，现在学习起来还是相当麻烦的。

我们平时处理的是文字类内容，如何训练自己的思维模式呢？大家会发现传统的两大逻辑系统 —— 演绎和归纳，在五千年的历史长河中，已经成为人类文明的一部分故事。但是你会发现像英国逻辑学、网上讲的另一个高级逻辑，并没有成为社会共识。这意味着普通人群对此了解不多。所以今天晚上我会给大家总结，记住一点即可：反四十（这里可能是指某种特定的观点或方法）。记住我们今天下午讲的内容，这将是非常重要的。

思思你就记住了。相应的，这样我们如果把这个现实世界视作一种可能世界，那么其他的可能世界会是怎样的？这些可能世界的数量有多少？你会发现你的认知能力变强了，同时你的思维也会变得更加灵活，不会那么容易受到全论演绎规则的思维局限。在技术上，我们现在这个阶段应该是处于红利期，而不是说体内普通文学位。因为这些工程太新了，随便发表文章、写书或做出成果都是不成熟的东西。教材也来不及编写，一般如果你听一听教材和博士生教材，我会提到对俄罗斯的提及，还有其他什么东西要跟您提，不是真的叫欧巴马（可能是误读），提到对欧巴马的提及还有什么问题现在有问题。

大模型能力测量中如何确定实体？如何定义性能速度？特别是大模型与人类方法类比时，人类有各种群体，但大模型在这方面似乎不适应。整体的范畴和属性其实这个问题很容易解决，并没有那么复杂。因为现在是大模型的时代，这一领域已经加入了很强的新分子，一个是叫大模型，另一个是评测。在评测的时候，实际上就有像之前提到的那种建设性的观念出现，这次我们讨论语言学的支撑其实很难解释。因此，只要有超级多的定义就足够了，无论是学习的超级多的定义变成了人类学位普遍的习惯，比如像是人类学位拥有的词汇试验，如智商、自立等，其实也是一种习惯。颜色也是一样，同样的这些大模型领域也是如此。

这一新的领域最终哪一个理论哪一个词会变得更加普遍？比如说有一个百讲得主，他最近提出一个新的概念，他认为一张是人工智能史上的可学习性是最重要指标。然而显然他的指标并不太平衡。也就是说第一个他的指标很难变成人类社会普遍承认的一种习惯，大家明白吗？其实这里面很微妙。我们人类实际上连绿色蓝色这些颜色也是一种学员的社会建构。而政治世界也不只限于这些事物。大家听懂了吗？所以也是这样，我们早期存出千万要实用主义操作主义，不用纠结这些细节，这会让那些人说清的，你这些哲学师根本失业几千年都不怕是的工钱大你失业几千年这个公司证明要么是英文关。

存在与否取决于实际难题。今天晚上未能解决的几个问题，可以通过强调我们之前所说的对实际干预进行扩展来处理。这一点一听就懂，当然这是一听就能理解的，因为老师讲得比较通俗易懂。实际上，真正看透问题是很困难的，非常困难。目前没有其他问题了，那么今天就先讲到这里。网上我们继续见，还有一百多天后网上再见。大家赶紧去吃饭吧，可以好的。
