## ChipNeMo: Domain-Adapted LLMs for Chip Design

[[2311.00176] ChipNeMo: Domain-Adapted LLMs for Chip Design](file:///Users/Daglas/Library/CloudStorage/Dropbox/zotero/storage/P3DTNQPX/2311.html)

Abstract—ChipNeMo 旨在探索大型语言模型 (LLM) 在工业芯片设计中的应用。我们没有直接部署现成的商业或开源 LLM，而是采用了以下领域自适应技术：自定义标记器、领域自适应连续预训练、带有领域特定指令的监督微调 (SFT) 以及领域自适应检索模型。我们在三个选定的 LLM 芯片设计应用程序上评估了这些方法：工程助理聊天机器人、EDA 脚本生成以及错误总结和分析。我们的研究结果表明，在三个评估的应用程序中，与通用基础模型相比，这些领域自适应技术能够显著提高 LLM 的性能，能够在一系列设计任务中以类似或更好的性能将模型大小缩小 5 倍。我们的研究结果还表明，在我们目前的结果和理想结果之间仍有改进的空间。我们相信，对领域适应性 LLM 方法的进一步研究将有助于在未来缩小这一差距。

I. 介绍

在过去的几十年里，电子设计自动化 (EDA) 算法和工具在芯片设计生产力方面取得了巨大的进步。再加上摩尔定律提供的晶体管密度的指数增长，EDA 使数十亿晶体管的功能丰富的复杂 SoC 设计得以开发。最近，研究人员一直在探索将人工智能应用于 EDA 算法和芯片设计过程的方法，以进一步提高芯片设计生产力 [1][2] [3]。然而，许多涉及与自然语言或编程语言接口的耗时的芯片设计任务仍然没有实现自动化。商业 (ChatGPT、Bard 等) 和开源 (Vicuna [4]、LLaMA2 [5] 等) 大型语言模型 (LLM) 的最新进展为帮助自动化这些与语言相关的芯片设计任务提供了前所未有的机会。事实上，早期的学术研究 [6] [7] [8] 已经探索了 LLM 在生成 RTL 方面的应用，RTL 可以在小型设计模块中执行简单任务，也可以为 EDA 工具生成脚本。

我们相信，LLM 有潜力通过使用生成人工智能来自动化许多与语言相关的芯片设计任务，如代码生成、通过自然语言界面对工程问题的响应、分析和报告生成以及错误分类，从而提高芯片设计生产力。在这项研究中，我们重点关注这三个特定的 LLM 应用程序：GPU ASIC 和架构设计工程师的工程助理聊天机器人，它了解内部硬件设计，能够解释复杂的设计主题；基于 Python 和 Tcl 的两个领域专用工具的 EDA 脚本生成，用于英文指定的 VLSI 时序分析任务；作为内部错误和问题跟踪系统的一部分的错误总结和分析。

尽管在大量互联网数据上训练的通用 LLM 在跨不同领域的生成人工智能任务中表现出非凡的能力 (如 Bubeck 等人 [9] 在中所证明的)，但 BloombergGPT [10] 和 BioMedLLM [11] 等最近的工作表明，特定领域的 LLM 模型在特定领域的任务上可以优于通用模型。在硬件设计领域，[6] [12] 表明对额外 Verilog 数据进行微调的开源 LLM (CodeGen [13]) 可以优于最先进的 OpenAI 模型。以这种方式定制 LLM 还避免了通过 API 向第三方 LLM 发送专有芯片设计数据所带来的安全风险。然而，从头开始为每个域训练特定于域的模型将是非常昂贵的，因为这通常需要数百万 GPU 训练小时。为了经济高效地训练特定领域的模型，我们建议将以下技术结合起来：基础模型的领域自适应预训练 (DAPT)[14] 与适用于领域的标记器，使用通用和特定领域指令的模型对齐，以及使用经训练的适用于域的检索模型的检索增强生成 (RAG)[15]。

如图 1 所示，我们的方法是从一个基本的基础模型开始，然后应用 DAPT，然后是监督微调 (SFT)。DAPT，也称为域内数据的持续预训练，已被证明在生物医学和计算机科学出版物、新闻和评论等领域是有效的。在我们的案例中，我们从专有硬件相关代码 (如软件、RTL、验证测试台等) 和自然语言数据集 (如硬件规范、文档等) 的集合中构建了特定领域的预训练数据集。我们清理并预处理原始数据集，然后用特定领域的数据继续预训练基础模型。我们将生成的模型称为 ChipNeMo 基础模型。DAPT 是在预训练中使用的一小部分代币上完成的，而且便宜得多，只需要几千个 GPU 小时。如第节 V 所述，对于我们的用例，我们发现这种方法比参数有效训练 (PEFT) 技术 (如 LoRA [16]) 更有效。

LLM 标记器将文本转换为用于 LLM 训练的标记序列。领域特定的标记化器通过为领域特定的术语 (如 RTL 中常见的关键字) 定制规则和模式来提高标记化效率。对于 DAPT，我们不能从头开始重新训练一个新的特定于域的标记化器，因为这会使基础模型无效。我们没有将 ChipNeMo 限制为基础模型使用的预先训练的通用标记器，而是将预先训练的标记器调整为我们的芯片设计数据集，只为特定领域的术语添加新的标记。

ChipNeMo 基础模型是需要监督微调 (SFT) 以适应聊

天等任务的完成模型。我们使用大量公开的通用聊天指令数据集进行多回合聊天，同时使用少量特定领域的指令数据集在 ChipNeMo 基础模型上执行 SFT，该模型产生了 ChipNeMo 聊天模型。我们观察到，具有通用聊天指令数据集的 SFT 足以将 ChipNeMo 基础模型与芯片设计领域中的查询对齐。我们还添加了少量特定于任务的 SFT 指令数据，这进一步改进了对齐。我们基于用作基础基础模型的 LLaMA2 模型的变体，训练了多个 ChipNeMo Foundation 和 Chat 模型。

为了提高工程助理聊天机器人应用程序的性能，我们还利用了检索增强生成 (RAG)。RAG 是一种开放式方法，用于为 LLM 提供用户查询的精确上下文。它从其数据存储中检索相关的领域内知识，以增强给定用户查询的响应生成。这种方法在将模型建立在特定问题的上下文中方面显示出显著的改进。至关重要的是，我们观察到，当用领域数据微调预先训练的检索模型时，检索命中率显著提高。这导致了模型质量的进一步提高。

我们强调了以下与 LLM 适应芯片设计领域相关的贡献和发现:

·我们在三个用例上展示了适用于领域的 LLM 的有效性：工程助理聊天机器人、EDA 工具脚本生成以及错误总结和分析。我们在基于专家评估的工程助理聊天机器人响应中获得了 7.4 分 (满分 10 分)，在 EDA 脚本生成中获得了 50% 以上的正确性，在总结和任务识别任务中获得了 4 至 5 分 (满分 7 分) 的专家评估评分。

·适用于领域的 ChipNeMo 模型显著优于在多项选择领域特定 AutoEval 基准测试和应用程序人工评估上评估的所有普通 LLM。

·对于模型可以从提示上下文生成文本的任务 (例如，与 RAG 点击聊天、摘要、使用所提供的文档生成代码)，领域自适应缩小了最先进的 LLaMA2 70B 模型和小得多的 13B 模型之间的差距 (小的增量训练成本可以使参数减少 5 倍，从而降低推理成本)。

·定制的令牌化器可将 DAPT 令牌数量减少多达 3.3%，而不会影响应用程序的有效性。

·在工程助理聊天机器人、EDA 脚本生成以及错误总结和分析方面，附加 1.1K 领域特定指令上的 SFT 显著提高了应用程序的熟练度，分别提高了 0.33 分 (满分 10 分)、18% 正确性和 0.79 分 (满分 7 分)。

·使用特定领域的数据微调我们的 ChipNeMo 检索模型，与预先训练的最先进的检索器相比，检索器命中率提高了 30%，从而提高了 RAG 响应的整体质量。

本文的组织结构如下。第节 II 介绍了我们的数据集和用于领域知识验证的自动评估基准。本节 III 概述了所使用的领域自适应和训练方法，包括自适应标记器、DAPT、 SFT 和 RAG。第节 IV 提供了每个应用程序和实验设置的详细信息。第节 V 介绍了实验结果，包括每个应用程序的人工评估。第节 VI 讨论了 ChipNeMo 的局限性和未来的工作。第节 VII 描述了相关的 LLM 方法以及针对芯片设计的 LLM 的其他工作。最后，完整的结果以及附加的模型训练细节和应用程序用例生成的文本示例在附录中进行了说明。

II. 数据集

A. DAPT 数据集

在领域自适应预训练 (DAPT) 过程中，我们从 NVIDIA 专有芯片设计特定数据源和公开可用数据集的组合中汇编数据集。

芯片设计数据集：我们的内部数据集包括与芯片设计相关的各种文本源，涵盖设计、验证、基础设施和内部文档。表 I 提供了过滤后收集的数据的细分，以及使用 LLaMA2 令牌化器的相应令牌数量。我们通过收集所有相关的内部数据来构建数据集，然后根据文件扩展名按文件类型进行过滤，并区分机器生成的内容和人工编写的内容。尽管我们对三个特定的用例进行了评估，但我们并没有将数据集具体限制在已知与这些用例相关的来源，因为我们相信结合额外的领域知识会提高性能。经过收集、清理和过滤，内部数据训练语料库拥有 231 亿个令牌。数据收集过程的更多细节见附录 A。

公共数据集：我们使用来自各种来源的公开可用数据样本来扩充芯片设计特定数据，这是开发基础大型语言模型的常见做法。我们的方法是重用来自其他语言模型的公共训练数据，并规定它必须是可公开访问的，并且与开源兼容。这些数据集与 LLaMA2 [5] 中使用的预训练数据具有高度相关性，目的是在 DAPT 期间保留一般知识和自然语言能力。ChipNeMo 使用的公共数据集可分为两组，自然语言和代码。对于自然语言部分，我们从维基百科的数据 [17] 中提取，因为它因其高数据质量而被广泛认为。对于代码，我们利用 GitHub 数据 [18]，专注于我们内部数据芯片设计数据集中的编程语言，如 C++、Python 和 Verilog。为了确保整个数据集代表预训练分布，我们执行了一个次采样操作，从这些公共数据集中采样了大约 9.2% 的总训练令牌，并平衡地表示了自然语言和代码。

数据混合：我们收集的领域数据中有很大一部分是由来自不同来源的未注释代码组成的。为了增强模型对特定领域知识的理解，我们在 2 到 4 个训练时期内对代码数据进行了下采样，同时对自然语言数据，特别是设计文档进行了上采样。我们还增加了我们认为与下游应用程序更相关的数据的表示，例如人工编写的 EDA 工具脚本。此外，我们合并了 1 个历元的公开可用域数据。表 I 中显示了用于训练的令牌分发的详细信息。

B. SFT 指令数据

在监督微调 (SFT) 过程中，我们使用了可供商业使用的通用聊天 SFT 指令数据集。该数据集主要由公开可用的指令遵循数据集组成，包括 OASST [19]、FLAN [20]、 P3 [21] 和少量的广泛领域专有数据集，包括各种主题，如头脑风暴、开放式问答、重写、总结等。需要注意的是，我们在这里讨论的 SFT 指令数据专注于一般的自然语言任务，不包含任何与芯片设计中的下游用例相关的信息或任务。该数据集总共包括 128000 个训练样本。

此外，我们精心组装了一个特定于领域的指令数据集，用于将模型与下游用例对齐。这些例子是由主题专家精心制作的，格式为单回合问答。表 II 描述了我们的特定领域指令数据集的数量。值得注意的是，与大量的生成聊天指令数据相比，特定领域指令数据集中的训练样本总数非常少。

C. AutoEval

TABLE II: 域 SFT 数据的细分。

为了快速、定量地评估各种模型的准确性，我们为每个用例建立了以多项选择题和答案形式构建的评估标准，旨在与已建立的基准 (如 MMLU [22]) 紧密一致。在制定这些选择题的过程中，与领域专家的合作至关重要。目标是确保每个问题至少包括一个复杂的答案选择，从而对领域专业知识有限的个人构成挑战。我们还仔细注意防止问题被我们特定领域的 SFT 数据无意污染。除了每个用例基准之外，还为一般电路设计知识创建了一个额外的基准，涵盖模拟和数字设计主题。评估基准的多项选择题数量如表 III 所示。

Domain Source

Design Knowledge (Design)

EDA Script Generation (Scripting)

Bug Summarization and Analysis (Bugs) Open Domain Circuit Design (Circuits)

Number of Questions 94

74

70

227

TABLE III: 特定领域的评估基准。

当我们报告上述基准的结果时，我们从五次不同的运行中获得平均结果，以减轻测试过程中方差和噪声的影响。每一次迭代都使用一组 5 个热点的例子，每个单独的运行都会引入变化。

除了这些特定领域的评估基准之外，我们还包括常用的公开 LLM 学术基准。此外，我们通过评估 HumanEval [23] for Python 和 VerilogEval [12] for Verilog 来衡量模型的代码生成能力。

III. ChipNeMo 域自适应方法

ChipNeMo 实现了多域自适应技术，以使 LLM 适应芯片设计域。这些技术包括芯片设计数据的自定义标记器、 使用大型领域数据语料库的领域自适应预训练、使用领域特定任务的监督微调，以及使用微调检索模型的检索增强生成。我们将在本节中详细说明每种技术。

A. 分词器

在调整预先训练的标记化器时，主要目标是提高特定领域数据的标记化效率，保持通用数据集的效率和语言模型性能，并最大限度地减少重新训练 / 微调的工作量。为了实现这一点，我们制定了一个四步方法:

·Step 1Step 1: 使用特定于域的数据从头开始训练令牌化器。

·Step 2Step 2: 从新的标记化器的词汇表中，识别通用标记化器中不存在的、在通用数据集中很少找到的标记。

·Step 3Step 3: 在步骤 2 中使用新标识的令牌扩展通用令牌化器。

·Step 4Step 4: 通过使用通用标记器初始化新标记的嵌入。

特别是对于步骤 4，当遇到新的令牌时，使用预先训练的通用令牌化器对其进行令牌化。新令牌的嵌入是通过对通用令牌生成器 [24] 生成的令牌的嵌入进行平均来确定的，并且输出层权重初始化为零。

步骤 2 通过选择性地引入在通用数据集中很少遇到的新令牌，有助于在通用数据集上保持预训练 LLM 的性能。并且步骤 4 减少了通过由通用令牌生成器引导的新令牌嵌入的初始化来重新训练 / 微调 LLM 所需的工作量。

B. 域自适应预训练

在我们的研究中，我们将 DAPT 应用于预训练的基础模型 LLaMA2 7B/13B。每个 DAPT 模型使用其相应的预训练的基础基础模型的权重进行初始化。我们将 DAPT 型号命名为 ChipNeMo。如第节 III-A 所 [24] 述，我们使用标记器扩充，并相应 [24] 地初始化嵌入权重。我们采用标准的自回归语言建模目标，对特定领域的数据进行进一步的预训练。所有模型训练过程都是使用 NVIDIA NeMo 框架 [25] 进行的，结合了张量并行 [26] 和闪光注意 [27] 等技术以提高效率。

我们的模型采用类似的配置进行一致的训练方案。采用了的 5·10−6 小学习率，并且使用 Adam 优化器来促进训练，而不使用学习率调度器。全局批量大小设置为 256，并且应用 4096 个令牌的上下文窗口，从而产生 1M 个令牌的有效批量大小。附录 B 中提供了详细的训练超参数。训练步骤的总数设置为 23200，相当于数据混合的大约 1 个历元。

图 2 说明了 ChipNeMo 在指定超参数下的训练损失。我们确实观察到了训练失利中的尖峰现象。与中 [28] 的假设相反，我们假设在我们的场景中，这些峰值可归因于「坏数据」，因为这些不规则现象似乎总是出现在同一模型的类似训练步骤中，甚至在不同的模型大小中也是如此。我们选择不解决这个问题，因为这些异常现象似乎并没有显着阻碍后续的训练步骤 (验证损失没有明显下降)，可能是由于我们应用了较低的学习率。

Fig. 2: 通过代币化器增强，ChipNeMo 的训练损失平滑。C. 监督微调

DAPT 之后，我们使用监督微调 (SFT) 进行模型对齐。我们对所有模型采用与 DAPT 相同的超参数训练配置，除了使用 128 的减少的全局批量大小。所有 SFT 数据都是根据以下聊天模板构建的:

<extra_id_0>System\n{system}

<extra_id_1>User\n{user_utterance}

<extra_id_1>Assistant\n{chipnemo_response}

...

我们采用自回归优化目标，实现了一种策略，其中与源自系统和用户提示的令牌相关的损失被掩盖 [5]。这种方法确保在反向传播过程中，我们的重点完全放在优化应答令牌

上。我们将我们的领域 SFT 数据集 (包括大约 1.1k 个样本) 与 128k 个样本的更广泛的通用聊天 SFT 数据集中结合起来。然后，在对数据进行随机洗牌后，我们对单个历元进行微调。我们进行了实验，涉及对特定领域的 SFT 数据集进行多个历元的扩充。然而，很明显，当遇到域内问题时，该模型很快表现出过度拟合的迹象，经常重复域 SFT 数据集中不相关的答案。

此外，我们仅使用通用聊天数据集进行了额外的 SFT，不包括任何特定领域的 SFT 数据。为了清晰起见，我们将所有 ChipNeMo 型号指定如下:

1) ChipNeMo 聊天：通过域和通用聊天数据对模型进行微调；

2) ChipNeMo 聊天 (noDSFT): 专门使用一般聊天数据进行微调的模型。

我们还直接在与聊天相关的模型上进行了 DAPT 实验，例如 LLaMA2 聊天模型。我们发现 DAPT 显著降低了模型的对齐性，使生成的模型对下游任务毫无用处。

D. 检索增强生成

众所周知，LLM 会产生不准确的文本，即所谓的幻觉 [29]。尽管这种现象还没有完全被理解，但我们仍然必须减轻幻觉，因为在工程助理聊天机器人的环境中，幻觉尤其有问题，因为准确性至关重要。我们的建议是利用检索增强生成 (RAG) 方法。RAG 试图从数据库中检索相关段落，并将其与问题一起包含在提示中，这使 LLM 能够得出更准确的答案。我们发现，在 RAG 中使用适用于领域的语言模型可以显著提高特定领域问题的回答质量。此外，我们发现，用适量的领域特定训练数据微调现成的无监督预训练密集检索模型可以显著提高检索精度。我们的适用于领域的 RAG 实现图如图 3 所示。

Fig. 3: RAG 实施变化

我们使用 Tevatron 框架 [30]，通过使用 3000 个特定于领域的自动生成样本对 e5_small_unsuproved 模型 [31] 进行微调，创建了我们的适用于领域的检索模型。样本生成和培训过程见附录 C。

即使微调检索模型带来了显著的收益，但事实仍然是，检索仍然难以处理那些不能直接映射到文档语料库中段落或需要更多段落中不存在的上下文的查询。不幸的是，这些查询也更能代表工程师在实际情况下提出的查询。将检索与适用于领域的语言模型相结合是解决这个问题的一种方法。

IV. LLM 应用程序

我们在设计团队中对潜在的 LLM 应用程序进行了调查，并将其分为四个部分：代码生成、问答、分析和报告以及分类。代码生成是指 LLM 生成设计代码、测试台、断言、 内部工具脚本等。; 问答是指 LLM 回答有关设计、工具、 基础设施等问题。; 分析和报告是指 LLM 分析数据并提供报告；triage 指的是 LLM 在给定日志和报告的情况下帮助调试设计或工具问题。在这项工作中，我们从每个类别中选择了一个关键应用程序进行研究，但分类类别除外，我们将其留作进一步研究。下面给出了每个应用程序的动机和技术细节。

A. 工程助理 Chatbot

该应用程序旨在帮助设计工程师回答他们的架构、设计、验证和构建问题，这可以在不影响其他人生产力的情况下显著提高他们的整体生产力。据观察，设计工程师通常喜欢头脑风暴、设计硬件和编写代码，但在等待他们所缺乏的设计知识的答案时可能会放慢速度。通过避免工程师基于错误的假设编写代码或调试他们不熟悉的代码，也可以提高设计生产力。内部研究表明，典型芯片设计师高达 60% 的时间用于调试或检查表相关任务，涉及一系列主题，包括设计规范、测试台构建、架构定义以及工具或基础设施。这些问题的专家通常分布在跨国公司的全球各地，因此并不总是方便立即找到帮助。因此，基于从内部设计文件、代码、任何记录的设计数据和技术通信 (如电子邮件和公司即时通信) 中提取的知识的工程助理聊天机器人可以帮助显著提高设计生产力。我们使用第节 III-D 中提到的适用于领域的 RAG 方法实现了此应用程序。

Fig.4:LLM 脚本生成器与 EDA 工具的集成

B.EDA 脚本生成

工业芯片设计流程中的另一个常见任务是编写 EDA 脚本来完成各种任务，如设计实现、内省和转换。这些脚本通常同时利用特定于工具的脚本库和自定义的内部脚本库。学习这些库、浏览工具文档以及编写和调试这些脚本可能会占用大量的工程时间。

LLM 已被证明擅长在广泛的任务 32 中进行小规模代码生成，因此定制这些模型以加快该领域特定任务中的工程师生产力是一种自然的选择。在这项工作中，我们专注于从自然语言任务描述中生成两种不同类型的脚本。第一种是利用 Tool1 的脚本，Tool1 是一个用于设计编辑和分析的内部 python 库。第二个是使用 Tool2 提供的命令接口的 Tcl 脚本，Tool2 是领先的工业静态时序分析工具。为了为这项任务构建特定领域的微调数据集，从设计专家那里收集了这两种工具的生产脚本。我们观察到，我们的 DAPT 模型可以为代码生成合理的内联注释。这使我们能够使用这些模型通过生成额外的内联注释来提高收集的脚本的质量。人类专家后来验证并更正了这些评论，并创建了相关提示。这些提示和代码对以第节 III-C 中讨论的格式构成了用于 DSFT 的数据。

为了以最有意义的方式提供和收集反馈，我们花费了大量精力构建图中所示的流程，工程师可以通过同一界面查询模型并运行生成的代码。这使我们能够对生成的代码的正确性充满信心，并通过让工程师了解他们可能需要多少次更正才能获得一个正常运行的脚本来提供准确的反馈。我们通过建立与工具服务器的交互式连接来支持 Tool1 和 Tool2 的集成。

此外，我们还提供了一个用户反馈表，使我们能够比较不同的模型，并从用户反馈中收集有价值的见解。这些宝贵的信息可以帮助我们进一步完善我们的模型。

C. Bug 总结与分析

在生产流程的各个阶段跟踪各种功能和错误的报告、分类、调试和解决是一个耗时的过程。工程经理会花费大量时间审查内部问题跟踪数据库，以建立对项目状态的了解，并帮助加快其执行速度。因此，一个能够查看所有支持信息、快速总结技术和管理数据以及建议下一步行动的工具将提高团队生产力。我们专注于使用 LLM 生成三种不同的输出 —— 一种侧重于技术细节，一种侧重管理细节，以及一种建议任务分配。

为了研究这些任务，我们使用了 NVIDIA 的内部错误数据库 NVBugs。该数据库用于整个公司的错误报告、跟踪和解决，以及一般任务和功能跟踪。我们希望 ChipNeMo

Fig.5:ChipNeMo 代币化器增强进。

模型在这项任务中表现良好，因为 DAPT 数据集中包含了大量的错误数据。此外，我们为该任务构建了一个特定于领域的 SFT 数据集，其中包括错误总结和任务分配任务的示例。

通常，错误描述包含大量的日志文件片段或代码转储，以及长的注释历史记录。在这种情况下，错误文本对于 LLM 上下文窗口来说太大了。为了解决这个问题，我们实施了两个解决方案。首先，我们找到了长路径名，并将其替换为较短的别名，以允许模型在不需要处理整个字符串的情况下，将错误中多个位置出现的路径关联起来。其次，我们将摘要任务拆分为增量任务，其中模型的任务是在多个摘要和 bug 数据块中积累数据。我们使用分层方法，首先将 bug 分离为适合上下文窗口的块。然后对这些块进行汇总，并对汇总进行累积，然后将其分离为块。重复此过程，直到整个摘要集适合单个上下文窗口并生成单个摘要。我们使用相同的方法，独立于用于摘要的 LLM。

V. 评估

我们在本节中评估我们的培训方法和应用程序性能。我们在训练方法评估中同时研究了 7B 和 13B 模型，在应用程序性能评估中仅研究了 13B 模型。为了进行比较，我们还评估了两种基线聊天模型：LLaMA2-13B-chat＊和 LLaMA2-70B-chat。LLaMA2-13B-Cot＊是 LLaMA2 13B 基础模型的基础，该模型使用我们的通用聊天指令数据集进行了微调，与使用来自人类反馈的强化学习（RLHF）训练的原始 LLaMA2-1 3B-Cot 模型不同。我们选择这样做是为了在相同的模型对齐方法下公平地比较领域自适应模型和基础模型。LLaMA2-70B-Chat 是用 RLHF 训练的公开发布的 LLaMA2 聊天模型，被认为是最先进的（SOTA）开源聊天模型。

A. 分词器

我们使用前面概述的四步过程将 LLaMA2 令牌化器（包含 32K 令牌）调整为芯片设计数据集。大约有 9K 个新令牌被添加到 LLaMA2 令牌生成器中。如图 5 所示，经过调整的令牌化器可以在各种芯片设计数据集中将令牌化效率提高 1.6％至 3.3％。我们没有观察到公共数据上的标记化器效率发生明显变化。重要的是，即使在 DAPT 之前，我们也没有观察到在使用自定义增强标记器时，LLM 在公共基准上的准确性显著下降。

B. 域自适应预训练

图 6 显示了芯片设计领域和开放领域学术基准的 AutoEval 基上 ChipNeMo 模型果。我研究结果可以总结如下：

1）DAPT 模型在开放领域学术基准上的准确性略有下

降。

2）DAPT 对域本身内的任务产生了实质性的积极影响。

这种效果表现在内部设计知识以及一般电路设计知识

的显著改进上。

3）使用更大、更高性能的基础模型可以在特定领域的任

务上产生更好的零样本结果。此外，在 DAPT 后，使

用高级基础模型会增强域模型，从而提高域内任务的

性能。

4）域内任务 DAPT 的改进与模型大小呈正相关，较大

的模型在 DAPT 后表现出更显著的领域特定任务性

能增强。

C. 训练消融研究

对于我们的消融研究，我们进行了多轮领域自适应预训练。我们提供了简要摘要，详细信息请参阅附录 B。

使用增强标记器和原始标记器进行训练之间的差异似乎可以忽略不计。因此，我们主要将学术基准的准确性下降归因于领域数据。此外，在包括学术基准在内的大多数任务中，公共数据集的删除仅略有倒退，但 Verilog 编码除外，我们在 Verilog 编码中观察到了明显的差异。这表明，GitHub Verilog 数据的加入有助增强 Verilog 的编码能力，特别是当基础模型在该领域缺乏足够的数据时。

在我们的探索中，我们尝试使用更大的学习率，如 CodeLLaMA［32］。我们观察到，在最初的训练步骤中，训练损失大幅增加。尽管这种方法最终导致了训练和验证损失的改善，但我们注意到，除了编码之外，所有特定领域和学术基准都有显著的退化。我们假设较小的学习率起到了双重作用，促进了通过 DAPT 提取领域知识，同时保持了不偏离基本模型太远的平衡，从而保持了一般的自然语言能力。

我们还探讨了参数有效微调（PEFT）在领域自适应预训练（DAPT）中的应用。在这项研究中，我们进行了两项涉及引入 LoRA 适配器［16］的实验，分别引入了 2640 万（小）和 2.112 亿（大）的附加参数。在这两种情况下，我们的研究结果表明，与全参数 DAPT 方法相比，领域内任务的准确性存在显著差距。此外，当比较小型和大型 PEFT 模型之间的结果时，我们观察到领域内任务准确性的边际提高，大型模型表现出轻微的改善。我们认为，这种现象可能归因于为了容纳大量信息而训练大量参数的必要性，以及 PEFT 模型对灾难性遗忘 33］的易感性。

D. 培训成本

所有型号都使用 128 个 A100GPU 进行了培训。我们估计了与 ChipNeMo 的域自适应预训练相关的成本，如表 IV 所示。值得注意的是，DAPT 在从头开始预训练基础模型的总成本中所占比例不到 1.5％。

Model Size Pretraining DAPT SFT

7B

184，320

2，620

90

13B

368，640

4，940

160

70B

1，720，320

TABLE IV: LLaMA2 模型在 GPU 小时内的培训成本。预培训费用来自［5］。

E. RAG 和工程助理 Chatbot

我们创建了一个基准来评估设计聊天辅助的性能，它使用 RAG 方法。该基准测试包括三类 88 个问题：体系结构 / 设计 / 验证规范（规范）、测试台回归文档（测试台）和构建基础设施文档（构建）。对于每个问题，我们都会指定黄金答案以及设计文件中包含答案相关知识的段落。这些问题由设计人员根据一组设计文档手动创建，作为检索的数据存储。它包括大约 1.8K 份文件，这些文件被分割成 67K 段，每段约 512 个字符。

首先，我们将我们的领域自适应检索模型与句子转换器［34］和 e5_small_unsuproved ［31］在每个类别上进行了比较。每个模型从数据存储中获取其前 8 个段落。

如图 7 所示，我们的领域自适应模型的性能比原始的 e5_ small_ unsuproved 模型好 2 倍，比句子转换器好 30％。Specs 类别中的查询直接来源于文档中的段落，因此它们的答案通常很好地包含在简洁的段落中，并清楚地解决了查询问题。另一方面，Testbench 和 Build 类别的查询并不是直接从段落中得出的，因此它们的答案在提取的段落中往往不那么明显，需要更多的上下文（详细示例请参

Fig.8: 不同模型的人类评价。仅模型表示没有 RAG 的结果。

RAG（命中）/（未命中）仅包括检索到的段落命中 / 未命中其理

想上下文的问题，RAG（总计）包括所有问题。

我们提出了以下意见：

RAG 著高分。RAGLLaMA2-13B- Chat＊、ChipNeMo-13B-Chat LLaMA2-70B-Chat 的得分分别提高 3.82、2.19 和 5.05。请注意，即使 RAG 未命中，分数通常也会更高，尤其是在 LLaMA2 模型上。我们假设额外的域内上下文有助于提高性能。

ChipNeMo-13B-Chat 模 RAG 评优于类似尺寸的 LLaMA2-13B-Chat＊2.88 和 1.25。·带有 RAG 的 ChipNeMo-13B-Chat 获与 RAG 的 5 倍大型号 LLaMA2-70B-Chat 相同的分数（7.4），其中 LLaMA2-70B-Chat 在提取命中答案方面做得更好；然而，领域自适应弥补了它的失误。·域 SFT 有助于将 ChipNeMo-13B-Chat 的性能提高 0.28（有 RAG）和 0.33（没有 RAG）。

所有模型的完整评估结果如附录 D 所示。

F.EDA 脚本生成

为了在 EDA 脚本生成任务上评估我们的模型，我们创

建了两种不同类型的基准测试。第一个是一组「简单」和

「中等」难度任务（1-4 行解决方案），可以在没有人为干

预的情况下通过与黄金反应进行比较来评估。由于构建和评估这些基准测试所需的工作，我们只有用于 Python 任

务的评估集。第二组任务（「困难」）来自我们的工程师选

择的真实用例场景。这些任务要困难得多，需要 10 行才

能解决。因为这些很难自动评估，我们让人类工程师判断

0％到 100％之间的正确性。表 V 中描述了这些基准的规

模。目前正在努力扩大这些基准的规模和范围，使我们能

够进一步改进这些模型。

我们发现，我们的模型无法回答我们的一些更艰巨的任

务。这些任务需要了解许多工具 API，而模型似乎无法在

保持控制流正确组织的同时决定正确的 API。为了缓解这

种情况，我们在提示中添加了一个人工策划的上下文，具

体针对每个问题。此上下文包含正确编写所需脚本所需的

不同函数或属性的说明。我们只为「上下文困难」基准类

别提供了这一点。这也使我们能够研究基于检索的解决方

案的可能效果，我们将其留给未来的工作。

从图 9 中的消融结果可以看出，DAPT 和领域 SFT 对

我们的问题都很重要。如果没有 DAPT，该模型对底层

API 几乎没有了解，并且在自动评估的基准测试中表现不

佳。域 SFT 进一步改进了结果。我们相信这是因为我们

的领域 SFT 数据有助于指导模型以最直接适用的方式呈

现最终脚本。

一个有趣的结果是 LLaMA2-70B 在「上下文困难」基准

测试中的通过率。它的性能比 Python 工具上的大多数模

型都要好，但在 Tcl 工具上却很差。这可能是因为当提供

了正确的上下文时，LLaMA2-70B 卓越的通用 Python 编

码能力能够解决它没有经过训练的新问题。然而，LLaMA

2-70B 模型无法将其编码能力推广到 Tcl 工具，可能是因

为它没有接触到大量的 Tcl 代码。这突出了 DAPT 在低

容量或专有编程语言方面的优势。

LLaMA2-13B-Chat＇ ChipNeMo-13B-Chat （noDSFT）ChipNeMo-13B-Chat

LLaMA2-70B-Chat

75

50

25

Tool1

Tool1

Tool1

Tool2

Automatic

Automatic

Human

Human

（Easy）

（Medium）

（Hard with Context）（Hard with Context）

Fig.9:EDA 脚本成评结果

G.Bug 总结与分析

为了评估我们的错误总结和分析模型，我们有一组 40

个错误，它们是总结的理想候选者。这包括有很长的评论

Evaluation Benchmark Name

Size

Tooll （Python）- Automatic （Easy）

150

Tooll （Python）- Automatic （Medium）

30

Tooll （Python）- Human （Hard with Context）

10

Tool2 （Tcl）- Human （Hard with Context）

10

TABLEV:EDA 脚本生成评基准