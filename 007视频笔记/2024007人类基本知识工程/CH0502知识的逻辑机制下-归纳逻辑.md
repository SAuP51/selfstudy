## CH0502知识的逻辑机制下-归纳逻辑

### PPT 内容

#### P1 —— 古德曼的解法

惯例与习惯：我们使用「绿色」而不是「grue」进行归纳推理，是因为「绿色」是我们习惯使用的谓词，它具有传统的意义和惯例支持。相反，「grue」是一个人为构造的谓词，没有这种传统的支持。

投射性原则：只有那些在过去成功用于归纳推理的谓词才是「投射性」的，即适合用于未来的归纳推理。我们之所以选择「绿色」而不是「grue」，是因为「绿色」已经在过去的归纳推理中证明了其有效性。

语言和概念框架：归纳推理的有效性与我们使用的语言和概念框架密切相关。不同的语言和概念框架可能会导致不同的归纳推理结果。

#### P1 —— 辛普森悖论

在控制了某些变量后，可能会得到与未控制这些变量时完全相反的统计关系。

影响身体痊愈的因素很多，比如服药的患者 A 比较年轻，身体抵抗力较强，即使不服药，依靠自身免疫力依然能够痊愈，这种情况下，虽然对比患者 B，患者 A 服药且痊愈了，但我们并不能认为服药是痊愈的原因，自身免疫力也可能是痊愈的原因。

#### P1 —— 辛普森悖论反事实框架：变量

潜变量（Potential variables）：与可观察变量相对，是不直接观察但是通过观察到的其他变量推断（通过数学模型）的变量（直接测量）。旨在用潜在变量解释观察变量的数学模型称为潜变量模型。

潜在结果（Potential outcome）和事实结果（Observed outcome）：在现实世界中，对于每一个研究对象，其在每一种干预下都存在一个可能一的结果，即潜在结果；而在真实观测数据中出现的结果称为事实结果。

协变量（Covariate Variable）：是研究单元的一些相关属性。它们可以不是我们选择干预的变量，但可能会给最终的结果带来影响。

#### P1 —— 匹配法

匹配法是利用一定观测到的匹配变量（matchingvariable），将处理组和控制组中匹配变量取值相同或相近的观测对象匹配起来，以消除两组可观测变量数据非平衡并纠正显性偏估的一种准实验方法。

当观测数据中的干预不是随机分配的时，观测数据原本不能直接产生因果结论，而匹配法试图通过各种事后的数据平衡技术，将观测数据中一些可能对因果结论产生混淆作用的数据信息副除，变换观测数据，使其能产生与随机实验数据具有相同因果推断效力的结论。

#### P1 —— 分层匹配的局限：维度的诅咒

随着匹配变量增多，层内的数据会变得更加稀疏，可能出现许多层内只包括处理组或控制组个体，甚至空层，使得样本中有大量控制组或处理组个体找不到可以与之相匹配的对象。

这一问题的根源在于人类的行为选择并非由单一变量决定，在大部分研究情境中匹配变量 X 是一个含有多维度的向量，传统的分层匹配无 X 法满足多维向量变量匹配的需要。

#### P1 —— 什么是断点回归

断点回归（regression-discontinuity， RD）是自然实验中的一种观察方法，简单理解就是观察在临界点处是否出现「断点」(cut-point），并分析引起「断点」的原因对因变量产生的影响（即因果效应）。

#### P1 —— 什么是双重差分

双重差分（Differences In Differences，DID）法的目标是获取相对同质的策略组和控制组。「相对」指除策略影响外，策略组和控制组的结果变量随时间的变化存在一个基本固定的差异。

对于相对同质的策略组和控制组，DID 法通过第一次的差分消除这个基本固定的差异，通过第二次的差分评估策略带来的实际效应。

#### P1 —— 约翰·斯诺的霍乱假说

最有趣的自然实验之一，也是最早的双重差分设计之一，是关于约翰斯诺如何通过一项巧妙的自然实验，让世人相信霍乱是通过水，而不是空气传播的。

#### P1 —— 平行趋势假设

倍差法背后隐藏着一个重要假设，即处理组和控制组的结果变量在不一接受干预的条件下应具有完全相同的变化趋势。只有这一假设成立，运用倍差法才能获得处理效应的无偏估计。

学界形象地将该假设称为「平行趋势假设」(parallel trend assumption）或「共同趋势假设」（common trend assumption）。

举例：提高最低工资是否会导致失业增加？

劳动经济学领域长期存在着一个争议 —— 提高最低工资是否会导致失业增加？按一般常识，提高最低工资会导致失业增加，因为提高最低工资会增加企业的生产成本，进而导致企业减少雇佣。

美国经济学家戴维·卡德（David Card）和艾伦·B. 克鲁格（Alan B. Krueger）于 1994 年在《美国经济评论》上发表的一篇题为《最低工资与就业：新泽西州和宾夕法尼亚州快餐行业案例研究》 （Minimum Wages and Employment：A Case Study of The Fast-food Industry in New Jersey andPennsylvania）的文章显示提高最低工资不见得会引发失业，相反还可能增加就业。

#### P1 —— 合成控制的提出

本文以巴斯克地区的恐怖主义冲突为案例，研究冲突对经济的影响。我们的分析基于两种不同的策略。首先，我们利用其他地区的组合来构建一个「合成」控制区，该控制区与 20 世纪 70 年代政治恐怖主义爆发前巴斯克地区的许多相关经济特征相似。这个没有恐怖主义的「反事实」巴斯克地区随后的经济演变与巴斯克地区的实际情况进行了比较。我们发现，恐怖主义爆发后，巴斯克地区的人均国内生产总值与合成对照地区相比下降了约 10 个百分点。此外，这种差距似乎随着恐怖活动的激增而扩大。本研究的第二部分利用 1998 年 9 月宣布的停战作为自然实验，来估计冲突的影响。

### 音频整理

各位同学，现在我们正式开始今天的课程。今天的内容较多，吸取上一次讲解演绎逻辑时录制的视频中速度较快的经验，今天我们将会稍微讲得慢一点。有些知识点可能会快速过。今天上午是从 9 点半讲到 11 点半左右，留出一些时间给大家交流和提问；晚上则是从 7 点讲到 9 点左右，同样会留出时间供大家交流和提问。

现在我们正式开始今天的课程。前面的内容已经进展到了第五部分的知识逻辑机制，在此之前，我们将对第四部分的知识逻辑机制上做一个简单的回顾。为什么我们在学习了多年的逻辑后，还是记不住用不上，日常生活中实际使用到的逻辑规则并不需要那么多呢？这主要是从人类作为求知者的角度出发，围绕第三种知识的常见形态进行探讨，将不同的处理机制最终归结为几个重要的方面，即一些基本的逻辑规则。

我们的分类方法如下：

第一类是从逻辑规则的角度来看，即使是一个盲人也能感知到的一些逻辑规则。这类规则与语言表达无关，我们称之为知觉类逻辑规则，它是相对底层的一种。例如，列举、对比、模拟以及在此基础上的组合和递归等。

接着是第二类，我们称之为语言符号类的逻辑规则。在日常生活中，大多数情况下进行的是符号处理，即任何一个语言系统本身都是由符号组成的，我们需要定义所使用的语言中每个字母或符号代表的意义。

接着是语法问题，即主语在前还是谓语在前。接下来是语义问题，A 指向的是什么？推导过程是从 A 到 B 再到 C 的逻辑关系是如何建立的。那么从这个角度来看，我们总结了逻辑这一基础性的重要内容，称之为「逻辑九则」。这一点大家已经比较清楚了。

今天我们要总结的是最重要的逻辑九则。接着，我们将开始沿着四大逻辑系统进行：演绎、归结、因果和高阶分别介绍了这些内容。这里重点介绍演绎逻辑，它作为逻辑的起点，被总结为四个子逻辑系统：词相逻辑、命题逻辑、关系逻辑、模态逻辑。

其中最经典的是三段论，属于词相逻辑；而处理符合命题则需要命题逻辑。接着是关系逻辑，它处理的是关系命题；最后比较重要的是模态逻辑，涉及可能世界的演绎逻辑，涵盖了所有可能事件的演绎过程，作为整个逻辑系统的起点。在今天的讲述过程中，大家会发现演绎逻辑中的命题逻辑和模态逻辑将反复出现。

今天我们要开始介绍的内容包括归纳逻辑、因果逻辑以及高阶逻辑。我们先从归纳逻辑讲起。归纳逻辑又称为经验概括，在日常生活中无论是小孩、青少年还是成年人都非常依赖习惯，而习惯本身就是一种典型的归纳逻辑。接下来我们将对归纳逻辑进行抽象的图示化，例如：观察一，铁加热后会膨胀；观察二，铜加热之后也会膨；观察三，铝加热之后也会膨；观察 n，其他金属加热之后也会膨。

首先，我们观察到三种具体的金属在加热之后都会膨胀；进一步地，其他已知的金属在加热后也表现出相同的特性。基于这些观察，我们可以进行一次归纳推理，并概括出一个普遍结论：所有已知的主要金属在加热后均会出现膨胀现象。这是基于当前世界上发现的大多数金属所得出的结论。但是我们也应该意识到，世界上可能还有一些新近发现的或未被完全研究的金属，它们是否同样遵循这一规律尚未可知。因此，在归纳推理中还包含了一个预测部分：即第 N+1 种金属在加热后也会膨胀。

这个图示非常重要，因为它清晰地展示了整个归纳过程可以分为两个阶段：第一是归纳概括，第二是归纳预测。我们从具体的观察入手，逐步形成系统的归纳逻辑。这一贡献主要归功于弗朗西斯·培根，他在 1620 年出版了《新工具论》一书，在书中提出了系统化的归纳逻辑理论，即著名的「培根三表法」。

培根的三表法简单来说分为三个步骤：第一，确定具备某一特性的对象（存在与具有表）；第二，排除不具有该特性的对象（差异表）；第三，分析这些特性在不同对象间的表现程度（程度表）。以加热现象为例，在《新工具论》中，培根列举了太阳光、火以及受压水和空气等 28 种不同的例子，发现它们共同的特点在于内部存在运动或摩擦。例如，光线的传播伴随着其内部的摩擦运动，而受压缩的空气则因为内部粒子的碰撞而产生热量。

接着，培根又观察到月光、灵光以及未受压水和空气等不具备热的现象，这些例子中没有发现内部摩擦与运动的存在。进一步地，他还注意到一些生物如鱼、野兽、神鸟等体内的温差变化，这种温度的变化是逐步进行的。例如，在对比不同星球的热能时，培根指出太阳显然比月亮更热；而在地球上的生物中，鸟类的体温波动范围通常大于鱼类，而蛇类的体温变化则又小于鸟类。

通过这些观察与分析，培根为归纳逻辑提供了一套系统的方法论，对后世科学研究方法的发展产生了深远的影响。

培根通过大量的观察，最终归纳出一个结论：热的本质是物体内部微小粒子的运动。大家注意一下，培根在这里运用的是最原始的归纳法。体现了一种精神，在 17 世纪这个年代，人们开始对世界上各种各样的现象进行观察，尝试总结出一类事物的本质。因此，这种追求在 17 世纪形成了所谓的本质主义。

前面我们讲到不同的逻辑系统及其历史背景时，已经提到数学和物理学等领域，尤其是以数学的公理系统为例，在古希腊时期已初步形成。到了 16、17 世纪，从这些公理系统中发现的世界规律被用来改造世界的已经差不多了。这个过程中，有一批智者，特别是培根为代表的人物，他们对这种现象不再满足。数学部分在很大程度上与神学捆绑在一起，最终形成了经验哲学。这一时期，人们更加重视对自然界的观察，并总结各种本质。因此，在 16、17 世纪诞生了无数类似的著作，比如探讨热的本质等课题。

这类思潮在科学史上被称为本质主义。随着逻辑学的进步，现代的归纳逻辑系统逐步形成，我们可以将其总结为三个子系统：枚举归纳、类比归纳和因果归纳。其中最特殊的是因果归纳，它既属于归纳逻辑部分，也属于因果逻辑部分。为什么需要有因果归纳以及因果逻辑系统，我们将在后面解释。

接下来，我们将详细探讨这三个逻辑系统的处理对象的区别。例如，枚举归纳通常处理的是比较具体的问题，像培根的这个例子就是一例。一个很典型的就是枚举归纳。什么是枚举？就是举例，这是比较直接的方法，但它容易导致过度概括。第二种是类比归纳，处理的对象具有相似性，启发性强，但可能容易误导人。第三种是因果归纳法（即穆勒方法），它基于世界的关系和数据，强调的是因果，复杂性比较高。

我们先来看一下枚举归纳。什么是枚举归纳？它以列举为核心，是所有归纳法的起点之一。今天讨论的内容在中小学时学习的演绎与归纳中一般指的是这个方法 —— 即通过列举一系列具体的实例或案例，最终从中得出一个普遍性的结论。这就是我们常说的从具体到一般的归纳过程，也就是枚举归纳。

它的数学表达可以用一个简单的公式来表示。我们可以举一个小孩子的例子来看：比如观察到所有的小狗都会叫，从而得出「所有的狗都会叫」这样的结论。这样就是通过具体的实例推导出普遍性的规律。

枚举归纳与我们的认知系统和信息加工机制有密切的关系，即列举，覆盖全么（这里指的是列举的范围和代表性）。从这个角度出发，我们将枚举归纳一般分为两类：一类是完全枚举归纳，即你的结论涉及到了所有可能的实例都已经被观察到。例如，你下了一个结论认为某个班上的同学全部是男生，这意味着该班上所有的人都已经由你统计并观察过。这种情况下就是所谓的完全枚举归纳。

另一种情况则是不完全枚举归纳，即只观察了一部分实例就得出结论，这具有一定的不确定性。比如，这个班上有 100 人，但有 20 人没有到现场，而这 20 人的性别你不认识也不确定是男生还是女生。像这种情况下就是所谓的不完全枚举归纳。在我们的日常生活中，绝大多数的归纳推理都是不完全的。因此，这导致了无数的认知偏差。

接下来，我们来看一下枚举归纳的一个逻辑规则总结：S1 是 P，S2 是 P……Sn 也是 P，所以所有的 S 都是 P。这一过程会导致许多复杂的问题，我们在后面会详细展开讨论。

通过这次介绍，希望大家能够了解枚举归纳法的优点和缺点。首先，它比较容易理解；然而它的缺点在于其不确定性、易受到偏差影响以及容易出现一些典型的反例。因此，在枚举归纳的基础上发展出了类比归纳法。与前者相比，类比归纳的推理机制有所不同。

在中小学阶段学习逻辑学时，我们通常只接触到枚举归纳这一层面。实际上，基于相似性的类比归纳在日常生活中更为广泛地使用。这是它的数学表达式：\[示例公式\] 下面我给大家举一个具体的例子：设 A 和 B 都是鸟，它们都有羽毛、都能下蛋。但我们知道，A 不能飞。通过这个前提 P（即 A 的特征）得出结论 Q（即 A 不能飞）。接着我们进行类比归纳推理，因为 A 和 B 在羽毛、鸟类特性及下蛋等方面相似，所以推测如果 A 不能飞，则 B 也不能飞。这是一个基于相似性的典型类比归纳例子。

侯世达对类比归纳极为推崇，他认为人类的思维基本上是通过类比的方式来进行的。然而，阳老师对此持有不同的观点。随着讨论的深入，我们将明白侯世达的问题所在 —— 他在这一推理模式中犯了一些相对复杂的错误。从这个角度看，虽然类比归纳在逻辑上强调比较差异，但关键在于如何区分简单与复杂的类比。

所谓简单的类比，是指我们在第三讲和第四讲中多次提到的一种新的知识论。这种知识论认为，底层受到人类大脑信息架构能力的限制。而人类的信息架构能力依然是受到七加减二的制约，也就是说我们在处理相似性的时候通常是在四至九个要素之内。所以，在这里侯士达有一个错误，他没有全面考察简单类比与复杂类比的事物，而是更多地将类比的流动替代了这两者之间的区别。这引出了一个非常有趣的话题。比如我们之前给大家画了一张图，展示了 AB 的特征。在什么情况下我们会产生所谓的「类比归纳」呢？请大家注意 P1、P2 和 P3。

我们的大脑接触某个事物时，假设只是初次接触，并且这次接触到的特征值非常多，比如说达到了 30-40 个这样的特征值。在这种情况下，产生类比归纳的难度其实比大家想象的要高得多，特别不容易进行有效的类比归纳。然而，在日常生活中，我们为什么能够如此频繁地进行类比归纳呢？这里涉及到另一句话：假设我们遇到一个非常复杂的失误，它包含 30 个特征值。

我们会逐级快速处理这些信息。具体来说，当面对一个复杂的事物时，比如由 100 个特征值构成的（为了便于说明，假设是 100 个），我们会迅速将其分为两类，例如 80 个和 20 个。这 80 个表示较强或显著的部分，而那 20 个则表示较少或较弱的部分。

当我们对比这些相似性时，如果它们之间存在差异，最初可能是 80 个部分相同没有差异，那么我们就会放弃这个对比。我们的大脑会迅速转向那些具有明显差异的 20 个部分进行比较。

接下来我们继续细分 20。我们将 20 细分为几个部分：例如，这一个是分针，这是 16，这边的分针是 4。这时我们发现，在 16 个特征上，AB 两个项目是相同的。此时，我们放弃进一步对比，转而关注四个特征，大家听懂了吗？这里面涉及到一个非常复杂的认知科学原理，这是我未来将要写的一本书的主题《认知原理》。请注意，当我们的大脑进行这种快速对比时，有第一层和第二层的处理。然而，我们的大脑并不是无限层级的机器，并不会无限制地增加层级到 N 层。大家听懂了吗？许多认知科学家以及经济学家都犯了一个严重的错误，实际上我们只能进行有限次的比较：一次、两次、三次、四次、五次左右，这是我们所有能力的极限。大家明白了吗？即使是最复杂的事物，比如我们在日常生活中遇到的一些复杂的决策 —— 例如现在的男朋友或者女朋友问题，是否要买房，你的大脑实际上会采用所谓的「适当对比原则」。这时，你往下对比到第五层就会停止了。这个原则会产生什么样的现象呢？就像再复杂的系统，在进行类比归纳时，我们基于相似性的假设。例如，你现在决定买房子，明明是在经历一个严格的计算过程，但当你去公司吃饭，和同事一起吃午饭时，同事说他最近买了一个不错的小区，你会发现你一下子跳过了这些对比。大家发现了吗？因为这时出现了非常强烈的一个相似性，我们的大脑就放弃了这个严格计算的对比过程，这就是所谓的类比归纳。

在日常生活中，这给我们带来了大量的走捷径的情况。所以，只要是在任何一次对比时，如果你身边恰好有一个极其鲜明的例子，那么这个例子需要具备什么样的特征呢？各位同学可以思考一下，显然一个很理性的人给你讲述时，这个购房决定是如何做出的呢？他说得对。虽然这样的感染很少见，但你会发现有两个同事给你这样讲：一个同事说他现在买房已经节约了一百个计算指标，并进行了五轮计算，最终选择了这套房子。你觉得这位同事能成功说服你吗？另一位同事则认为这房子交通方便，值得购买；再有一位同事称赞这房的环境好，劝你也买下来；还有的同事告诉你这是学区房，建议你买下它。大家发现了吗？这就是我们人类在进行类比归纳时一个突出的规律：当你在做复杂的类比归纳时，身边接触到简单的类比归纳，大多数情况下这些简单类比归纳中的实体数量不会超过三个，最多谈 ABC 三点，即 AB 比较、BC 比较和 AC 比较就差不多了。这样的简单类比一旦完成，会对你正在进行的复杂类比产生严重影响。

这种类比在我们日常生活中其实非常普遍且重要，但往往被忽略。这是一种图示性的归纳，其优点在于人类的大脑直觉是如何产生的 —— 因为它具备很强的灵活性和启发性，能够帮助我们找到捷径，就像跨越了五轮计算一样，简单而鲜明的例子更容易吸引成功。这种确定性依赖于相似性的度量，但很多时候我们的相似性度量往往是错误的。例如，你可能认为 80 和 20 这两个数字中，80 是相同的，20 是不同的；但实际上，在现实世界中可能是 20 相同，80 不同。因此，在进行类比归纳时，我们容易出现一些问题。

第三种归纳方法被称为因果归纳。它比前面介绍的规则稍微复杂一些。在这个方法中，通常会分为直接因果规律和间接因果规律。我们可以看一下它的逻辑规则：前面提到的方法是从一般到具体的，而这一种则是从具体到一般的归纳。这里所指的不是简单的观察一个事件，而是要同时观察两个事件，并且在时间上有先后顺序 —— 先发生的是前一个事件，后发生的是另一个事件。在这种情况下，人类大脑会开始形成一种归纳，认为按一下开关会导致灯亮。接下来，我们进一步归纳出结论：当第 N+1 次按下开关时，灯依然会亮。这就是因果归纳的一个非常典型的过程。

因果归纳最重要的贡献者是约翰·穆勒，他在 1843 年出版了《逻辑体系》一书，这是该书的早期版本。现在这本书通常被翻译为《密尔逻辑》，也被称为密尔法则。在 1863 年，大约培根提出归纳法后的 200 年，密尔对培根的方法进行了扩展。培根最初的三表法更多地涉及到我们今天所说的枚举归纳，而不太涉及因果归纳的部分。此时，穆勒对其进行了扩充。

他在书中写道：「归纳法的主要目标是确定每一个原因的结果。」那么，在培根的三表法基础上，他将其扩展为五种方法：一致法、差异法、共变法、共用法和剩余法。这些方法对于具备大学文化基础的同学来说，基本都是容易理解的。例如：

一致法

所有中毒的顾客都吃了沙拉，而其他菜品各不相同。

结论：沙拉可能是导致中毒的原因。

差异法

有一位顾客没吃沙拉且没有中毒，而其他顾客吃了沙拉并中毒。

结论：沙拉是导致中毒的可能原因。

共变法

中毒的严重程度与顾客吃的沙拉量成正相关。

结论：沙拉的摄入量与中毒程度相关，进一步支持沙拉是原因。

共用法

所有中毒的顾客都吃了沙拉，没吃沙拉的顾客没有中毒。

结论：沙拉很可能是中毒的原因。

剩余法

假设其他食物（如牛排、冰淇淋）不会导致中毒，剩下的轻微中毒可能由沙拉之外的其他食物引起。

结论：沙拉是主要嫌疑，但其他食物可能也有影响。

穆勒的这套古典归纳逻辑基本上涵盖了归纳推理能覆盖的所有原则。在这个基础之上，20 世纪我们开始发展出更多的归纳统计技术。这些统计技术是我们今天讨论归纳逻辑的起点，其中最重要的是干预和假设检验。我们今天绝大多数科学家所使用的科学知识和科学诚信，其实是建立在 20 世纪这两个核心归纳方法的基础之上。

然而，到了 21 世纪，我们 20 年前在大学学习的这套思维方法已经开始过时了。尤其是在统计学、经济学以及人工智能领域，这些方法变得非常落后。这就是为什么我们需要引入第三套逻辑系统的原因。

在大学高等教育阶段学到的知识往往已经过时了。这些过时的知识成为我们讨论的起点。现在，让我们来了解一下什么是概率和假设检验。

前面我们可以发现，穆勒的说法实际上还是一种定性的文字描述。我们需要将这种定性描述转换为定量描述，这就引出了概率的概念。

最早提出概率相关表述的是 17 世纪的数学家伯努利。他的研究成果综合了当时的知识。伯努利认为，概率可以用 0 到 1 之间的数值表示，1 代表最大可能性，0 代表最小可能性。这是一种相对客观的概率表示方法。

什么是客观概率？客观概率是指我们观察 100 件事情，然后发现某一类事件在这 100 件事情中出现的频率。这就是最早的概率概念。然而，我们会发现这种客观概率对解释日常生活中的大量现象其实有一定局限性。因为我们日常生活中的许多现象都与我们的信心、主观认知以及已有的知识背景密切相关。

基于这一认识，贝叶斯注意到了客观概率的缺点。有趣的是，贝叶斯在世时并没有发表关于贝叶斯定理的论文。他去世两年后，他的一位朋友将这篇论文发表在因果皇家科学院的院刊上，这就是著名的贝叶斯定理。

贝叶斯定理也可以称为贝叶斯概率。它指的是一种什么样的概率呢？贝叶斯概率是指人们会根据已有的信息和个人信念来估计概率。这与伯努利提出的概率不同，后者是通过实际观察 100 件事情，然后计算某类事件在这 100 件事情中出现的频率。

实际上，某一类事情发生的频率，比如说它发生了 80 次，那么我们认为它的概率是 0.8。贝叶斯定理对此进行了扩充。例如，有些人不相信这个概率，比如陈启腾，他不仅观察了 100 次，而是观察了 1000 次，他认为这个事情发生的概率很小。

这就意味着，不同人拥有的先验概率可能是不同的。另一个人可能观察了 1000 次，发现发生了 800 次；而陈启腾观察了 1000 次，只发现发生了 100 次。那么，陈启腾对这个事情再次发生的信心是不足的，他的信心程度可能在 0.1 左右，而另一个人对这个事情的信心程度可能在 0.8 左右。

贝叶斯定理的意思是什么呢？就是相信陈启腾和另一位卢晓彤这两位同学，一个 0.1，一个 0.8，他们之前的观察会影响到他们在这次事件上的判断。这就是最著名的贝叶斯定理。

贝叶斯定理是很多事情讨论的起点，今天我们也会反复提到它。很多同学都明白了，我就不再详细解释了。

接下来我们说一下另一个重要的概念：先验知识中的假设。假设可以理解为一种预测或声明。这里面最重要的是零假设和备择假设。零假设是指没有效应、无差异、状态不变的情况；备择假设则是与之相反的情况。

在进行假设检验时，我们会遇到两类典型的错误：第一类错误和第二类错误。第一类错误一般称之为「错杀好人」，比如我们对一位男生说「你怀孕了」。第二类错误则是「放走坏人」，比如我们对一位明明怀孕的女生说「你没有怀孕」。

假设检验的早期发展与著名统计学家威廉·戈塞特（William Gosset）有关。他在研究如何改进家族啤酒厂的啤酒质量时，发明了这个方法。

这是最重要的一种假设检验的统计技术，称为独立样本 T 检验。它的作用是评估不同种类的谷物对啤酒质量的影响。这里涉及两个重要的术语：T 检验和独立样本。

我们前面已经明白，我们现在要比较两种小麦对啤酒质量的影响。假设它们都是正态分布，我们是否能够抽取一些具有代表性的数据，如平均值等？然而，在日常生活中，获得正态分布的数据有一定难度，通常需要较大的数据量。对于这种大量数据，我们在日常研究中相对难以获得。

因此，这就形成了这个最重要的统计技术 —— T 检验。它不再需要正态分布那么大的样本量，而是使用较小的样本量就能满足 T 分布的要求。大家可以将 T 分布理解为一个尾部较厚的分布。当数据量增加到一定程度时，它就会变成正态分布。

威廉·戈塞特（William Sealy Gosset）在这项研究技术上提出了一种新的分布，最终形成了这个最重要的假设检验统计技术 —— 独立样本 T 检验。

我可以给大家举一个通俗的例子：假设现在我们是一位教师，要比较自己教的两个班（A 班和 B 班）学生的学习成绩。我们可能无法获得这两个班上所有同学的成绩，因此我们从 A 班和 B 班各抽取一部分同学的成绩，然后对这部分同学的每个分数进行比较。

在统计学中，我们经常需要比较两组数据。假设我们有 A 班和 B 班，每班各有 1000 人。由于收集全部 1000 人的数据比较困难，我们可以从每个班级中抽取 100 人作为样本进行比较。

我们可以将 A 班和 B 班的学生一一对比，例如 A 班的第一个学生与 B 班的第一个学生，第二个学生与第二个学生，以此类推。当 A 班学生的分数高于 B 班学生时，我们记录一次。这样，通过 100 对学生的比较，我们可以大致了解 A 班和 B 班学生的成绩情况。假设在多数情况下，A 班的成绩都优于 B 班，那么我们就可以说明两个班级之间存在显著差异。

这个例子是一个简化版的独立样本 t 检验。实际上，在日常生活中，独立样本 t 检验通过 20 世纪统计学的发展已经变得非常复杂。我们刚才提到的 1000 人样本可能符合正态分布，但 100 人的样本可能更接近于 t 分布。无论是 t 分布还是正态分布，它们都遵循某种分布规律。然而，在日常生活中，我们遇到的大量数据可能并不遵循特定的分布规律，而是具有更强的随机性。

因此，统计学的技术发展将数据处理分为两类：一类是遵循分布规律的，称为参数检验；另一类是不遵循特定分布规律的，称为非参数检验。统计学已经发展出了无数方法来处理这两类数据。

例如，当我们进行独立样本 t 检验时，如果涉及更多的变量，我们就可能需要使用多元变量方差分析。此外，还有相应的显著性检验方法。这些参数检验方法在非参数统计中也有相对应的三类方法。

在我们的日常生活和科研工作中，非参数统计方法的使用频率非常低。翻阅科学论文，大约一万篇中才能找到十篇使用非参数统计的文章。绝大多数情况下，我们都是以数据的正态分布、T 分布等参数统计方法为主。这类统计方法是我们统计学的主流，当今大多数重要的实验科学发现都建立在这个基础之上。

然而，这一基础现在面临着很强的挑战，这一点我们后面会讲到。目前大家应该已经明白了因果归纳的优点和缺点。它能够初步揭示变量间的关系，应用范围广泛。我们今天绝大多数的实验科学知识其实都来自于以参数假设检验为主流的方法。但它也有明显的缺点，最严重的是相关性不等于因果关系等问题。

这里我要提醒大家，因果归纳和我们后面要讲的因果逻辑有很多相似之处，所以我们后面还会详细讲解。前面我们评判演绎推理的标准是有效性，而现在评判归纳推理的标准是可靠性。归纳的可靠性指的是归纳能否作为一种可靠的推理方法。一般来说，我们可以从几个角度来评判归纳偏差。

接下来，我们对前面讲的三种归纳进行一个详细的总结。大家要注意，我们最熟悉的是类比归纳。这种归纳方法我们很容易忽视，但在日常生活中我们使用得最多的恰恰是类比归纳。因果归纳则与我们的整个科学知识体系强烈相关，它强调相关性的归纳逻辑。

这种方法的优点我们大家也已经明白了。在演绎逻辑的时期，数学、物理、化学都是从公理系统出发的，尤其是数学。当时在 16 世纪、17 世纪，这些学科的发展几乎是同步的。

在这个时期，归纳逻辑逐渐形成。数学转变为实验数学、统计数学和应用数学；物理学分化为实验物理和统计物理；化学也发展出实验化学和统计化学。这些学科不再仅仅从演绎角度出发，而是更加注重实践和应用。因此，21 世纪绝大多数人类文明的成果都建立在归纳逻辑的基础之上。

另一个重要意义是促进了科学家群体的职业化。1660 年，伦敦皇家学会成立，我们前面提到的贝叶斯定理就发表在该学会的刊物上。同样，1666 年巴黎科学院成立。这个时期逐步出现了今天我们所说的科学家这一群体。

接下来，我们要讨论归纳逻辑的局限性，这是一个非常重要且有趣的问题。只有理解了归纳逻辑的局限，我们才能真正让自己的知识体系迈入 21 世纪。归纳逻辑最重要的局限是所谓的「归纳之谜」。我们在前面的课程中已经介绍了休谟问题，这个问题在哲学史上还有另一种称呼，叫做「旧归纳之谜」。

休谟问题源自他的《人性论》（这本书后来被《人性研究》取代）。休谟指出，我们的因果判断来自习惯和经验。他对归纳推理提出了强烈质疑，主要质疑两点：

第一，归纳概括是非必然的。例如，我们每天都看到太阳升起，但这并不意味着明天一定能看到太阳升起。假设宇宙即将毁灭，那么第 n+1 天太阳可能就不会升起了。

休谟认为归纳概括的过程注定是非必然的，即得出的结论不一定正确。即使是像太阳每天升起这样我们已经观察了无数次的现象，也不能保证它永远如此。我们永远不能把「明天太阳还能升起」这样的预测视为绝对。

这就引出了第二个关于归纳问题的质疑：归纳预测是循环的。我们如何证明归纳推理的有效性呢？我们依赖的是过去的成功经验。例如，我们过去每天都看到太阳升起，所以我们认为未来太阳每天都会升起。但是，这本身又是一种归纳推理，因此形成了一种循环论证：用归纳来证明归纳。

让我们回到图表来看，会更清楚。大家可以发现，我们看到的是太阳在过去的每一天都升起了。然而，当我们说第 N+1 天太阳一定会升起时，永远会存在一个新的可能性。在进行归纳概括时，我们实际上把它变成了一种全集，即「所有天太阳都必然升起」。当我们进行推理时，就意味着明天太阳还会正常升起。

我们用金属膨胀的例子来说明这个问题。当我们归纳出「所有金属加热后都会膨胀」这个结论后，如果发现一种新的金属，我们就会预测它加热后也会膨胀。这个论证循环是如何出现的呢？假设我们发现了一种新的金属，但它不会膨胀，这时科学家可能就不会将它归类为金属了。大家明白了吗？这就是归纳逻辑的天然弊端。

这些天然的弊端在 21 世纪成了很多更经历用的点。大家听懂了吗？这就是休谟问题。休谟认为，人类的归纳推理注定是无效的。他比较反对培根的观点。那么，他是如何解决归纳存在的两个逻辑漏洞的呢？他认为归纳概括是非必然的。

休谟认为归纳推理不是必然的，但在日常生活中我们仍然大量使用这种推理方式。例如，我们认为明天太阳会升起，金属加热会膨胀。然而，当面对更复杂的情况时，这种逻辑变得十分复杂。

休谟解释说，归纳概括的非必然性源于人类心理的习惯。我们习惯性地认为明天太阳会升起，所有金属都会因加热而膨胀。同样，对于归纳预测的循环论证，休谟也认为是一种经验习惯。我们基于过去的经验，认为所有金属加热都会膨胀，并将这种属性归因于金属本身。

因此，休谟提出了他最著名的观点：「习惯是人生的最大指导。」这是因为休谟认识到，尽管人类日常使用的归纳推理如此不可靠，存在着归纳概括不完全和归纳预测循环论证的深层问题，但我们仍然每天都在使用这种推理方式。这既是我们的心理习惯，也是我们的经验习惯。

休谟的重要贡献在于他敏锐地意识到所有规则在形式上都有局限性。这种局限性在哲学上被称为「不对称」，并在 21 世纪形成了一个重要流派 —— 不对称哲学。休谟在解决这个问题时，将焦点完全集中在习惯上，但并没有进行更深入的剖析。

需要注意的是，这个话题涉及复杂的哲学和逻辑难题，需要更多的解释和讨论才能全面理解。

为了使解释更加通俗易懂，我们可以画一些图。在日常对话中，我们通常会遇到两类归纳推理。第一类是典型的归纳推理，例如「太阳会升起」、「金属加热会膨胀」等。第二类是反向归纳推理，即质疑既有结论，如「太阳明天不会升起」或「金属加热不会膨胀」。显然，在我们的日常生活中存在这两类推理，一种是好的，另一种是不好的。

休谟从形式上指出了所有归纳推理在形式上都是有局限性的。因此，我们需要继续思考在内容上哪些归纳是可能是好的，哪些可能是坏的。这是休谟的一个重要贡献。

20 世纪一位伟大的哲学家古德曼在他的著作《事实、虚构与预测》中做出了重要贡献。这本书于 1954 年出版，我在 2014 年读过，并在许多著作中提到过。直到最近准备课程时，我才真正理解了这本书的精髓。

古德曼的贡献主要体现在「新归纳问题」上。他构建了一个非常巧妙的思想实验。我在 2013 年曾用文学的方式描述过这个实验，当时的描述可能比原文更加优美。现在我会慢慢讲解这个例子，他是如何巧妙地构建的。

首先，请大家想象自己有一颗宝石。你每天都在观察它，发现第一天、第二天、第三天、第四天它都是绿色的。这时，你可能会形成一个归纳概括：这颗宝石永远是绿色的。大家明白吗？

古德曼非常聪明，他发明了一个新的英文词「grue」（绿蓝）。这个词之前在英语中并不存在，是由「green」（绿）和「blue」（蓝）组合而成。他给「grue」下了一个定义：在 2024 年 8 月 25 日之前被观察到的物体是绿色的，但在这个日期之后变成蓝色。我们把这种宝石称为「绿蓝」。

现在，让我们回到最初的宝石例子。显然，我们观察的这颗宝石...

首先，我们给出了一个定义，大家注意，这是一个或者的关系。只要在今天之前观察到它全是绿色的，我们就可以将它定义为绿色。但是，如果从明天开始观察到它是蓝色的，我们就可以将它定义为绿蓝色。这构建的是一个或者的关系，而非并的关系。

显然，我们之前提到的那个宝石也是绿蓝的。这时，我们就形成了两个结论：第一个结论是我们的宝石是绿色的；第二个结论是我们的宝石是绿蓝色的。在今天之前，我观察到它全是绿色的，符合「绿蓝」这个词的定义。

此时，当我们进行归纳概括、归纳推理或归纳预测时，就会形成一个矛盾的结论。大家发现了吗？这个矛盾的结论是如何形成的呢？第一个结论是宝石永远是绿色的，因为它一直是绿色的，我们归纳概括时得出的结论是绿色的。第二个结论来自于我们引入的「绿蓝」这个词，它有一个特点：过了某个时间点，到明天就开始变成蓝色。这个说法归纳预测的结果是，过了一个时间点后它变成蓝色了。这两个结论显然是相互矛盾的。

大家理解了吗？这就是古德曼的《事实、虚构与预测》这本书所提出的问题。这本书在 20 世纪和 21 世纪的哲学著作中排名大约第三或第四，仅次于维特根斯坦的《哲学研究》。

你会发现，这是一个很典型的哲学家的思维方式。我们谈维特根斯坦的时候，讨论的是家族相似性的思想实验；而在讨论古德曼时，我们谈的是「绿蓝」悖论。这显然与我们的生活常识完全冲突。为什么会形成这么强烈的反直觉结果呢？这在哲学史上被称为「新归纳之谜」，与休谟问题相对应。它实际上是将休谟问题进一步深化了，提出了一个新的问题：我们究竟如何确定哪些归纳是合理的？显然，像这个归纳，为什么引入一个词之后，整个逻辑就发生了变化？

古德曼对这个问题的解答方式很有趣。你会发现，这些哲学家在提出问题的同时，也在尝试解答这些问题。古德曼认为这个预设是基于习惯的，他遵循了休谟的传统。我们习惯了使用「绿」这个词，但像「绿蓝」这样的词是人造的新词，它并没有得到我们语言习惯和生活经验的支持。

接下来，古德曼提出了一个新的哲学体系，称之为「投射哲学」。他认为在日常生活中广泛使用的归纳推理是有效的，并将其称为「投射」。「投射」的意思是表示「绿色」这个词能够从过去投射到今天，再投射到未来。而我们制造的新词「绿蓝」不具备这种投射能力，既不能从过去投射到今天，也不能投射到未来。因此，它会被时间的河流阻挡，必然产生悖论。

通过绿蓝悖论，我们发现归纳推理深受我们使用的词汇和概念的影响。当我们把「绿色」这个词换成「绿蓝」时，整个推理就完全行不通了。在这个基础上，古德曼形成了更复杂的哲学体系。

古德曼的新归纳主义促进了 20 世纪哲学家对归纳推理合理性的深入探讨。目前，对这个问题解答相对成功的是一位在 1988 年发表论文的哲学家。我们前面介绍的贝叶斯概率作为一种主观概率，实际上能够较好地解答这一问题。

绿蓝是人类构造的，它没有受到传统支持，因此其先验概率强度较低。相比之下，日常使用的词语已经广泛应用，其先验概率强度相对较高。

在计算时，我们需要对未来进行预测，得出似然值。然后，结合先验概率和似然值，计算后验概率，最终得出结论。显然，所有概率都是绿色的情况比所有概率都是绿蓝的情况更有可能，其可能性干预会更高。大家理解吗？这是从贝叶斯的角度来解答的。

索伯的方案其实有一定创新。通过索伯的解答，我想让大家意识到一个很重要的问题：我们前面谈到的新归纳之名，不仅让我们从形式上意识到归纳推理存在先天局限，同样也让我们从内容上意识到存在好的归纳和坏的归纳。当遇到一个坏词时，整个归纳推理可能变得太过荒谬。

那么，我们该如何解决这个问题呢？举个例子，假设遇到一个顽固者，他拿着一个说明称明年太阳不会升起，所有金属碰到加热不会膨胀。对付顽固者最好的方法其实就是贝叶斯理论。我们可以告诉他，过去数千万年来太阳每天都会升起，这个先验概率是非常高的。而我们碰到一个罕见事件的先验概率是非常低的。大家听懂了吗？这是对付顽固者相对较好的一种方法。

接下来，我们再看一下归纳推理的第二个较大局限。在古希腊时期，所有学者一生下来就注定是博学者，不受限于特定学科。但在现代科学体系的发展下，我们每个人都被无穷无尽的学科所牵制。最终，我们需要掌握大量的实验性知识，如实验心理学、实验物理学、实验医学等。你会发现我们被大量无穷无尽的碎片化知识所束缚。在构建人类知识全貌方面存在较为严重的局限性。

除此之外，还有第三个重要的局限，即因果归纳。在这方面的实验往往是受限的。我们有大量的实验无法进行，尤其是涉及伦理道德和人类相关的实验。

在处理因果关系时，因果归纳存在一个显著的特点：它往往是单线程的，即 A 导致 B 或 A 导致 C。然而，我们的日常生活经验告诉我们，事物发展的结果通常是多因多果的。在处理这种复杂的因果关系时，因果归纳变得非常无力。

这就是为什么我反复强调，对于从事科研的同学来说，今天的实验科学这种认知方式所能带来的红利已经变得越来越小。在学术界，仅靠实验科学很难获得显著成果。从 17 世纪到 21 世纪，我们已经有整整 300 年的实验科学历史，最容易产出成果的时期已经过去了。

对于 21 世纪见多识广的人来说，实验科学中那种单一原因导致单一结果的结论越来越难以具备说服力。因此，我们需要开发新的逻辑处理系统。

对于前面提到的演绎与归纳这两个经典的逻辑系统，我们可以进行一个简单的总结。演绎逻辑面临的问题是：演绎的普遍前提来自哪里？例如，数学推理中的公理来自何处？而归纳逻辑面临的问题则是为什么你能从具体事例中总结出一般规律。这是目前这两个逻辑系统的核心问题。围绕这两个逻辑系统的问题，最终形成了三个不同的哲学分支。

第一个分支是演绎逻辑，其结论包含在前提中。演绎的普遍前提来自于先验论，最典型的代表是康德。康德的先验论是最容易被人接受的，他将人的普遍先验论系统化。我们前面也讲过，康德的先验论，尤其是关于范畴的论述，已经得到认知科学的实证支持。例如，人一出生就具备对空间和时间的感受能力。

第二个分支是经验论，第三个分支是介于两者之间的折中派。这些分支都涉及归纳逻辑的问题，也就是大家现在所熟知的休谟问题，即归纳之谜和新归纳之谜。归纳逻辑实际上有一个默认的前提，那就是自然的齐一性。这意味着未来的事实必然来自于过去，比如未来太阳升起这一事实与过去太阳升起是具有齐一性的。

这种自然的齐一性表明未来受制于过去，未来的事实类似于过去，无论是相似性还是绝对性，这些只是不同的表达方式。这个概念非常类似于一个公理，它特别像是一个演绎逻辑。因此，我们最终构建了两大逻辑的一种新的解法，这实际上变成了一个类似于「先有鸡还是先有蛋」的问题。

目前，我们有一个相对较好的认识：演绎逻辑的普遍前提来自于归纳逻辑，而归纳逻辑的前提又来自于演绎逻辑。这样，它们就形成了一个相互依存的循环。通过这种方式，我们能够解决许多问题。

我们已经攀登到了因果逻辑，终于到达了因果逻辑这一层次。

演绎逻辑和归纳逻辑大家相对熟悉，而因果逻辑则是大家最不熟悉的。既然我们已经有了归纳逻辑，为什么还要引入因果逻辑？特别是在我们已经有了因果归纳的情况下，为什么还要引入因果逻辑系统？这个问题在哲学史上一直存在强烈争议。不少哲学家认为人类只有两大逻辑系统：演绎和归纳，不存在其他逻辑系统。但现在，尤其是 21 世纪以来，越来越多的哲学家不同意这个观点。

让我们回顾一下 21 世纪的观点。前面我们对因果归纳这个逻辑规则进行了总结：A1 导致 B1，A2 导致 B2，A3 导致 B3，所以 An+1 会导致 Bn+1。这里会出现一个非常有趣的问题：如果我们将「导致」这个典型的因果关系词换成「伴随」，即 A 伴随 B，会不会显得更像一个典型的因果归纳？

这时我们会发现，传统的因果归纳似乎变成了一种文字游戏。你用「导致」，我用「伴随」，本质上都是在描述相关性。无论是相关、伴随还是因果，都是在描述某种关系或直接影响。

统计学之父皮尔逊，作为现代统计学的创始人和奠基者，就是从这个角度出发的。他强烈反对我们应该研究任何因果关系，认为因果关系是不可证实的。他发明了一个最重要的概念 —— 相关系数。所有接受过高等教育的人都接触过相关系数这个概念。

我们可以用一个简单的例子来说明：假设我们有 100 个 A 主题的数据和 100 个 B 主题的数据，我们之前是通过比较大小来分析它们之间的关系。现在，我们可以用更精确的方法来分析这种关系（皮尔逊相关系数）。

皮尔逊相关系数是一种计算两个变量之间关系的统计方法。它通过累积相关系数，最终用一个整体数字来代表变量间的关系程度。

皮尔逊作为现代统计学的奠基者之一，他从多个方面反对因果经验主义。首先，他认为因果经验主义所追求的因果关系是不可验证的。这让我们想起前面讨论过的「本质」这个概念。从培根最初探求热的本质，到后来经过三百年的发展，科学界形成了一种称为「本质主义」的思潮。这种思潮导致研究者们试图探索情绪、动机、认知等各种现象的本质。

然而，皮尔逊认为这种做法是毫无意义的，并且会阻碍科学的进步。他认为因果关系是不可验证的，这一观点可以从两个角度来理解：从形式上看，休谟已经证明了因果关系的不可验证性；从内容上看，因果关系的描述受制于所使用的语言和概念，稍微改变用词，其含义就可能完全不同。

基于这些观点，皮尔逊成为了操作主义的代表人物之一。操作主义放弃了追求事物本质的做法，而是转向建立可操作的概念。例如，在研究幸福感时，我们首先需要给出幸福感的操作性定义，然后才能进行相关研究。

皮尔逊之所以要放弃因果关系而转向相关关系，是因为相关关系更加客观，并且可以通过数据进行验证。在皮尔逊的时代，通过数学方法验证因果关系是非常困难的。当时验证因果关系的唯一方法是随机对照组双盲实验，这种方法在许多情况下难以实施。

因此，皮尔逊提出了相关系数这一概念，为统计学的发展做出了重要贡献。

从 20 世纪皮尔逊所处的时代出发，他认为人类注定无法以合理的成本验证因果关系。我们最终只能在医学领域通过发明药物，采取昂贵且难度很高的随机对照组试验来验证因果关系。

第三点，皮尔逊认为应该放弃因果关系研究，主要是因为这种研究很容易导致错误和偏见。通过我们前面对因果归纳的介绍，可以发现确实如此，因果推断很容易受到人的主观影响。在皮尔逊强大的影响力下，整个统计学界差不多有 30 年时间放弃了对因果关系的探讨。在这 30 年里，统计学界达成了共识。

如今 21 世纪，我们在统计学教材中学到的内容，无论是心理学、经济学还是其他学科的统计教育，都不会涉及因果关系的探讨，也不会有其定义。唯一涉及因果关系的就是随机双盲组实验。可以看出，皮尔逊的影响一直延续到 21 世纪的今天。

皮尔逊的观点之所以如此有影响力，是因为它确实有一定道理。我们可以用一个问题来看看当今教科书上学到的心理统计、教育统计、经济学统计存在什么根本性的局限，以及为什么皮尔逊的批评是成立的。

这是一个常见的关于因果效应的探讨：我们如何确定 X 和 Y 之间的因果关系？通常的做法是让人服用某种药物，然后观察是否能改善健康状况。这时会出现一个非常经典的问题，称为辛普森悖论。它指的是：一类患者服用某种药物后身体状况改善了，但没有考虑到另一种情况，即这批患者不服用这种药物，身体状况是否也会改善。这一点非常关键。

这是整个统计学的一个真正突破，非常关键，我在这里要放慢一点解释。传统统计学是如何处理的呢？它是这样处理的：把其他因素当作混杂变量，比如人口学变量（如性别、年龄等），这些都称为混杂变量。然后通过消除混杂变量后再来寻找因果效应。

接下来就开始做实验，找一批病人来吃药。我们一般将参与者分为处理组和对照组，也称为吃药组和没吃药组。这时，吃药组就会开始出现各种各样的情况，没吃药组也可能出现各种各样的情况。这个图非常关键，大家会发现这里漏掉了一类很重要的情况，就是没有考虑如果不吃药会怎么样。

在传统统计学中，对这类情况的处理更多是把它当作一个群体层面的因果效应。比如说，我们举个例子：假设有一篇论文是关于治疗抑郁症的某种疗法。通过传统统计学的处理方式，得出的结论其实是一个群体层面的有效性，即这个疗法可能从群体层面确实对抑郁症治疗是有效的。

但是具体到个体上，如果没有接受这个疗法，他的抑郁症会不会变好？对于这部分问题，传统统计学的处理是非常无力的。所以说，当年我在心理学系就读时学习心理测量和心理统计学，接受到的教育都是这样的：任何心理学结论都是概率性的，有一个置信区间，比如 0.95 的置信区间或 0.99 的置信区间。

当时统计学老师似乎在告诉我们，要相信任何心理学的结论都是概率性的，也就是说在群体层面有效，但在个体层面一定要谨慎处理。这种思维方式会导致一些复杂的问题。在现实生活中，我们无法让一个人同时服药和不服药。这是一个无法在现实中实现的复杂问题。由此可见，群体有效和个体有效是两个不同的概念。

在心理学科普文章的评论区，经常会有人说：「你们讨论的是群体有效性，但我个人的经历证明这种方法对我是无效的。」传统上，我们很难处理这种情况。面对这些质疑，我们不应该简单地认为对方缺乏心理学或科学训练。实际上，21 世纪的科学已经能更好地解决这个问题了（个人补充：用随机对照去消除）。

2『之前看书籍「2018099女士品茶」知道了，统计学最早解决该问题的是费舍尔，通过引入随机性来消除那些混杂变量。（2024-12-01）』

皮尔森认为只有相关没有因果，这种观点有一定道理。但这种反对意见也引发了一些问题：难道我们真的无法发现因果关系吗？难道因果关系仅仅是一种语言游戏吗？如果是这样，我们今天介绍的所有统计学教育最终都变成了这样的认识，我们是不是只能从群体层面讨论因果效应？

例如，为了证明一种药物有效，使其达到上市标准，我们会发现说明书中会留出一些容错空间。这时，似乎就变成了群体层面的因果效应证明。

让我们看看同时代的研究者是如何逐步迭代的。首先，他们引入了随机化的概念，形成了随机对照组实验（通常称为 RCT）。这已成为当前新药研发的标准，所有药物要达到上市标准都必须通过随机对照组实验。随机对照组实验主要处理的是这一部分问题。

然而，随机对照组实验也存在一些典型问题，比如无法完全控制人与人之间的大量行为互动等因素。

在这个基础上形成了双盲实验。我们可以发现，通过二十世纪的进展，实验方法从对照组实验发展到随机对照组实验，再到双盲实验。双盲实验可以理解为随机对照组实验的迭代版本，即让参加实验的人和主持实验的人都不知道自己属于哪个组。

这种实验方法存在几个很大的问题：首先是成本很高。例如，药物研发动辄需要花费十亿美元，难以进行。随机对照组实验一般只能用于非常昂贵的药物研发部分。在经济学、心理学领域，真正的随机对照组实验其实并不多见。其次是伦理道德问题，比如为什么让这批人吃药，而不让另一批人吃药。

因此，历史上出现了一个较大的问题：除了随机对照组实验之外，其他因果效应难道真的无法验证吗？事实上，很多哲学家和统计学家对这个问题提出了质疑。

皮尔森先生是古典因果逻辑的提出者。1878 年，他总结了三大逻辑系统：演绎逻辑、归纳逻辑和因果逻辑。他将归纳逻辑和因果逻辑视为可以扩展的逻辑。这也是皮尔森最著名的演讲，收录在他的著作中。在皮尔森的基础上，以及更多哲学家的努力下，形成了真正的因果逻辑。我们可以用一个简单的图示来总结这一发展过程。

《Chance, Love, and Logic: Philosophical Essays》

[如何形成清晰的观点 (豆瓣)](https://book.douban.com/subject/33979590/)

逻辑推理可以分为几种主要类型：演绎逻辑从特定结论到普遍性命题的推理，即从具体到一般的归纳逻辑；归纳从一般到具体的演绎逻辑。而因果逻辑与前两者有所不同，它是一种从结果推导原因的逻辑方法。

最早研究因果逻辑的是皮尔斯（Pierce）。他最初主要从逻辑学和哲学的角度进行研究，并未对其进行深入的形式化工作。皮尔斯的因果逻辑也被称为溯因推理或最佳解释推理。这种推理方法是指当一个结果发生后，我们寻找多种可能的解释，并从中找出最能解释该结果的原因。这种方法也被认为是溯因逻辑的一种表现形式。

与皮尔斯偏重哲学和逻辑学的研究不同，另一个流派对皮尔斯的相关理论进行了强烈的批评。他们最终从数学、统计学和哲学等多个角度成功论证，我们确实能够在非随机对照组实验之外验证因果关系，并进行数学计算。这个流派通常被称为因果推理或因果推断。

在本书中，我们将这两个流派统称为因果逻辑。皮尔斯的方法可以被视为寻找最佳解释的一个例子。

回到我们前面讨论的逻辑系统，可以发现因果逻辑更多地涉及到「如果...那么...」的推理结构。理解了这一点，我们就能更容易地理解皮尔斯等人观点中的不足之处。例如，他们努力寻找最佳解释，但这种解释通常是在事情已经发生之后才得出的。实际上，比最佳解释更重要的是预测和预防。

在现代因果逻辑争论中，人们发现比事实更重要的是反事实。这意味着我们需要考虑「如果」的情况，例如一个人如果不吃药，他的抑郁症能否好转。这种思考方式被称为反事实思维，在心理学的不同领域有着不同的称呼。在儿童心理学中，我们称之为「反事实思维」；在心理理论中，我们也使用这个术语；在行动心理学中，我们称之为「执行意图」。由此可见，反事实思维是人类的一种关键能力。

现代因果逻辑体系就建立在反事实思维的基础之上。反事实思维的最早描述可以追溯到休谟质疑归纳法时的论述，但真正的突破发生在 1973 年。这一年，哲学家大卫·刘易斯（与模态逻辑创始人同名）在 32 岁时出版了《反事实条件句》一书。

反事实条件句的一个简单例子是：「如果昨天我买了那张彩票，今天我就是百万富翁了。」显然，说这句话的人并没有买彩票。这种表达方式被称为反事实条件句。

值得注意的是，这里提到的两位刘易斯虽然同名，但并无血缘关系。前面提到的刘易斯在模态逻辑中提出了一个重要概念 —— 可能世界。这个概念是在我们之前讨论演绎逻辑发展过程中提到的。

总的来说，反事实思维为我们提供了一种新的认识世界的方式，让我们能够思考在现实世界中可能性不大的情况。人既吃药又不吃药，那么我们分析让他不吃药这个事情发生在可能世界。大家听懂了吗？这就形成了刘易斯的反事实框架可能世界。刘易斯认为存在无数个可能世界，每个世界呈现不同的情况。

我们可以将这些世界理解为平行世界或可能世界。显然，有些可能世界距离我们的现实世界比较远，有些则比较近。距离我们比较近的被称为最接近的可能世界。

第三个概念是反事实的真假。它实际上指的是假设语句的真假取决于在最接近的可能世界中结果是否真的发生。

让我们举一个通俗易懂的例子来解释这个概念。刘易斯规则考虑没有某个条件时结果会如何变化。比如说，在现实世界中，我设置了闹钟，最终上班没有迟到。可能事件一是我没有设置闹钟，然而我也没有迟到。可能事件二是我没有设置闹钟，我迟到了。

在这种情况下，迟到又可以分为 15 分钟、30 分钟、一个小时、两个小时等。显然，最不太可能的情况是没有设置闹钟但迟到两个小时。相对而言，更可能发生的是没有设置闹钟迟到 30 分钟。所以，在众多可能世界中，最有可能发生的那个就是最接近的可能世界。

从这个讲解出发，我们来看个体在 21 世纪因果逻辑中的四个要素。第一个要素一般被翻译为「单元」。但为了保持我们对基本知识体系的统一，我们在前面全部使用「实体」这个词。大家需要注意，这只是我们使用的词汇，在因果逻辑中，绝大多数时候是把它翻译成「单元」。它的含义是指我们究竟是处理一个班、一个年级还是一个学校这样的实体。

在因果推断中，第二个要素是「干预」，通常用字母 T 表示。干预可以理解为一种处理或行动，比如设置闹钟或吃药等。

干预通常分为两种类型：人工干预和自然干预。人工干预是指我们主动进行的处理，如在新药研发中让一组人吃药，另一组不吃药。自然干预则是由大自然形成的，如某地水质较好、山势特殊、气候炎热或寒冷等。

另一种常见的分类方法是将干预分为二值干预和多值干预。在日常生活中，我们接触最多的是二值干预，如吃药与否。而在科学研究中，我们还会遇到多值干预，例如让不同的人学习 30 分钟、1 小时或 2 小时等不同时长。

第三个重要概念是「变量」。在随机对照试验中，我们经常使用一些客观指标作为变量，如小麦产量。然而，在因果关系研究中，很多变量并没有客观指标，这些变量被称为「潜变量」。潜变量无法直接观察，比如一个人的幸福感或主观感受。事实上，绝大多数心理学变量都是潜变量。

潜变量上定义的另一个重要概念是潜在结果和事实结果。这个概念的含义是，每个人在接受每种处理后可能获得的所有可能结果。在现实世界中实际进行的处理所获得的结果通常称为事实结果。而在可能世界中进行处理所获得的结果则称为反事实结果。无论是事实结果还是反事实结果，都构成了一个结果空间，包含各种可能获得的结果。

协变量指的是与我们研究相关的一些属性，例如人口学变量、性别变量等。这些变量不是我们的干预处理，但会影响到研究结果。这就是反事实框架中的第三个重要概念。

第四个重要概念是效应。我们前面提到，绝大多数心理学研究处理的是平均因果效应，它是基于群体的推理。因此，这种方法不一定能获得个体的因果效应。从 20 世纪逻辑学和统计学的发展到 21 世纪，我们现在已经能够更好地获得个体层面的因果效应了。

通过这些概念，我们可以重新思考随机对照组实验。你会发现这种实验方法存在一个较大的问题：我们最终获得的只是事实结果，而无法获得反事实结果。我们无法知道那些被安排服药的实验组成员如果不服药，他们的病情会如何发展。这是传统随机对照组实验的一个主要问题。

第二个较大的问题是，在无干预的观察环境下，我们的实验往往不是随机分配的。这意味着实验样本可能存在偏差，而且这种偏差可能比我们预期的要大得多。

使用反事实框架来审视我们今天的统计学教材，可能会发现一些问题。

因果推理现在能够很好地解决这个问题。大家可以看到，他们重点是如何基于反事实来进行因果推断。这一解决思路形成了两个大的流派：第一个流派称为潜在结果框架，第二个流派称为结构因果模型。这两个大的流派各有优缺点。相对而言，潜在结果框架出现得更早一些，而结构因果模型则是在其基础上解决了一些前者无法解决的问题。

这两个流派的根本区别在于：潜在结果框架是把因果关系表达为结构方程。结构方程是什么意思呢？大家可以简单地理解为我们在初中、高中、大学都学过的回归方程。实际上，你可以把结构方程理解为比回归方程包含更多类型变量以及更复杂的一种方程。应该这么说，我们学过的那种线性回归方程一般都会被认为是结构方程中的一种特例。

而结构因果模型则是把因果关系表示为因果图，而不是结构方程。

接下来我们来看一下两个流派是如何进行因果推断的。首先看一下潜在结果框架，这个模型是由学者鲁宾在 1974 年首次提出的，另一个重要成果是在 1983 年发表的。这两个成果阳老师讲到过。鲁宾实际上是从反事实的思路出发，针对观察数据提出了各种各样的方法。

我们可以看一下鲁宾的这篇论文。这里有传统的一元回归方程和多元回归方程，他用这些来表示效应。那么我们重点来看一下鲁宾他们这批人是怎么来估算因果效应的，他们进行了一些重要的创新。总共这个流派最终有五个技术。这五个技术具体是如何做的，各位同学今天可能难以完全理解。

各位同学一定要明白这些方法各自解决的是什么问题。你要清楚在这个研究方向上它能帮你解决什么问题，未来还可以邀请更多的人来协作。接下来我会介绍一些统计方法。

首先我们来看第一类，叫做自然实验。自然实验是由 1989 年诺贝尔经济学奖得主提出的。我们都知道随机对照组实验，但它的要求太高了。在医学新药研发领域，可能需要投资 10 亿美元才能完成。但在经济学、心理学研究中，我们很难做到随机对照实验。那么该怎么办呢？

这位诺贝尔经济学奖得主发现，在经济学领域，尤其是政策研究方面，当政策实施后可能会产生某些影响，这时很难进行随机对照实验设计。传统上，经济学与实验科学的关系相对较远。但这位得主提出了一种新的实验技术，他认为世界本身就已经包含了真正的随机性。那么，我们为什么不从这种自然存在的随机性出发，来获得因果效应呢？

举个例子，比如人的基因天然具有随机性。他就是从这个角度出发，最终形成了一个很大的研究流派，叫做自然实验。自然实验指的是利用自然界中随机发生或随机分配的事件作为研究因果效应的工具。

我们可以对比一下几种研究方法。第一类是民科们进行的自然观察研究，这种偏向自然观察，统计学基础较差。第二类是新药研发中的随机对照实验。除了这两类实验之外，还有第三类，就是我们刚才讨论的自然实验。它既不同于民科学者简单地收集数据，也不同于严格的随机对照实验，而是利用自然界中已存在的随机性来进行研究。

已经具备随机性的数据可以用来进行研究分析。让我们来看一些具体的例子。前面我们谈到的基因就是一个例子，兄弟姐妹之间的基因差异就是一种天然具备随机性的现象。这是一种自然实验的设计。

自然实验的设计在我们现在的研究中是一个较新的概念，大约在最近十年才开始兴起。需要注意的是，我们今天讨论的所有因果推断的新知识都是最近十年才开始流行的。如果你在学术圈发表论文，使用这些方法基本上现在是比较容易的，因为这个领域的研究竞争还不太激烈。相比之下，实验研究的竞争则比较激烈。

自然实验对我们的一个启发是什么呢？我们发现它的逻辑规则实际上是去寻找自然界中天然具备的随机性。这意味着我们在日常生活中需要思考哪些现象具有天然的随机性，并利用它们来进行研究。

接下来我们来看第二个技术，叫做倾向得分匹配。我们刚才谈到匹配的时候，你会发现在进行随机对照组实验时，我们实际上就是在进行匹配，尽量减少干扰变量的影响。例如，在进行实验设计时，我们会自然地排除一些干扰变量，比如年龄过大或有严重基础疾病的人可能不会被选入某些研究中（新药研发）。

这些是传统的匹配方法，其中还包括一些典型的方法，如分层匹配。然而，这些传统匹配方法面临的最严重问题是所谓的「维度诅咒」。比如说，我们可能保证被试的性别相近、年龄相近、家庭收入相近等。通常来说，我们会考虑三四个人口学变量，再加上一个在研究中通常称为社会家庭经济地位（SES）的变量。SES 通常包括三到七个变量，结合人口学变量和其他相关变量。

在研究设计中，社会经济地位、家庭经济状况和年龄、性别、家庭收入等通常是最常见的匹配考虑变量，一般会选取三到七个变量。然而，这种做法会导致一个严重问题：当我们按照不同变量进行分层时，比如先按照年龄将 1000 人分成 500 人一组，再按性别分成 250 人一组，随着分层的继续，样本量会越来越小。这不仅超出了人类的认知负荷，也使得可用于分析的数据量大幅减少，进行匹配变得非常困难。

以股票投资为例，研究一个公司的竞争力可能就涉及 30 多个变量。在这种情况下，要保证比较对象在所有维度上都相似几乎是不可能的。因此，实际操作中，我们通常只选取三到七个变量进行匹配，这种做法虽然简单粗暴，但也是无奈之举。

这种困境被称为「维度诅咒」。为了解决这个问题，倾向得分匹配方法应运而生。这个方法于 1983 年被发明，其核心思想是通过数学方法（如协方差分析）将多维度的匹配变量进行降维，形成一个综合的分数，即倾向得分。

例如，在比较一个新的量化交易模型对股市获利的效益时，需要考虑的变量可能包括公司规模、员工人数、社会口碑等众多因素。倾向得分匹配方法可以将这些繁多的变量综合成一个倾向分数（通过协方差），从而简化匹配过程，使得比较更加可行和有效。

假设中国股市上有 5000 家公司，我们通过倾向得分为每家公司计算一个分数。举例来说，我们可以用 1、2、3、4 来表示 A、B、C、D 四家公司，它们分别获得的倾向得分可能是 0.9、0.8、0.7 等。

当我们要进行某类实验研究时，比如探讨一种算法，我们应该选择倾向得分接近的公司作为 A 组和 B 组。这种方法解决了许多传统实验研究中难以处理的问题。

这种方法是由罗森鲍姆（Rosenbaum）和鲁宾（Rubin）在 1983 年共同发明的。它使得在传统实验研究中难以处理的因果效应变得容易验证，同时也简化了控制干扰变量的过程。通过使用倾向得分来分配研究对象，我们可以确保不同组别之间的倾向得分相近，从而更容易验证因果效应。

这篇 1983 年的论文中提出了倾向得分的具体计算公式。本质上，它是将多维变量转化为一个倾向得分。计算步骤通常涉及使用逻辑回归等统计技术。获得分数后，开始进行匹配。常见的匹配方法有三种：近邻匹配、卡尺匹配和马氏距离匹配统计方法。

在进行倾向得分匹配后，我们不需要过多关注具体的匹配过程，而是使用统计方法进行验证。这相当于检验匹配是否成功，然后进行匹配后的计算。这个阶段的计算与传统假设检验的计算方法类似，主要是在某个置信区间（如 0.95 或 0.99）内检验差异是否显著。整个过程主要包括前面三个步骤。

接下来，我将用一篇具体的心理学论文作为例子来说明。这是一篇发表在中国的心理学论文，研究的问题是独生子女和非独生子女在情绪适应方面是否存在差异。我们可以发现，这个研究涉及许多干扰变量，如性别、城乡差异、家庭类型、父母教育水平、是否留守儿童以及父母职业等。在心理学研究中，处理超过三个以上的干扰变量通常非常困难。

那么，我们如何通过倾向得分来处理这些变量呢？首先，我们对所有这些变量进行计算，得到一个倾向得分值。计算完成后，我们获得了这个值。接下来，我们看一下这个例子中的下一步是什么。

在获得倾向得分后，我们开始对独生子女和非独生子女样本进行匹配。匹配前和匹配后，我们使用类似于独立样本 T 检验的方法来检验它们之间是否存在显著差异。这就是所谓的匹配后分析。

在匹配后分析中，我们对匹配前和匹配后的样本进行比较。这里使用的是独立样本 T 检验。从结果中我们可以看到，在生活满意度等方面，计算出来的结果显示没有显著差异。

让我们回顾一下整个过程：首先，我们计算各个变量的倾向得分。计算出来后，接下来我们进行步骤 2 和 3：实施匹配和诊断。（不断将样本拆分为等空间样本，直到独生组和非独生组的倾向分数无差异。我们随后检验两组在每个特征变量上的差异是否显著，进而检验是否达到倾向分数匹配的目的。）

将样本分为两组，使其成为没有差异的两组。在这个过程中，两组被拆分出来后，就相当于匹配成功了。然后，我们拿着这两组进行检验，看它们在生活满意度、主观幸福感、自我效能感、孤独感、抑郁和教育这七个变量上是否存在显著差异。研究结果发现，这些变量之间没有显著差异。

通过这种思路，我们可以看到许多传统实验科学的研究结论可能并不成立。倾向得分匹配的逻辑规则实际上是尽量找到与处理组相似的对照组。传统的寻找方法效率不够高，而且人为随机分组往往效果不佳，所以现在出现了这种新的方法。

接下来，我们来看第三种方法：断点回归。断点回归与自然实验密切相关，它是由一位经济学家在 1960 年左右提出的。我们来看一个经典研究，以理解断点回归的关键点。

在很多情况下，我们会遇到类似这样的情况：比如高考录取分数线设在 550 分。那么，550 分就成为一个界限，549 分和 551 分的考生实际上没有本质区别。如果我们要研究某个问题，当发现这两组人之间存在一定的因果效应时，就可以用这个方法来证明因果关系。这就是断点回归发现因果效应的原理。

断点回归还有一些技术细节，如找到断点，将变量进一步细分等。这些都是断点回归方法中的重要组成部分。

一个经典研究基本上是断点回归。这种方法从 1990 年代开始兴起，是近 20 年来经济学最著名的研究方法之一。从图表可以看出，这种研究方法的增长曲线在 2000 年左右开始呈现急剧上升的趋势。

接下来我们快速回顾一下这个经典研究。该研究结合了自然实验的方法。具体来说，研究对象是以色列的一所学校，它与其他地方有所不同。该校人为设置了一个规定：所有中小学班级的人数上限为 40 人。如果超过 40 人，就必须拆分成两个班级。这个 40 人的规定就成为了断点回归分析的断点。

让我们看一下这个研究的结论。研究的核心原理是这样的：我们找到某个特定的临界点附近的结构变化。比如在这个以色列学校的例子中，当班级人数从 39 人变为 41 人时，我们要观察是否发生了任何变化。如果 39 人和 41 人之间没有发生任何变化，那么我们就要质疑干预效应是否存在。但是，如果在这个断点处发生了明显的变化，那就意味着这个变量（班级人数）非常重要。

这个道理对大家来说其实很容易理解。比如高考，如果你差一分没有达到一本线，那就意味着你无法进入一本院校。我们可以拿 985 高校的分数线来举例，甚至可以更极端一点，以北大清华的分数线为例。假设北大清华的分数线是 660 分，那么我们就可以比较低于 660 分和高于 660 分的考生的最终结果。

断点回归是一种统计分析方法，它可以帮助我们计算考上北大清华后人们收入的变化。这种方法的优点在于，它可以揭示一些可能存在的显著断点，同时也能帮助我们发现某些影响并不存在。北大清华这个问题就是一个很好的例子，因为它接近于一个极限词，断点往往刚好卡在一个特定位置。这使得断点回归成为一种特别有效的工具，能帮助我们推翻一些不成立的结论。这就是断点回归在逻辑学上的应用。

接下来我们来看第四种技术，叫做双重差分法。这个概念可能有点难理解，让我来解释一下。与教科书上常见的统计处理技术不同，双重差分法是这样的：假设我们让一批人参与实验，A 组吃药，B 组作为对照组不吃药。实验结束后，我们会得到一些结果，然后进行假设检验。

双重差分法的特点在于，它引入了时间维度，对同一批人进行两次测试。比如说，我们先进行一次如上所述的实验，几年后再进行一次相同的实验。这样，我们就获得了两组数据。传统的方法通常只进行一次差分检验，而双重差分法则进行两次。最终，我们会得到一个类似于两次差分检验结果的图表。

这种方法从传统的一次差分检验扩展为双重差分，使我们能够更好地推测反事实情况，比如没有吃药的人在实验期间是否也会有所改善。双重差分法主要解决的就是这个问题。这种方法的提出者是约翰·斯诺，这是世界上最经典的自然实验方法之一（约翰·斯诺的霍乱假说）。

实验是最经典的双重差分设计。这个设计在自然实验中也是最具代表性的，在科学史上非常有名。感兴趣的同学可以继续查一下这个例子。大家能看到吗？这就是相应的双重差分，1849 年一次，1854 年一次。这是双重差分最原始的一个图。

这是双重差分中因果效应的估计，包括第一次差分和第二次差分。大家看到了吗？最终这个结果就是相应的。再看一下这个图，对于传统的随机对照实验来说，它只能处理群体效应，对个体效应处理得不太到位。大家听懂了吧？双重差分最核心的优势是更有说服力，因为它比较了干预前后的变化。

这也是一个经典研究，是 2018 年诺贝尔经济学奖得主的研究，非常有名。这些都是一些经典研究，我们快速略过，估计大家都已经脑子转不动了。

对我们来说，双重差分的核心意义是要明白，我们要发现因果效应，推断一个事情是否确实存在因果关系。我们尽量要引入一个时间变量。

接下来是合成控制法。这部分相对来说不像前面几个方法那么重要。大家会发现，双重差分更多是从时间的角度出发，而合成控制法更多是从其他角度出发。大家可以看一下，它是从这个角度触发的。

这是合成控制法的一个经典研究，发表于 2003 年。好的，那么我们快速略过这部分。

最后，我想说的是，这些方法都有各自常见的应用场景。大家会发现，到了因果推断这部分，和前面的逻辑就非常不一样了。它特别注重的不在是符号、语法、语义，他更重要的是放在推导这部分了。

自然实验的推导主要是寻找自然存在的随机分布数据。倾向得分匹配，是经济学家通过倾向得分匹配方法提高样本的相似性。断点回归则侧重于寻找临界值。双重差分关注实验组和控制组在不同时期的变化情况，而合成控制法则是在控制组合成后分析各自的适用情况。

潜在结果框架相对易于理解，且这些方法巧妙且成本低廉。经济学、心理学、社会学和政治学等领域都存在大量自然实验数据，如基因数据、智商测试结果、高考分数、分班人数和政策规定等。这些方法在近几十年来对计量经济学产生了重大影响，形成了专门的研究流派。

然而，这些方法也存在一些缺点。首先，它们需要事先明确所有变量和因果关系，这严重依赖研究者的理论知识和学科背景。其次，这种方法难以应用于人工智能领域。最后，当涉及到大量变量时，比如 30 个以上，这些方法的应用会变得非常复杂。在研究中，如果涉及的变量超过 100 个，计算就会变得异常复杂。20 年前，我曾为一家世界 500 强公司研究顾客满意度，当时涉及的变量约有 30 个。即便如此，计算协方差已经变得非常复杂，最终的结果难以直观理解。

目前，绝大多数心理学研究论文的模式是：几位教授或学者假设一个方程，推测 A 变量对 B 变量可能有影响，或者假设没有影响。最终，他们往往会得出支持自己假设的结论。这种研究方法存在很大问题，因为它无法真实反映现实世界中复杂的关系。

鉴于这些技术的局限性，出现了第二类方法，即结构因果模型。这是由著名科学家、图灵奖得主珀尔提出的。2018 年出版的《为什么》和《因果推断》等书都涉及了这个主题。这方面的内容比统计方法更难理解，但非常重要，因为当前人工智能的最新进展都受到珀尔思想的影响。

我们已经讲了 246 周，总共计划 350 页，还剩约 130 页晚上再讲。虽然我们无法详细讲解每一项技术，但最重要的是进行全局扫描，让大家意识到这些领域已经取得了突破。这样才能避免我们的知识停留在独立样本 t 检验、方差分析等基础统计方法的层次。

值得注意的是，五年前贝叶斯推断的计算还非常困难，但现在技术难度已经大大降低。五年前无法实现的事情，现在已经变得可行。

开源软件真正成熟了，非常容易计算。这意味着我们可以利用自然界和社会中存在的大量数据进行计算。在这种情况下，发表论文变得相对容易。但是，真正的人工智能突破并不在结构方程那部分。结构方程那一块是用来发表经济学、理学论文的。真正的人工智能突破是在布尔代数这一块。今天晚上我们暂时不讲这个话题，先让大家提问。

#### Q&A

Q1

有一位听众问道：「我是在药企从事研发工作。我想知道，随着这些理论和模型的进展，未来是否有可能替代我们现在进行的随机双盲实验？」

对此，我要特别提醒大家一点，这也是我另一个演讲《站在 3000 年繁荣的起点》中提及的。传统的随机对照试验（RCT）成本太昂贵了。目前，人工智能正在逐步取代全范围的研发，这个趋势已经变得非常明朗，特别是在技术上有了很多突破。

举个例子，我们可能需要花费十亿美元在日常生活中进行 30 到 100 次随机对照组实验，而且可能还需要 3 到 10 年的时间。但是在计算机模拟中，可能一年时间就能运行几十万次实验。所以这绝对是未来的趋势。

因此，我特别提醒所有从事科研和高端研究的同学们，你们都要投身于计算模拟这种认知方式。计算模拟直接涉及到因果推断、因果逻辑等这些新技术。大家要明白，这些新技术真的都是最近五年才成功普及的。

十年或十五年前，我第一次接触珀尔，麻省理工的一个好朋友向我推荐了这方面的论文。那时我问他如何计算，大家都说非常复杂，说不明白。但最近五年，这些计算变得特别简单，特别容易，就像我们使用 Excel 一样容易。

所以说，现在是一个红利时期。但如果你这个时候还在做传统的随机对照实验，时间成本都很昂贵。大家听明白了吗？

Q2

对于文字类工作者来说，如何训练自己的思维模式是一个颇具挑战性的问题。

我们传统的两大逻辑系统 —— 演绎法和归纳法，经过 5000 年的科普，已经成为人类社会的共识。然而，因果逻辑和高阶逻辑等新兴逻辑体系尚未成为人类社会的共识，这意味着我们正处于一个巨大的机遇期。

对于普通大众来说，最重要的是要学会反事实。我们应该思考：如果现实世界是这样的，那么可能的世界会是怎样的？可能的世界有多少种？通过这种思考方式，我们的认知水平会得到提升，思维也会变得更加灵活，不易受到传统演绎推理规则的局限。

在技术方面，我们现在应该是处于红利期，而不是说已经进入普通区域。因为这些技术太新了，随便发表、随便写出、随便做成果都是令人兴奋的东西。这些新兴的内容甚至还来不及写入教材。

Q3

关于大模型能力测量的问题，特别是如何确定实体、如何定义性速度，以及大模型与人类的类比问题 —— 人类有各种群体，但大模型在这方面似乎不适应整体的范畴和属性。

其实这个问题并不复杂，因为现在大模型领域已经发展出了强大的新分支，包括大模型评测。在评测过程中，我们可以借鉴语言学的方法，虽然这种方法很难解释，但只要有超越性的表现就足够了。

多性定义已经足够了。无论是学术界还是人类社会，都有一些普遍的习惯。比如说，人类社会拥有的一些词语，像智商、自立等，其实也是一种习惯。颜色的概念同样是一种习惯。在大模型领域也是如此，这个新兴领域最终会形成一些被广泛接受的术语。

例如，最近有一位博主提出了一个新概念，他认为在人工智能史上，一个人的可学习性是最重要的指标。然而，这个指标显然是不太全面的。首先，这个指标很难成为人类社会普遍承认的一种习惯。大家要明白，这里面有很微妙的地方。我们人类对于绿色、蓝色等颜色的认知，其实也是一种学习得来的社会建构。事实上，整个世界都是如此，不仅仅是这些事情。

因此，我们早期就主张要采用实用主义和操作主义的方法，不要过分纠结于这些细节。让那些哲学家去讨论这些问题吧，他们可能几千年都无法达成共识。重要的是要解决实际问题。今天我们讨论的内容，可以看作是对我们之前所说的「贝叶斯干预」这一概念的扩充。

这些内容听起来可能比较容易理解，因为我讲得比较通俗。但实际上，真正理解这些概念是非常困难的，特别是对于外行人来说。

如果目前没有其他问题，今天的讲解就到这里。我们晚上还有很多内容要继续讨论。大家现在可以去吃饭了。