### 01

阳志平 2023/12/01

活水智能# 买了近 300 个域名

大模型时代以来，国内域名买了 252 个，国外也买了几十个。用作后续产品上线。

最出乎我意料的是，某些 4 个字左右的好域名还没被人买走。

### 02

阳志平 2023/12/01

活水智能 #

今天晚上，我约了一个新手同学开产品会。在一个月前，她还是个编程新手；而今晚，我计划将一个大有前途的产品交给她开发，还是全栈开发。我个人预判，她应该 42 天左右，能开发得出来。这就是 AI 时代的奇迹。

大家耐心期待。

### 03

阳志平 2023/12/01

学编程跟学写作是一样，永远是干中学，速度更快。

编程、写作的知识点有个很大特色，永无止境。甚至，对一个 Rust 开发者来说，必须知道的知识点，对 Python 开发者来说，丝毫不重要。负责客户端开发与负责 Web 前端开发的，两个人掌握的知识点也是完全不同。

所以，通过项目，才能约束知识搜索空间，快速提高编程技能。

### 04

阳志平 2023/12/02

报道摘录。

查理·芒格去世：巴菲特的好搭档，金句频出的亿万富翁

律师之子

查理·托马斯·芒格于 1924 年 1 月 1 日出生在奥马哈，父亲是律师阿尔弗雷德·凯斯·芒格，母亲是弗洛伦斯·（拉塞尔·) 芒格。小时候，他周六在一家杂货店打工，这家店当时归巴菲特的祖父所有。（巴菲特本人也在那里工作过一段时间，但两人直到很久以后才认识。) 17 岁时，查理进入密歇根大学学习数学，但在大二的时候发生了珍珠港事件，他加入了陆军航空队。

晋升为少尉后，他被派往帕萨迪纳的加州理工学院接受气象学培训。他在那里结识了当地一位鞋店老板的女儿南希·哈金斯，两人结婚时，他 21 岁，她 19 岁。他们后来养育了三个儿女。

他很快被派往阿拉斯加州诺姆，在那里开始显露一种日后让他受益匪浅的才能。

「在军队里和当小律师时的打牌经验锻炼了我的经商技巧，」芒格在珍妮特·洛尔于 2000 年出版的《巴菲特幕后智囊：查理·芒格传》 (Damn Right! Behind the Scenes with Berkshire Hathaway Billionaire Charlie Munger) 中说。

「你必须学会的是在形势不利的情况下及早认输，而如果有大牌在手，就要下重注，因为你不常拿到大牌。机会出现，但不是经常的事，所以一旦它降临就要紧紧抓住。」

在 1946 年退伍之前，自称在脸皮厚这方面是黑带高手的芒格就已经申请了哈佛法学院 —— 他的父亲是那里毕业的 —— 尽管他作风散漫，并且没有本科学历。最后他靠着内布拉斯加老乡罗斯科·庞德 -— 法学院退休院长，芒格家的朋友 —— 出面干预才得以入学。

以优异成绩毕业后，芒格回到加州，开始从事法律工作。他最终自立门户，开办了芒格、托尔斯和奥尔森律师事务所。不过他的人生在这里出现波折：他和妻子离婚了；两人的唯一儿子泰迪在 9 岁时因白血病夭折；他的财务状况急转直下。

在基本上已经破产的时候，芒格的女儿莫莉向他抱怨那辆破破烂烂的黄色庞蒂亚克。「爸爸，这辆车太难看了，简直就是一团糟，」她说。「你为什么还要开它呢？」在洛尔的传记中，他回忆自己当时说，「为了劝退拜金女。」

为了东山再起，并利用自己在数学方面的过人才能 (「我一向会选数学课，因为不用怎么费力就能拿到‘A'，」他说），他开始在业余时间投资股票、商业和房地产。

「我很快意识到，我还是宁愿成为我们的那些有钱且有趣的客户，而不是他们的律师，」他说。

他的投资给他带来了第一个百万美元。

芒格在 1956 年与南希·巴里·波特维克结婚，三年后偶然结识了巴菲特。当时芒格回到奥马哈筹办活动，纪念不久前去世的父亲，结果他收到了当地奥马哈俱乐部的午餐会邀请函。两人的共同朋友把巴菲特引荐给他。

那一周晚些时候，芒格出席了一场晚宴，巴菲特也受邀到场。两人一拍即合，整晚都在聊。巴菲特后来回忆说，"他会被自己的笑话逗得乐不可支，我心想，" 这跟我合得来。」我也是这样的。」

「对财富的渴望」

很快，两人开始几乎每天通电话谈投资策略。「沃伦的商业模型显然比我的强，」芒格这里指的是他以小时计费的法律服务。「他反复跟我说，我的谋生方式简直是发疯，他的方式更好，我应该效仿他。」

芒格被征服了。「和沃伦一样，我对财富有着相当强烈的渴望，」罗杰·洛温斯坦在他的《巴菲特传》

(Buffett: The Making of an American Capitalist, 1995 年) 中引用他的话说。「倒不是因为我想要法拉利 —— 我想要独立。我不顾一切想要独立。我认为给别人发请款单是件屈辱的事。」

芒格开始和巴菲特联手投资魏斯可金融公司 (Westco Financial) 和时思糖果 (See's Candies) 之类的企业，而后以副总裁的身份正式加入他的公司。他说入职第一年「我还是有一只脚踩在律所，以备万一资本家这条路走不下去」。

两人将伯克希尔经营成了一个逾 5000 亿美元的庞然大物，从 1965 年到 2014 年，其原始股平均每年的收益是 21.6%, 比标准普尔 500 的 9.9% 增幅高出一倍以上。(公司的名称来自巴菲特早期收购的一家每况愈下的马萨诸塞州纺织厂伯克希尔·哈撒韦。）

通过与比尔和梅琳达·盖茨合作创办的「捐赠誓言」组织，巴菲特一直热心于慈善事业，劝说亿万富翁们将其财富的至少一半捐赠出来。然而引入注目的是，芒格并没有参与。他说他倒不是不想立誓。他的妻子、于 2010 年去世的南希希望将她那一半遗产传给子孙，「所以我何止是没有立誓，」他说:「我觉得我去捐大笔钱是很虚伪的。我已经违背了这件事的根本精神。」

芒格身后留下了第一段婚姻养育的两个女儿温蒂和莫莉·芒格；第二段婚姻养育的女儿艾米莉·芒格·奥格登；以及三个儿子小查理、巴里和菲利普；两个继子威廉和戴维·波特维克；15 个孙辈；七个曾孙辈。

### 05

阳志平 2023/12/02

在人生发展咨询时，经常碰到二三线城市的同学，缺乏出路。

其实，除了大家都容易想到的写作、主播路线之外，还有一条就是成为开发者。

在二三线城市，如果担任公务员或者类似职务，其实工作不太忙，时间较多。但工作上长期以来，缺乏挑战刺激，以及与聪明人的链接，并且较难获得高收入。

成为开发者，在 21 世纪其实是一个好选择。

我个人有个感触，2023 年生成式人工智能爆发以来，大量优质软件也跟着爆发了。很多以前，觉得功能复杂、界面美观的软件，需要一个庞大团队才能干出来，现在两三人的小团队足以干得非常漂亮了。

大家加油。

### 06

阳志平 2023/12/02

大模型时代来临之后，我经常的感触是，有限的肉体，赶不上日新月异的时代变迁。有无数值得做，也值得深入做的想法。那么，在自己心有余而力不足的前提之下，就是鼓励与支持更多同学来做。2、3 人组成一个产品团队，然后深耕某条产品线。按照我统一的方法论去打。某种意义上，这是一种新的 YC 模式。不过不同的是，我们是更大的时间周期、更低试错成本的做法。当成本降到最低，那么做什么都是胜出了。

也许，18 个月、3 年、6 年、12 年后，整个生态又不一样了。

大模型这波为什么来得这么猛？本质上，在于三件事。

数据：大量开源数据被释放出来了。尤其是英文数据。这些数据在大模型时代之前其实也普遍存在了，然而，受限于知识产权法律，无法大规模释放出来。而大模型巧妙地绕过了知识产权法律规定，让知识产权法成了一个模糊的灰色地带。因此，大规模释放出来了。我也拿到了无数数据了。

模型: openai 非常有勇气地，拿 10 亿美金试错，给大家证明文本领域 transformer 这条路线是可以跑通的。

而图像领域的扩散模型也被证明，可以跑通的。于是，大家大幅照搬即可。

算力：目前其实在快速下降。18 个月后，训练模型达到临界值所需算力会越来越小。而开源社区涌现的各类框架，也在大幅降低训练成本、推理成本。

目前在席卷一切。什么领域，都可以大模型化，毕竟，思路不同了。

### 07

阳志平 2023/12/02

活水智能 #

梁文锋团队发布的大模型。我个人预判，是国内当前最好的模型了。可以与文心一言、chatglm、minimax 三家列为第一梯队。

[深度求索发布 67B 大模型，以「开源」加速 AGI 时代到来](https://mp.weixin.qq.com/s?__biz=Mzk0OTYwNzc3NQ==&mid=2247483952&idx=1&sn=8eea05bcacaa8b74ddef822dd1727dc4&chksm=c3548e00f42307165b65a575bedb4dc8de46204da1b9f9cc32949d9a9107a7543980613f9b02&mpshare=1&scene=1&srcid=1202QNBkhDxca29RH5REaRxz&sharer_shareinfo=0db5d6dc4d0c19eea3cec26511465e4b&sharer_shareinfo_first=0db5d6dc4d0c19eea3cec26511465e4b&version=4.1.12.99346&platform=mac#rd)

### 08

阳志平 2023/12/02

活水智能# AI 编程课同学笔记

看到本周有个同学的笔记，笑死我了

提醒大家一下:

1、AI 编程课，之所以坚持这么设计，让大家接触 Rust, 是有考虑的，让大家看到不同语言的优势。

2、其实，本周学会的技术，这里有无数套利空间。大家可以更大胆地去设想自己的结业大项目。很多东西，对内行来说，就是一行代码；对外行来说，就非常值钱了。

3、另外，QT、Rust 都是非常非常非常有前途的大领域。未来尽量深耕。可能未来 Rust 版本的 QT 也会很快出现。这又是一个全新的套利空间。再就是 Rust 版本的 llama.cpp、whisper.cpp 也会很快出现。

感想：成功把 github 上某视频转 gif 的项目做成了本地应用，我感觉我已经飘了。

AI 时代，技术不再是问题，你只要有好点子就行。（算法暂时除外）

### 09

阳志平 2023/12/02

活水智能# AI 线下工作坊下周周末首期内测

跟活水那边的同事对了几轮大纲。给各位同学准备的内容较为丰富了。

首期内测在北京举办成功之后，会立即复制到全国。

活水讲师团队将手把手，线下教会大家一些先进知识，这样比大家线上折腾速度会快一些。

而 AI 线下工作坊的第二期、第三期、第四期内容，我也想好了。

未来，活水智能的 AI 知识普及次序是这样的：

1、我提前搞定一些技术难点。部分涉及商业机密大的，列为活水 AI 产品矩阵，欢迎感兴趣的同学组队深耕开发。

2、涉及商业机密不大的，且与知识工作者的生产力强相关的。我先在闭门分享会上，普及给大家。然后再由活水团队将闭门分享会的提纲挈领的讲解，拆成更细致、更完整、更严谨的线下工作坊课程，普及给大家。

明年，应该是整个社群关于 AI 知识大跃进的一年。

### 10

阳志平 2023/12/02

活水智能# 那些稀奇古怪的大模型

01 金融大模型

1.1 FinGPT (推荐！）

[AI4Finance-Foundation/FinGPT: Data-Centric FinGPT. Open-source for open finance! Revolutionize 🔥 We release the trained model on HuggingFace.](https://github.com/AI4Finance-Foundation/FinGPT)

目前最完善、最实用的金融大模型，可以直接用于炒股了。

1.2 FinGLM

[MetaGLM/FinGLM: FinGLM: 致力于构建一个开放的、公益的、持久的金融大模型项目，利用开源开放来促进「AI+金融」。](https://github.com/MetaGLM/FinGLM)

还在路上，基于 GLM 的金融大模型线路。

1.3 金融大模型：轩辕

近日，度小满正式开源国内首个千亿级中文金融大模型 ——「轩辕」。轩辕大模型是在 1760 亿参数的 Bloom 大模型基础上训练而来，在金融名词理解、金融市场评论、金融数据分析和金融新闻理解等任务上，效果相较于通用大模型大幅提升。目前，轩辕模型已可以在 Huggingface 中申请下载，面向所有金融机构开放。

[xyz-nlp/XuanYuan2.0 · Hugging Face](https://huggingface.co/xyz-nlp/XuanYuan2.0)

02 医疗大模型

2.1 meditron（强烈推荐！可能是目前最好的）

Meditron 是一套开源的医学大型语言模型（LLM）。

我们发布了 Meditron-7B 和 Meditron-70B，它们通过继续对全面策划的医学语料库进行预培训，包括精选的 PubMed 论文和摘要、国际公认的医疗指南的新数据集和通用领域语料库，适应了 Llama-2 的医疗领域。

根据相关数据微调的 Meditron-70B 在多项医学推理任务上优于 Llama-2-70B、GPT-3.5 和 Flan-PaLM。

[epfLLM/meditron: Meditron is a suite of open-source medical Large Language Models (LLMs).](https://github.com/epfLLM/meditron)

2.2 太一模型

面向生物医学领域，本项目收集整理了丰富的中英双语生物医学自然语言处理 (BioNLP) 训练语料，总共包含 38 个中文数据集，覆盖 10 种 BioNLP 中文任务；102 个英文数据集，覆盖 12 种 BioNLP 英文任务。本项目根据任务类型，设计制定任务数据统一格式，对数据集进行了统一格式转换。出色的中英双语 BioNLP 多任务能力：通过丰富的中英双语任务指令数据（超过 100W 条样本) 进行大模型指令微调，使模型具备了出色的中英双语生物医学智能问答、医患对话、报告生成、信息抽取、机器翻译、标题生成、文本分类等多种 BioNLP 能力。优秀的泛化能力：除了生物医学领域，模型仍具备通用领域对话能力，并通过设计指令模板多样性，使模型具备了较优秀的指令理解能力，在同类任务的不同场景下具有较好的泛化能力，并激发了模型一定的零样本学习能力。

[太一 · 模型库](https://www.modelscope.cn/models/DUTIRbionlp/Taiyi-LLM/summary)

2.3 本草医学大模型

本项目开源了经过中文医学指令精调 / 指令微调 (Instruct-tuning) 的 LLaMA-7B 模型。我们通过医学知识图谱和 GPT3.5 API 构建了中文医学指令数据集，并在此基础上对 LLaMA 进行了指令微调，提高了 LLaMA 在医疗领域的问答效果。

[SCIR-HI/Huatuo-Llama-Med-Chinese: Repo for BenTsao [original name: HuaTuo (华驼)], Instruction-tuning Large Language Models with Chinese Medical Knowledge. 本草（原名：华驼）模型仓库，基于中文医学知识的大语言模型指令微调](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)

2.4 启真医学大模型

QiZhenGPT: An Open Source Chinese Medical Large Language Model

本项目利用启真医学知识库构建的中文医学指令数据集，并基于此在 LLaMA-7B 模型上进行指令精调，大幅提高了模型在中文医疗场景下效果，首先针对药品知识问答发布了评测数据集，后续计划优化疾病、手术、检验等方面的问答效果，并针对医患问答、病历自动生成等应用展开拓展。

[CMKRG/QiZhenGPT: QiZhenGPT: An Open Source Chinese Medical Large Language Model｜一个开源的中文医疗大语言模型](https://github.com/CMKRG/QiZhenGPT)

03 法律大模型

3.1 ChatLaw - 法律大模型（推荐！）

ChatGPT 浪潮下，人工智能的不断扩展和发展为 LLM 的扩散提供了肥沃的土壤，目前医疗、教育、金融领域已逐渐有了各自的模型，但法律领域迟迟没有明显进展。

为了促进 LLM 在法律甚至其他垂直应用落地的开放研究，本项目开源了中文法律大模型，并针对 LLM 和知识库的结合问题给出了法律场景下合理的解决方案。

ChatLaw 法律大模型目前开源的仅供学术参考的版本底座为姜子牙 - 13B、Anima-33B，我们使用大量法律

新闻、法律论坛、法条、司法解释、法律咨询、法考题、判决文书等原始文本来构造对话数据。

[PKU-YuanGroup/ChatLaw: 中文法律大模型](https://github.com/PKU-YuanGroup/ChatLaw)

3.2 LexiLaw

LexiLaw 是一个经过微调的中文法律大模型，它基于 ChatGLM-6B 架构，通过在法律领域的数据集上进行微调，使其在提供法律咨询和支持方面具备更高的性能和专业性。

该模型旨在为法律从业者、学生和普通用户提供准确、可靠的法律咨询服务。无论您是需要针对具体法律问题的咨询，还是对法律条款、案例解析、法规解读等方面的查询，LexiLaw 都能够为您提供有益的建议和指导。

同时，我们将分享在大模型基础上微调的经验和最佳实践，以帮助社区开发更多优秀的中文法律大模型，推动中文法律智能化的发展。

[CSHaitao/LexiLaw: LexiLaw - 中文法律大模型](https://github.com/CSHaitao/LexiLaw)

3.3 LaWGPT：基于中文法律知识的大语言模型

LaWGPT 是一系列基于中文法律知识的开源大语言模型。

该系列模型在通用中文基座模型（如 Chinese-LLaMA、ChatGLM 等）的基础上扩充法律领域专有词表、大规模中文法律语料预训练，增强了大模型在法律领域的基础语义理解能力。在此基础上，构造法律领域对话问答数据集、中国司法考试数据集进行指令精调，提升了模型对法律内容的理解和执行能力。

[pengxiao-song/LaWGPT: 🎉 Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. 基于中文法律知识的大语言模型](https://github.com/pengxiao-song/LaWGPT)

04 开发大模型

太多了，哈哈。

05 测试大模型

5.1 Test-Agent

Test Agent 旨在构建测试领域的「智能体」, 融合大模型和质量领域工程化技术，促进质量技术代系升级。我们期望和社区成员一起合作，打造创新的测试领域解决方案，构建 24 小时在线的测试助理服务，让测试如丝般顺滑。

[codefuse-ai/Test-Agent: Agent that empowers software testing with LLMs; industrial-first in China](https://github.com/codefuse-ai/Test-Agent)

5.2 Testgpt

您的人工智能测试伴侣代表您编写测试，自动化，让您在不牺牲测试的情况下更快地构建和发货。

[fayez-nazzal/TestGPT: Your AI testing companion that writes tests on your behalf, automated to get you to build and ship faster without sacrificing unit tests.](https://github.com/fayez-nazzal/TestGPT)

5.3 PentestGPT

什么是 PentestGPT? PentestGPT 是由大型语言模型（LLM）授权的渗透测试工具。它旨在自动化渗透测试过程。它建立在 ChatGPT API 之上，并以交互式模式运行，以指导渗透测试人员进行整体进度和特定操作。

[GreyDGL/PentestGPT: A GPT-empowered penetration testing tool](https://github.com/GreyDGL/PentestGPT)

06 科研大模型

6.1 达尔文

Darwin 是一个开源项目，致力于预先训练和微调科学文献和数据集上的 LLaMA 模型。达尔文专为科学领域设计，重点是材料科学、化学和物理学，整合了结构化和非结构化的科学知识，以提高语言模型在科学研究中的功效。

[MasterAI-EAM/Darwin: An open-source project dedicated to build foundational large language model for natural science, mainly in physics, chemistry and material science.](https://github.com/MasterAI-EAM/Darwin)

6.2 TechGPT-2.0

TechGPT-2.0 较 TechGPT-1.0 新加了许多领域知识。除了 TechGPT-1.0 所具备的计算机科学、材料、机械、冶金、金融和航空航天等十余种垂直专业领域能力，TechGPT-2.0 还在医学、法律领域展现出优秀的能力，并扩充了地理地区、运输、组织、作品、生物、自然科学、天文对象、建筑等领域文本的处理能力。TechGPT-2.0 还对幻觉、不可回答、长文本处理等问题进行了能力增强。

同时，TechGPT-2.0 对部署的硬件要求更低，使用 NVIDIA 4090 单机单卡、或昇腾 910A 单机单卡就可完成 TechGPT-2.0 模型部署。

[neukg/TechGPT-2.0: TechGPT 2.0: Technology-Oriented Generative Pretrained Transformer 2.0](https://github.com/neukg/TechGPT-2.0)

07 航天大模型

7.1 AviationGPT

这篇论文探讨了大型语言模型在航空领域的应用。航空业有大量复杂的非结构化文字数据，充斥着技术术语和专业术语。由于数据稀缺，该领域的模型构建受到限制。大型语言模型的出现为改变这种状况提供了机会。作者提出了 AviationGPT，它基于开源的 LLaMA-2 和 Mistral 架构，并在精心策划的航空数据集上进行持续训练。实验结果表明，AviationGPT 具有多种优势，能够解决各种自然语言处理问题，并在航空领域内提供准确和相关的回答，并显著提高性能。通过 AviationGPT，航空业增强了解决更复杂研究问题和提高国家航空空域系统（NAS）运营效率和安全性的能力。

地址：

[AIAA\_AviationGPT\_v5](https://arxiv.org/pdf/2311.17686.pdf)

08 大模型评测的大模型

8.1 MMMU

我们引入了 MMMU: 一个新的基准，旨在评估需要大学级学科知识和深思熟虑推理的大规模多学科任务的多模态模型。MMMU 包括从大学考试、测验和教科书中精心收集的 11.5K 多模态问题，涵盖六个核心学科：艺术与设计、商业、科学、健康与医学、人文与社会科学以及技术与工程。这些问题跨越 30 个主题和 183 个子领域，包括 32 种高度异质的图像类型，如图表、图表、地图、表格、乐谱和化学结构。与现有的基准不同，MMMU 专注于具有特定领域知识的高级感知和推理，挑战模型来执行类似于专家面临的任务。我们对 14 个开源 LMM 和专有的 GPT-4V (ision) 的评估凸显了 MMMU 带来的重大挑战。即使是先进的 GPT-4V 也只能达到 56% 的准确性，这表明有很大的改进空间。我们相信 MMMU 将激励社区建立面向专家通用人工智能（AGI）的下一代多模式基础模型。

[MMMU-Benchmark/MMMU: This repo contains evaluation code for the paper "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"](https://github.com/MMMU-Benchmark/MMMU)

8.2 ArcMMLU

ArcMMLU 是一个专为图书馆与信息科学（Library & Information Science, LIS）打造的中文大语言模型评测基准，旨在评估大语言模型在 LIS 学科领域的知识掌握和推理能力，其中涵盖了档案学、数据科学、图书馆学和信息学等四个关键细分领域。

[stzhang-patrick/ArcMMLU](https://github.com/stzhang-patrick/ArcMMLU)

---

辛苦 @Alex @妙生等同学继续完善我这个清单，整理成一个新的 awesome 清单。我已经将你们添加为管理员啦。其他想参与维护的同学请联系他们两人。依然记得中英语双语版本。按照通用底座、评测、代码、科研、新药研发、金融、医疗、法律、驾驶、航空航天以及更多各行各业等等撰写。

各位同学也可以将自己发现的开源大模型写在评论区。通用大模型先不要写啦，那类报道很充分了。我们更关心垂直路领域的大模型。

[OpenMindClub/awesome-models](https://github.com/OpenMindClub/awesome-models/)

### 11

阳志平 2023/12/02

活水智能 #大模型新手指南 (强烈推荐）

可能是目前最好的大模型科普了。由顶尖 AI 科学家联合撰写。

本文旨在弥合人们对语言模型的研究与公众对其理解之间的知识鸿沟。作者从科学的角度出发，介绍了语言模型的概念、其在当前研究中的定位以及对其了解的边界。本文主要解决的问题是帮助公众更好地理解语言模型的技术，超越现有的技术和宣传材料。通过实验研究，作者提供了一种科学的视角。

[[2311.17301] Language Models: A Guide for the Perplexed](https://arxiv.org/abs/2311.17301)

### 12

阳志平 2023/12/03



