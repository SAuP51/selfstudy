### 01

阳志平 2023/12/01

活水智能# 买了近 300 个域名

大模型时代以来，国内域名买了 252 个，国外也买了几十个。用作后续产品上线。

最出乎我意料的是，某些 4 个字左右的好域名还没被人买走。

### 02

阳志平 2023/12/01

活水智能 #

今天晚上，我约了一个新手同学开产品会。在一个月前，她还是个编程新手；而今晚，我计划将一个大有前途的产品交给她开发，还是全栈开发。我个人预判，她应该 42 天左右，能开发得出来。这就是 AI 时代的奇迹。

大家耐心期待。

### 03

阳志平 2023/12/01

学编程跟学写作是一样，永远是干中学，速度更快。

编程、写作的知识点有个很大特色，永无止境。甚至，对一个 Rust 开发者来说，必须知道的知识点，对 Python 开发者来说，丝毫不重要。负责客户端开发与负责 Web 前端开发的，两个人掌握的知识点也是完全不同。

所以，通过项目，才能约束知识搜索空间，快速提高编程技能。

### 04

阳志平 2023/12/02

报道摘录。

查理·芒格去世：巴菲特的好搭档，金句频出的亿万富翁

律师之子

查理·托马斯·芒格于 1924 年 1 月 1 日出生在奥马哈，父亲是律师阿尔弗雷德·凯斯·芒格，母亲是弗洛伦斯·（拉塞尔·) 芒格。小时候，他周六在一家杂货店打工，这家店当时归巴菲特的祖父所有。（巴菲特本人也在那里工作过一段时间，但两人直到很久以后才认识。) 17 岁时，查理进入密歇根大学学习数学，但在大二的时候发生了珍珠港事件，他加入了陆军航空队。

晋升为少尉后，他被派往帕萨迪纳的加州理工学院接受气象学培训。他在那里结识了当地一位鞋店老板的女儿南希·哈金斯，两人结婚时，他 21 岁，她 19 岁。他们后来养育了三个儿女。

他很快被派往阿拉斯加州诺姆，在那里开始显露一种日后让他受益匪浅的才能。

「在军队里和当小律师时的打牌经验锻炼了我的经商技巧，」芒格在珍妮特·洛尔于 2000 年出版的《巴菲特幕后智囊：查理·芒格传》 (Damn Right! Behind the Scenes with Berkshire Hathaway Billionaire Charlie Munger) 中说。

「你必须学会的是在形势不利的情况下及早认输，而如果有大牌在手，就要下重注，因为你不常拿到大牌。机会出现，但不是经常的事，所以一旦它降临就要紧紧抓住。」

在 1946 年退伍之前，自称在脸皮厚这方面是黑带高手的芒格就已经申请了哈佛法学院 —— 他的父亲是那里毕业的 —— 尽管他作风散漫，并且没有本科学历。最后他靠着内布拉斯加老乡罗斯科·庞德 -— 法学院退休院长，芒格家的朋友 —— 出面干预才得以入学。

以优异成绩毕业后，芒格回到加州，开始从事法律工作。他最终自立门户，开办了芒格、托尔斯和奥尔森律师事务所。不过他的人生在这里出现波折：他和妻子离婚了；两人的唯一儿子泰迪在 9 岁时因白血病夭折；他的财务状况急转直下。

在基本上已经破产的时候，芒格的女儿莫莉向他抱怨那辆破破烂烂的黄色庞蒂亚克。「爸爸，这辆车太难看了，简直就是一团糟，」她说。「你为什么还要开它呢？」在洛尔的传记中，他回忆自己当时说，「为了劝退拜金女。」

为了东山再起，并利用自己在数学方面的过人才能 (「我一向会选数学课，因为不用怎么费力就能拿到‘A'，」他说），他开始在业余时间投资股票、商业和房地产。

「我很快意识到，我还是宁愿成为我们的那些有钱且有趣的客户，而不是他们的律师，」他说。

他的投资给他带来了第一个百万美元。

芒格在 1956 年与南希·巴里·波特维克结婚，三年后偶然结识了巴菲特。当时芒格回到奥马哈筹办活动，纪念不久前去世的父亲，结果他收到了当地奥马哈俱乐部的午餐会邀请函。两人的共同朋友把巴菲特引荐给他。

那一周晚些时候，芒格出席了一场晚宴，巴菲特也受邀到场。两人一拍即合，整晚都在聊。巴菲特后来回忆说，"他会被自己的笑话逗得乐不可支，我心想，" 这跟我合得来。」我也是这样的。」

「对财富的渴望」

很快，两人开始几乎每天通电话谈投资策略。「沃伦的商业模型显然比我的强，」芒格这里指的是他以小时计费的法律服务。「他反复跟我说，我的谋生方式简直是发疯，他的方式更好，我应该效仿他。」

芒格被征服了。「和沃伦一样，我对财富有着相当强烈的渴望，」罗杰·洛温斯坦在他的《巴菲特传》

(Buffett: The Making of an American Capitalist, 1995 年) 中引用他的话说。「倒不是因为我想要法拉利 —— 我想要独立。我不顾一切想要独立。我认为给别人发请款单是件屈辱的事。」

芒格开始和巴菲特联手投资魏斯可金融公司 (Westco Financial) 和时思糖果 (See's Candies) 之类的企业，而后以副总裁的身份正式加入他的公司。他说入职第一年「我还是有一只脚踩在律所，以备万一资本家这条路走不下去」。

两人将伯克希尔经营成了一个逾 5000 亿美元的庞然大物，从 1965 年到 2014 年，其原始股平均每年的收益是 21.6%, 比标准普尔 500 的 9.9% 增幅高出一倍以上。(公司的名称来自巴菲特早期收购的一家每况愈下的马萨诸塞州纺织厂伯克希尔·哈撒韦。）

通过与比尔和梅琳达·盖茨合作创办的「捐赠誓言」组织，巴菲特一直热心于慈善事业，劝说亿万富翁们将其财富的至少一半捐赠出来。然而引入注目的是，芒格并没有参与。他说他倒不是不想立誓。他的妻子、于 2010 年去世的南希希望将她那一半遗产传给子孙，「所以我何止是没有立誓，」他说:「我觉得我去捐大笔钱是很虚伪的。我已经违背了这件事的根本精神。」

芒格身后留下了第一段婚姻养育的两个女儿温蒂和莫莉·芒格；第二段婚姻养育的女儿艾米莉·芒格·奥格登；以及三个儿子小查理、巴里和菲利普；两个继子威廉和戴维·波特维克；15 个孙辈；七个曾孙辈。

### 05

阳志平 2023/12/02

在人生发展咨询时，经常碰到二三线城市的同学，缺乏出路。

其实，除了大家都容易想到的写作、主播路线之外，还有一条就是成为开发者。

在二三线城市，如果担任公务员或者类似职务，其实工作不太忙，时间较多。但工作上长期以来，缺乏挑战刺激，以及与聪明人的链接，并且较难获得高收入。

成为开发者，在 21 世纪其实是一个好选择。

我个人有个感触，2023 年生成式人工智能爆发以来，大量优质软件也跟着爆发了。很多以前，觉得功能复杂、界面美观的软件，需要一个庞大团队才能干出来，现在两三人的小团队足以干得非常漂亮了。

大家加油。

### 06

阳志平 2023/12/02

大模型时代来临之后，我经常的感触是，有限的肉体，赶不上日新月异的时代变迁。有无数值得做，也值得深入做的想法。那么，在自己心有余而力不足的前提之下，就是鼓励与支持更多同学来做。2、3 人组成一个产品团队，然后深耕某条产品线。按照我统一的方法论去打。某种意义上，这是一种新的 YC 模式。不过不同的是，我们是更大的时间周期、更低试错成本的做法。当成本降到最低，那么做什么都是胜出了。

也许，18 个月、3 年、6 年、12 年后，整个生态又不一样了。

大模型这波为什么来得这么猛？本质上，在于三件事。

数据：大量开源数据被释放出来了。尤其是英文数据。这些数据在大模型时代之前其实也普遍存在了，然而，受限于知识产权法律，无法大规模释放出来。而大模型巧妙地绕过了知识产权法律规定，让知识产权法成了一个模糊的灰色地带。因此，大规模释放出来了。我也拿到了无数数据了。

模型: openai 非常有勇气地，拿 10 亿美金试错，给大家证明文本领域 transformer 这条路线是可以跑通的。

而图像领域的扩散模型也被证明，可以跑通的。于是，大家大幅照搬即可。

算力：目前其实在快速下降。18 个月后，训练模型达到临界值所需算力会越来越小。而开源社区涌现的各类框架，也在大幅降低训练成本、推理成本。

目前在席卷一切。什么领域，都可以大模型化，毕竟，思路不同了。

### 07

阳志平 2023/12/02

活水智能 #

梁文锋团队发布的大模型。我个人预判，是国内当前最好的模型了。可以与文心一言、chatglm、minimax 三家列为第一梯队。

[深度求索发布 67B 大模型，以「开源」加速 AGI 时代到来](https://mp.weixin.qq.com/s?__biz=Mzk0OTYwNzc3NQ==&mid=2247483952&idx=1&sn=8eea05bcacaa8b74ddef822dd1727dc4&chksm=c3548e00f42307165b65a575bedb4dc8de46204da1b9f9cc32949d9a9107a7543980613f9b02&mpshare=1&scene=1&srcid=1202QNBkhDxca29RH5REaRxz&sharer_shareinfo=0db5d6dc4d0c19eea3cec26511465e4b&sharer_shareinfo_first=0db5d6dc4d0c19eea3cec26511465e4b&version=4.1.12.99346&platform=mac#rd)

### 08

阳志平 2023/12/02

活水智能# AI 编程课同学笔记

看到本周有个同学的笔记，笑死我了

提醒大家一下:

1、AI 编程课，之所以坚持这么设计，让大家接触 Rust, 是有考虑的，让大家看到不同语言的优势。

2、其实，本周学会的技术，这里有无数套利空间。大家可以更大胆地去设想自己的结业大项目。很多东西，对内行来说，就是一行代码；对外行来说，就非常值钱了。

3、另外，QT、Rust 都是非常非常非常有前途的大领域。未来尽量深耕。可能未来 Rust 版本的 QT 也会很快出现。这又是一个全新的套利空间。再就是 Rust 版本的 llama.cpp、whisper.cpp 也会很快出现。

感想：成功把 github 上某视频转 gif 的项目做成了本地应用，我感觉我已经飘了。

AI 时代，技术不再是问题，你只要有好点子就行。（算法暂时除外）

### 09

阳志平 2023/12/02

活水智能# AI 线下工作坊下周周末首期内测

跟活水那边的同事对了几轮大纲。给各位同学准备的内容较为丰富了。

首期内测在北京举办成功之后，会立即复制到全国。

活水讲师团队将手把手，线下教会大家一些先进知识，这样比大家线上折腾速度会快一些。

而 AI 线下工作坊的第二期、第三期、第四期内容，我也想好了。

未来，活水智能的 AI 知识普及次序是这样的：

1、我提前搞定一些技术难点。部分涉及商业机密大的，列为活水 AI 产品矩阵，欢迎感兴趣的同学组队深耕开发。

2、涉及商业机密不大的，且与知识工作者的生产力强相关的。我先在闭门分享会上，普及给大家。然后再由活水团队将闭门分享会的提纲挈领的讲解，拆成更细致、更完整、更严谨的线下工作坊课程，普及给大家。

明年，应该是整个社群关于 AI 知识大跃进的一年。

### 10

阳志平 2023/12/02

活水智能# 那些稀奇古怪的大模型

01 金融大模型

1.1 FinGPT (推荐！）

[AI4Finance-Foundation/FinGPT: Data-Centric FinGPT. Open-source for open finance! Revolutionize 🔥 We release the trained model on HuggingFace.](https://github.com/AI4Finance-Foundation/FinGPT)

目前最完善、最实用的金融大模型，可以直接用于炒股了。

1.2 FinGLM

[MetaGLM/FinGLM: FinGLM: 致力于构建一个开放的、公益的、持久的金融大模型项目，利用开源开放来促进「AI+金融」。](https://github.com/MetaGLM/FinGLM)

还在路上，基于 GLM 的金融大模型线路。

1.3 金融大模型：轩辕

近日，度小满正式开源国内首个千亿级中文金融大模型 ——「轩辕」。轩辕大模型是在 1760 亿参数的 Bloom 大模型基础上训练而来，在金融名词理解、金融市场评论、金融数据分析和金融新闻理解等任务上，效果相较于通用大模型大幅提升。目前，轩辕模型已可以在 Huggingface 中申请下载，面向所有金融机构开放。

[xyz-nlp/XuanYuan2.0 · Hugging Face](https://huggingface.co/xyz-nlp/XuanYuan2.0)

02 医疗大模型

2.1 meditron（强烈推荐！可能是目前最好的）

Meditron 是一套开源的医学大型语言模型（LLM）。

我们发布了 Meditron-7B 和 Meditron-70B，它们通过继续对全面策划的医学语料库进行预培训，包括精选的 PubMed 论文和摘要、国际公认的医疗指南的新数据集和通用领域语料库，适应了 Llama-2 的医疗领域。

根据相关数据微调的 Meditron-70B 在多项医学推理任务上优于 Llama-2-70B、GPT-3.5 和 Flan-PaLM。

[epfLLM/meditron: Meditron is a suite of open-source medical Large Language Models (LLMs).](https://github.com/epfLLM/meditron)

2.2 太一模型

面向生物医学领域，本项目收集整理了丰富的中英双语生物医学自然语言处理 (BioNLP) 训练语料，总共包含 38 个中文数据集，覆盖 10 种 BioNLP 中文任务；102 个英文数据集，覆盖 12 种 BioNLP 英文任务。本项目根据任务类型，设计制定任务数据统一格式，对数据集进行了统一格式转换。出色的中英双语 BioNLP 多任务能力：通过丰富的中英双语任务指令数据（超过 100W 条样本) 进行大模型指令微调，使模型具备了出色的中英双语生物医学智能问答、医患对话、报告生成、信息抽取、机器翻译、标题生成、文本分类等多种 BioNLP 能力。优秀的泛化能力：除了生物医学领域，模型仍具备通用领域对话能力，并通过设计指令模板多样性，使模型具备了较优秀的指令理解能力，在同类任务的不同场景下具有较好的泛化能力，并激发了模型一定的零样本学习能力。

[太一 · 模型库](https://www.modelscope.cn/models/DUTIRbionlp/Taiyi-LLM/summary)

2.3 本草医学大模型

本项目开源了经过中文医学指令精调 / 指令微调 (Instruct-tuning) 的 LLaMA-7B 模型。我们通过医学知识图谱和 GPT3.5 API 构建了中文医学指令数据集，并在此基础上对 LLaMA 进行了指令微调，提高了 LLaMA 在医疗领域的问答效果。

[SCIR-HI/Huatuo-Llama-Med-Chinese: Repo for BenTsao [original name: HuaTuo (华驼)], Instruction-tuning Large Language Models with Chinese Medical Knowledge. 本草（原名：华驼）模型仓库，基于中文医学知识的大语言模型指令微调](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese)

2.4 启真医学大模型

QiZhenGPT: An Open Source Chinese Medical Large Language Model

本项目利用启真医学知识库构建的中文医学指令数据集，并基于此在 LLaMA-7B 模型上进行指令精调，大幅提高了模型在中文医疗场景下效果，首先针对药品知识问答发布了评测数据集，后续计划优化疾病、手术、检验等方面的问答效果，并针对医患问答、病历自动生成等应用展开拓展。

[CMKRG/QiZhenGPT: QiZhenGPT: An Open Source Chinese Medical Large Language Model｜一个开源的中文医疗大语言模型](https://github.com/CMKRG/QiZhenGPT)

03 法律大模型

3.1 ChatLaw - 法律大模型（推荐！）

ChatGPT 浪潮下，人工智能的不断扩展和发展为 LLM 的扩散提供了肥沃的土壤，目前医疗、教育、金融领域已逐渐有了各自的模型，但法律领域迟迟没有明显进展。

为了促进 LLM 在法律甚至其他垂直应用落地的开放研究，本项目开源了中文法律大模型，并针对 LLM 和知识库的结合问题给出了法律场景下合理的解决方案。

ChatLaw 法律大模型目前开源的仅供学术参考的版本底座为姜子牙 - 13B、Anima-33B，我们使用大量法律

新闻、法律论坛、法条、司法解释、法律咨询、法考题、判决文书等原始文本来构造对话数据。

[PKU-YuanGroup/ChatLaw: 中文法律大模型](https://github.com/PKU-YuanGroup/ChatLaw)

3.2 LexiLaw

LexiLaw 是一个经过微调的中文法律大模型，它基于 ChatGLM-6B 架构，通过在法律领域的数据集上进行微调，使其在提供法律咨询和支持方面具备更高的性能和专业性。

该模型旨在为法律从业者、学生和普通用户提供准确、可靠的法律咨询服务。无论您是需要针对具体法律问题的咨询，还是对法律条款、案例解析、法规解读等方面的查询，LexiLaw 都能够为您提供有益的建议和指导。

同时，我们将分享在大模型基础上微调的经验和最佳实践，以帮助社区开发更多优秀的中文法律大模型，推动中文法律智能化的发展。

[CSHaitao/LexiLaw: LexiLaw - 中文法律大模型](https://github.com/CSHaitao/LexiLaw)

3.3 LaWGPT：基于中文法律知识的大语言模型

LaWGPT 是一系列基于中文法律知识的开源大语言模型。

该系列模型在通用中文基座模型（如 Chinese-LLaMA、ChatGLM 等）的基础上扩充法律领域专有词表、大规模中文法律语料预训练，增强了大模型在法律领域的基础语义理解能力。在此基础上，构造法律领域对话问答数据集、中国司法考试数据集进行指令精调，提升了模型对法律内容的理解和执行能力。

[pengxiao-song/LaWGPT: 🎉 Repo for LaWGPT, Chinese-Llama tuned with Chinese Legal knowledge. 基于中文法律知识的大语言模型](https://github.com/pengxiao-song/LaWGPT)

04 开发大模型

太多了，哈哈。

05 测试大模型

5.1 Test-Agent

Test Agent 旨在构建测试领域的「智能体」, 融合大模型和质量领域工程化技术，促进质量技术代系升级。我们期望和社区成员一起合作，打造创新的测试领域解决方案，构建 24 小时在线的测试助理服务，让测试如丝般顺滑。

[codefuse-ai/Test-Agent: Agent that empowers software testing with LLMs; industrial-first in China](https://github.com/codefuse-ai/Test-Agent)

5.2 Testgpt

您的人工智能测试伴侣代表您编写测试，自动化，让您在不牺牲测试的情况下更快地构建和发货。

[fayez-nazzal/TestGPT: Your AI testing companion that writes tests on your behalf, automated to get you to build and ship faster without sacrificing unit tests.](https://github.com/fayez-nazzal/TestGPT)

5.3 PentestGPT

什么是 PentestGPT? PentestGPT 是由大型语言模型（LLM）授权的渗透测试工具。它旨在自动化渗透测试过程。它建立在 ChatGPT API 之上，并以交互式模式运行，以指导渗透测试人员进行整体进度和特定操作。

[GreyDGL/PentestGPT: A GPT-empowered penetration testing tool](https://github.com/GreyDGL/PentestGPT)

06 科研大模型

6.1 达尔文

Darwin 是一个开源项目，致力于预先训练和微调科学文献和数据集上的 LLaMA 模型。达尔文专为科学领域设计，重点是材料科学、化学和物理学，整合了结构化和非结构化的科学知识，以提高语言模型在科学研究中的功效。

[MasterAI-EAM/Darwin: An open-source project dedicated to build foundational large language model for natural science, mainly in physics, chemistry and material science.](https://github.com/MasterAI-EAM/Darwin)

6.2 TechGPT-2.0

TechGPT-2.0 较 TechGPT-1.0 新加了许多领域知识。除了 TechGPT-1.0 所具备的计算机科学、材料、机械、冶金、金融和航空航天等十余种垂直专业领域能力，TechGPT-2.0 还在医学、法律领域展现出优秀的能力，并扩充了地理地区、运输、组织、作品、生物、自然科学、天文对象、建筑等领域文本的处理能力。TechGPT-2.0 还对幻觉、不可回答、长文本处理等问题进行了能力增强。

同时，TechGPT-2.0 对部署的硬件要求更低，使用 NVIDIA 4090 单机单卡、或昇腾 910A 单机单卡就可完成 TechGPT-2.0 模型部署。

[neukg/TechGPT-2.0: TechGPT 2.0: Technology-Oriented Generative Pretrained Transformer 2.0](https://github.com/neukg/TechGPT-2.0)

07 航天大模型

7.1 AviationGPT

这篇论文探讨了大型语言模型在航空领域的应用。航空业有大量复杂的非结构化文字数据，充斥着技术术语和专业术语。由于数据稀缺，该领域的模型构建受到限制。大型语言模型的出现为改变这种状况提供了机会。作者提出了 AviationGPT，它基于开源的 LLaMA-2 和 Mistral 架构，并在精心策划的航空数据集上进行持续训练。实验结果表明，AviationGPT 具有多种优势，能够解决各种自然语言处理问题，并在航空领域内提供准确和相关的回答，并显著提高性能。通过 AviationGPT，航空业增强了解决更复杂研究问题和提高国家航空空域系统（NAS）运营效率和安全性的能力。

地址：

[AIAA\_AviationGPT\_v5](https://arxiv.org/pdf/2311.17686.pdf)

08 大模型评测的大模型

8.1 MMMU

我们引入了 MMMU: 一个新的基准，旨在评估需要大学级学科知识和深思熟虑推理的大规模多学科任务的多模态模型。MMMU 包括从大学考试、测验和教科书中精心收集的 11.5K 多模态问题，涵盖六个核心学科：艺术与设计、商业、科学、健康与医学、人文与社会科学以及技术与工程。这些问题跨越 30 个主题和 183 个子领域，包括 32 种高度异质的图像类型，如图表、图表、地图、表格、乐谱和化学结构。与现有的基准不同，MMMU 专注于具有特定领域知识的高级感知和推理，挑战模型来执行类似于专家面临的任务。我们对 14 个开源 LMM 和专有的 GPT-4V (ision) 的评估凸显了 MMMU 带来的重大挑战。即使是先进的 GPT-4V 也只能达到 56% 的准确性，这表明有很大的改进空间。我们相信 MMMU 将激励社区建立面向专家通用人工智能（AGI）的下一代多模式基础模型。

[MMMU-Benchmark/MMMU: This repo contains evaluation code for the paper "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI"](https://github.com/MMMU-Benchmark/MMMU)

8.2 ArcMMLU

ArcMMLU 是一个专为图书馆与信息科学（Library & Information Science, LIS）打造的中文大语言模型评测基准，旨在评估大语言模型在 LIS 学科领域的知识掌握和推理能力，其中涵盖了档案学、数据科学、图书馆学和信息学等四个关键细分领域。

[stzhang-patrick/ArcMMLU](https://github.com/stzhang-patrick/ArcMMLU)

---

辛苦 @Alex @妙生等同学继续完善我这个清单，整理成一个新的 awesome 清单。我已经将你们添加为管理员啦。其他想参与维护的同学请联系他们两人。依然记得中英语双语版本。按照通用底座、评测、代码、科研、新药研发、金融、医疗、法律、驾驶、航空航天以及更多各行各业等等撰写。

各位同学也可以将自己发现的开源大模型写在评论区。通用大模型先不要写啦，那类报道很充分了。我们更关心垂直路领域的大模型。

[OpenMindClub/awesome-models](https://github.com/OpenMindClub/awesome-models/)

### 11

阳志平 2023/12/02

活水智能 #大模型新手指南 (强烈推荐）

可能是目前最好的大模型科普了。由顶尖 AI 科学家联合撰写。

本文旨在弥合人们对语言模型的研究与公众对其理解之间的知识鸿沟。作者从科学的角度出发，介绍了语言模型的概念、其在当前研究中的定位以及对其了解的边界。本文主要解决的问题是帮助公众更好地理解语言模型的技术，超越现有的技术和宣传材料。通过实验研究，作者提供了一种科学的视角。

[[2311.17301] Language Models: A Guide for the Perplexed](https://arxiv.org/abs/2311.17301)

### 12

阳志平 2023/12/03

刚看到隔壁方军老师的感叹，强调 AI 帮我们做阅读摘要，其实意义不大。其实，这个就是我反复强调的一个观点：可以拿 A 忽悠别人，但不能忽悠自己。

AI 能提高我们建立知识脉络、理解知识难点的速度，然而，真正地将知识脉络、知识难点记忆到人类大脑中，才算真正记忆成功。这个时候，必然会遵从必要难度理论这些，受制于人类大脑的信息输入输出速度的瓶颈。

这个原理，几乎是 AI 时代做一切产品的前提。其实很多产品经理一直没理解透。

这个数学题目，很多人算不清的，参见我之前写的一条：

大模型懂得这么多，然而，你的大脑能记住多少？用得上多少？人类短暂一生，如何与大模型相处，才可以利益最大化？假设大模型训练语料以指数级别增加，算力成本以线性比例下降。

这道计算题，就是我们知识工作者即将面临的时代新问题。算清楚了，里面其实也是无数套利空间。

### 12

阳志平 2023/12/03

活水智能# ChatGPT 及其超越：教育中的生成性人工智能革命

生成型人工智能 (AI) 模型，特别是 Chat-GPT 模型的广泛采用和使用，引发了探索其在教育领域潜在应用的研究热潮。

本调查调查了 2022 年 11 月至 2023 年 7 月期间发表的学术文献，特别针对 Scopus 索引的 Q1 和 Q2 期刊的高影响力研究。

这项调查深入到实际应用和影响的生成性人工智能模型在不同范围的教育背景。通过对近期学术文献的全面和严格评估，本调查旨在阐明生成人工智能模型，特别是 ChatGPT，在教育中的不断发展的作用。通过揭示这一动态领域的潜在利益、挑战和新趋势，本次调查力求有助于理解人工智能与教育之间的联系。本综述的发现将使教育者、研究者和决策者能够就人工智能技术与学习环境的集成做出明智的决策。

cc @北北 @Excelsior

[[2311.15198] ChatGPT and Beyond: The Generative AI Revolution in Education](https://arxiv.org/abs/2311.15198)

### 13

阳志平 2023/12/03

活水智能# 如何利用大语言模型和检索增广生成构建一个能适应任何课程并提供准确答案的人工智能导师

人工智能正在通过数据驱动的个性化学习解决方案改变教育。本文介绍了 Al Tutor，一个创新的 web 应用程序，它使用最先进的大型语言模型 (LLM) 提供任何学科的个性化教学。

AI 导师吸收课程材料，构建适合课程的自适应知识库。当学生提出问题，它检索最相关的信息，并生成详细的，会话的反应引用支持证据。该系统采用先进的大型语言模型和检索增强生成 (RAG) 技术，实现精确、自然的问答。

我们提出了一个全功能的网络界面和视频演示，展示了人工智能导师的多才多艺，在不同的科目和它的能力，以产生令人信服的教学反应。虽然这是一个初始的原型，但这项工作代表着向支持 A1 的辅导系统迈出了开拓性的一步，该系统可以民主化地获得高质量、定制的教育支持。

cc @北北 @Excelsior

[[2311.17696] How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation](https://arxiv.org/abs/2311.17696)

2023053How-to-Build-an-AI-Tutor

### 14

阳志平 2023/12/03

活水智能# AI 编程课学员笔记

来自 @阿盛同学的精彩笔记。也供其他同学参考。点评一二，就是，成年人不要再是消费者导向的学习，不要始终是学习爱好者，而是创作者，用作品说话。

成年人 18 岁以后的学习，应该尽量是作品导向。也就是要么不学，要学就一定要做出一个作品。对于 AI 编程课来说，就是一个能被他人使用的 AI 软件产品。

祝福更多同学，通过 AI 编程课打开新世界的大门。

在完成 W4 的作业后，才发现前面学的所有知识点都串起来了，在刚开课时不太理解的问题，直到把 web 代码放上服务器时，才明白这样设计的原因，开始「有点感觉了」。

比如：

01 为什么要学使用终端和制作 CLI 应用？

这是从《笨方法学 python》就有的疑问，明明有那么多编译器可以用，为啥还要记这么指令。在照着男秀的教程把云主机部署时，进入服务器终端，拉取代码、配置 API KEY、启动应用，感觉并不陌生，要是一直用的都是可视化界面，这时哪怕照着教程操作都会有点慌。

02 为什么要一上来就学习类？

在编程课的第一周就讲到「类」的概念，可能只有开智敢这么干吧。在最初看到课程时会感觉「是不是有点太快了」, 但在这两周学下来会发现，从一开始就建立好「面向对象编程」的思维，可能是最快上手到「能做作品」的路径，在刚开始的 CLI 应用，熟悉了类的属性和方法，后面做网页用的 fastapi 和数据库，用到「类」来做规范数据传输和操作，对于「面向对象」有了点感觉。

在 WEB 应用部分我其实学得挺慢的，但在完成了这一个 MVP 后，能感觉到是更有信心做网站了，哪怕是小小的「麻雀」, 也是从空白文档把五脏六腑都敲出来了。小结一下学习方法，可能会需要再优化，先做一个记录：

03 让 AI 坐在副驾，而不是握着方向盘

在第一次做基础语法的作业，发现输入注释就能自动出答案了，就关闭了 AI 插件，到现在都还没有打开。复现课程里的案例和做作业，都是用类似《笨方法学 Python》的方式，手工古法敲代码，遇到卡点或是不懂的概念向 GPT 提问。特别是到了 WEB 应用的部分，有多个关联文件需要调试，至少得知道各个模块的功能是什么，是怎么交互的，需要让 GPT 协助时才能描述清楚。

比如在配置 apikey 部分，课程里没有具体讲保存和调用的方法，问了 GPT 才知道是用.env 文件保存，用 py 一 dotenv 库的 load_dotenv 进行调用，再把.env 放在.gitignore 里，不会同步到仓库里，后来在助教开小灶时发现用的也是这个方法。

04 学习课程视频的方法

WEB 应用的视频应该是我在过去一年里，观看时长最久的课程了，几乎是逐帧反复看，跟着敲代码和翻文档，用到文档里的代码，也不是从课程示例代码复制，而是从 fastapi 官方文档里拿过来改。看着视频里按照文档一步步把功能实现，才了解阳老师说要「看黑客编程视频」的学习方法，一边是能直观地看到高手是怎么实现一个功能，一边是会看到即使是高手，也还是要不断地查文档，拿示例代码改过来使用，能把文档的内容转为自己的代码，是入门的关键一步。

回顾学习的过程，我是先完整看一遍实操案例课程，对需要实现的功能有哪些模块、要用到哪些库和整体运作逻辑有印象，然后是分段看视频，比如数据库部分的 4 个模块，是逐个看完，知道这个模块做完后是有哪些部分或是函数，最后才是逐帧跟着写代码和翻文档。这样的「笨方法」学起来特别慢，但对整个项目文档都有印象。

05 不同模块的关联

在 WEB 应用中，用 Python 作为主要语言，编写后端逻辑，调用前端模块各个模块，同时能操作数据库增删查改。

HTML 是编写页面的骨架，用标签层级把网页分成不同的区域和功能块，除了基本的标签语法，还有 jinja2 语句的使用，可以使用循环和分支等语法，让代码更简洁优雅。

JS 是让网页能「动起来」, 对页面内的操作可以做出反应，每一个按键、输入框和选择框等，背后都是有 JS 在做出相应，这部分的语法跟 Python 差别比较大，并且 GPT 对 JS 的报错处理和生成代码会差点意思，但具体概念的解释还是可用的，上手难度会略高一些，也是后面要重点突破的地方。

CSS 是网页的皮肤，能对 HTML 中的元素进行排版，把白底黑字堆在一起的按键和框框的页面，变得整齐美观。这部分代码在第一版里我是几乎直接复制示例代码的，在实现完核心功能后才回过头来做，把生成页面中的输出框隐藏了，在点击「生成」后才在输入框下方直接显示出内容，就改了这一个小功能，用到的是 JS 控制 CSS 中元素的 display 属性，让我对 JS 和 CSS 的交互有了新的理解。

后来梳理了一下实现一个新功能的步骤，也在开小灶时问了助教，大致的步骤是：先完成后端的业务逻辑，如果需要读写数据库，就先把数据库部分完成，然后写后端部分的接口，接着是 HTML 的页面内容，再把 JS 的交互完成，最后是 CSS。

06 小结

这次课程中最难忘的是把 WEB 代码部署到腾讯云的时候，换用另一台电脑和手机都能打开网页，起初还以为是不是局域网访问，当使用手机流量也能访问时，才意识到我已经完成了一个云端应用的部署，像是打开了新世界的大门。

### 15

阳志平 2023/12/03



### 16

阳志平 2023/12/03




