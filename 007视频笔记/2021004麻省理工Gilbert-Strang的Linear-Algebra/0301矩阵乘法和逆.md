## 0301矩阵乘法和逆

I've been talking, I've been multiplying matrices already, but it's certainly time for me to discuss the rules for matrix multiplication, and the interesting part is there are many ways you can do it, and they all give the same answer. And they're all important. So matrix multiplication, and then come inverses. So we mentioned the inverse of a matrix, but that's a big deal. Lots to do about inverses and how to find them.

Okay, so I'll begin with how to multiply two matrices. First way. Okay, so suppose I have a matrix A multiplying a matrix B and giving me a result, well, I could call it C, A times B. Okay. So let me just review the rule for this entry. That's the entry in row I and column J. So that's the IJ entry. Right there is cij. We always write the row number and then the column number. So maybe I take it c34, just to make it specific. So instead of ij, let me use numbers, c34. So where does that come from, the entry? It comes from row three here, row three, and column four, as you know, column four.

And can I just write down, or can we write down the formula for it? C is, if we look at the whole row and the whole column, the quick way for me to say it is row three of A, I could use a dot for dot product, I won't often use that actually, dot column four of B. But this gives us a chance to just, like, use a little matrix notation. What are the entries? What's this first entry in row three? The first, the first, that number that's sitting right there is A, so it's got two indices and what are they? Three one. So there's an A three one there. Now what's the first guy at the top of column four? So what's sitting up there? B one four, right. So that this dot product starts with a31 times b14. And then what's the next, so this is like I'm accumulating this sum, then comes the next guy, a32, second column, times b24, second row, so it's ba32b24, and so on.

Just practice with indices. Oh, let me even practice with a summation formula. So this is, most of the course I use whole vectors. I very seldom get down to the details of these particular entries, but here we better do it. So, I'm, it's some kind of a sum, right, of things in row three, column K, shall I say, times things in row K, column four. You see that that's what we're seeing here. This is K is one. Here K is two, on along, so up. So the sum goes all the way along the row and down the column, say one to n. So that's what the C3,4 entry looks like, a sum of A3KBK4. It just takes a little practice to do that.

Okay. And, oh, well, maybe I should say, when are we allowed to multiply these matrices? What are the shapes of these things? The shapes are, if we allow them to be not necessarily square matrices. If they're square, they've got to be the same size. If they're rectangular, they're not the same size. If they're rectangular, this might be, well, I always think of A as M by N. M rows, N columns. So that sum goes to N. Now what's the point, how many rows does B have to have? N. N. The number of rows in B, the number of guys that we meet coming down, has to match the number of ones across. So B will have to be N by something. Whatever. P. So the number of columns here has to match the number of rows there, and then what's the result? What's the shape of the result? What's the shape of C, the output? Well, it's got these same M rows, or it's got M rows, and how many columns? P. M by P. Okay. So there are M times P little numbers in there, entries, and each one looks like that.

Okay. So that's the standard rule. That's the way people think of multiplying matrices. I do it too. But that's, I want to talk about other ways to look at that same calculation, looking at whole columns and whole rows. Okay. So can I do A, B, C again? A, B equaling C again. But now tell me about, yeah, let me, I'll put it up here. So here goes A, again, times B, producing C. And again, this is M by N, this is N by p, and this is m by p. Okay. Now I want to look at whole columns. I want to look at the columns of, in fact, here's the second way to multiply matrices. Because I'm going to build on what I know already. How do I multiply a matrix by a column? How do I, I know how to multiply this matrix by that column. Shall I call that column one? That tells me column one of the answer. The matrix times the first column is that first column. Because none of this stuff entered that part of the answer. The matrix times the second column is the second column of the answer.

Do you see what I'm saying? That I could think of multiplying a matrix by a vector, which I already knew how to do, and I can think of just p columns sitting side by side, just like resting next to each other. And I multiply A times each one of those, and I get the p columns of the answer. You see, this is quite nice, to be able to think, okay, matrix multiplication works so that I can just think of having several columns, multiplying by A, and getting the columns of the answer. So like here's column one. I call that, here's a, shall I call that column one? And what's going in there is A times column one. Okay. So that's the picture a column at a time.

So what does that tell me? What does that tell me about these columns? columns. These columns of C comes from A times this, and A times a vector is a combination of the columns of A. And it makes sense, because the columns of A have length m, and the columns of C have length m. And every column of C is some combination of the columns of A, and it's these numbers in here that tell me what combination it is. Do you see that? That in that answer C, I'm seeing combinations of these columns.

Now, suppose I look at it, that's two ways now. The third way is look at it by rows. So now let me change to rows. Okay. So now I can think of a row of A, a row of A, multiplying all these rows here and producing a row of the product. So this row takes a combination of these rows, and that's the answer. So these rows of C are combinations of what? Of, tell me how to finish that. The rows of C, when I have a matrix B, it's got its rows, and I multiply by A, and what does that do? It mixes the rows up, it makes, it creates combinations of the rows of B, thanks. Rows of B. That's what I wanted to see. That this, that this answer, I can see where the pieces are coming from. The rows in the answer are coming as combinations of these rows. The columns in the answer are coming as combinations of those columns.

And now that, so that's three ways. Now you can say, okay, what's the fourth way? Now you can say, okay, what's the fourth way? The fourth way, so that's, now we've got, like, the regular way, the column way, the row way, and what's left? The one that I can, I want to tell you about, well, one way is columns times rows. What happens if I multiply, so this was row times column, it gave a number. Okay, now I want to ask you about column times row. What does, if I multiply a column of A times a row of B, what shape am I ending up with? So if I take a column times a row, that's definitely different from taking a row times a column. So a column of A was, what's the shape of a column of A? M by one. A column of A is a column. It's got M entries and one column. And what's a row of B? It's got one row and P columns. So what's the shape, what do I get if I multiply a column by a row? I get a big matrix. I get a full-sized matrix.

If I multiply a column by a row, I get, should we just do one? Let me take the column two, three, four, times the row one, six. That is that product there. I'm just following the rules of Macri's multiplication. Those rules are just looking like kind of petite, kind of small, because the rows here are so short and the columns there are so short, but they're the same length, one inch. So what's the answer? What's the answer if I do two, three, four times one, six, just for practice? Well, what's the first row of the answer if I do two, three, four times one, six? Just for practice. Well, what's the first row of the answer? Two, twelve. And the second row of the answer is? Three, eighteen. And the third row of the answer is? Four, twenty-four. Actually, what am I, I mean, that's a very special matrix there. Very special matrix. What can you tell me about its columns, the columns of that matrix? There are multiples of this guy, right? There are multiples of that one, which follows our rule? We said that the columns of the answer were combinations, but to take a combination of one guy, it's just a multiple. The rows of the answer, what can you tell me about those three rows? They're all multiples of this row. They're all multiples of one sixth, as we expect. But I'm getting a full-sized matrix.

And now, just to complete this thought, if I have, now, let me write down the fourth way. A, B is a sum of columns of A times rows of B. So then, for example, if my matrix was two, three, four, and then had another column, say, 7, 8, 9, and my matrix here has, say, started with 1, 6, and then had another column like 0, 0, then here's the fourth way. Okay? I've got two columns there, I've got two rows there. So the beautiful rule is, seeing the whole thing by columns and rows is that I can take the first column times the first row and add the second column times the second row. So that's the fourth way that I can take columns times rows, first column times first row, second column times second row, and add. Actually, what will I get? What will the answer be for that matrix multiplication? Well, this one is just going to give a zero, so in fact, I'm back to this. That's the answer for that matrix multiplication.

I'm sort of like happy to put up here these facts about matrix multiplication because it gives me a chance to write down special matrices like this. This is a special matrix. All those rows lie on the same line. All those rows lie on the line through 1, 6. If I draw a picture of all these row vectors, they're all the same direction. If I draw a picture of these two column vectors, they're in the same direction. Later I would use this language. Not too much later, either. I would say the row space, which is like all the combinations of the rows, is just a line for this matrix. The row space is the line through the vector 1, 6. All the rows lie on that line. And the column space is also a line. All the columns lie on the line through the vector 2, 3, 4. So this is like a really minimal matrix, and it's because of these ones.





我们已经讨论过矩阵乘法，现在是时候深入探讨矩阵乘法的规则了。有趣的是，有多种方法可以进行矩阵乘法，它们都能得到相同的结果，而且每种方法都很重要。接下来，我们将讨论矩阵乘法，然后是矩阵的逆（inverse）。我们之前提到过矩阵的逆，但这是一个重要的话题，关于逆矩阵及其求解方法还有很多内容需要探讨。

让我们从如何将两个矩阵相乘开始。第一种方法是这样的：假设我们有一个矩阵 A 乘以一个矩阵 B，得到的结果我们称之为 C，即 A 乘 B。现在，让我们回顾一下结果矩阵中某个元素的计算规则。我们关注的是第 i 行第 j 列的元素，也就是 cij。在矩阵中，我们总是先标注行号，然后是列号。为了具体一点，让我们以 c34 为例。那么，这个元素 c34 是如何得到的呢？它来自于第一个矩阵的第 3 行和第二个矩阵的第 4 列。

让我们来推导一下这个公式。如果我们观察整行和整列，可以快速地表示 C 为 A 的第三行与 B 的第四列的内积（dot product）。这给了我们一个机会来使用一些矩阵符号。让我们仔细看看这些元素。

在第三行的第一个位置，我们有 A 矩阵的一个元素。它有两个索引，分别是 3 和 1，所以这个元素是 A₃₁。同样，在第四列的顶部，我们有 B 矩阵的元素 B₁₄。因此，这个内积的第一项是 A₃₁ 乘以 B₁₄。接下来，我们继续累加：第二项是 A₃₂ 乘以 B₂₄，以此类推。

让我们通过索引来练习一下。我们甚至可以用求和符号来表示这个公式。在这门课程中，我通常使用整个向量进行操作，很少深入到具体元素的细节，但在这里我们最好这样做。

C₃₄ 是某种求和，对吧？它是第三行第 K 列的元素与第 K 行第四列的元素的乘积之和。我们可以看到，当 K=1 时，我们得到第一项；当 K=2 时，得到第二项，依此类推。这个求和沿着行和列进行，从 1 到 n（假设矩阵是 n×n 的）。

因此，C₃₄ 的计算公式可以表示为：

C₃₄ = ∑(k=1 to n）A₃ₖBₖ₄

这个公式表示了矩阵乘法中单个元素的计算方法。掌握这种表示方法需要一些练习，但它是理解矩阵乘法内部机制的关键。

让我们来讨论一下矩阵乘法的条件以及结果矩阵的形状。首先，我们需要明确，在什么情况下我们可以对两个矩阵进行相乘？这涉及到矩阵的维度问题。

对于两个矩阵 A 和 B，它们不一定要是方阵（即行数和列数相等的矩阵）。如果它们是矩形矩阵，它们的大小可以不同，但需要满足特定条件。

让我们假设矩阵 A 的维度是 M × N，即 M 行 N 列。这意味着在进行矩阵乘法时，我们需要对 N 个元素进行求和。那么，矩阵 B 必须有多少行呢？答案是 N 行。换句话说，B 的行数必须与 A 的列数相匹配。

所以，如果我们将 B 的维度表示为 N × P（其中 P 可以是任意正整数），那么 A 的列数必须等于 B 的行数。这是矩阵相乘的基本条件。

现在，让我们考虑结果矩阵 C 的形状。C 将继承 A 的行数和 B 的列数。因此，C 的维度将是 M × P。

总结一下：
- 矩阵 A：M × N
- 矩阵 B：N × P
- 结果矩阵 C：M × P

这意味着结果矩阵 C 中将包含 M × P 个元素，每个元素都是通过 A 的一行与 B 的一列进行点乘（逐元素相乘后求和）得到的。

这就是矩阵乘法中维度的重要性，它决定了哪些矩阵可以相乘，以及结果矩阵的形状。

让我们回顾一下矩阵乘法的标准规则。这是人们通常思考矩阵相乘的方式，我也经常使用这种方法。不过，现在我想讨论一下看待同一计算的其他方法，特别是关注整列和整行的方法。

让我们再次使用矩阵 A、B 和 C 来说明。我们仍然是 A 乘以 B 等于 C。我会把它写在这里。所以这里是 A 乘以 B，得到 C。再次强调，A 是 M×N 矩阵，B 是 N×P 矩阵，C 是 M×P 矩阵。

现在，我想让大家关注整列。实际上，这是矩阵相乘的第二种方法。这种方法是基于我们已经掌握的知识来构建的。我们知道如何将矩阵乘以一个列向量，对吧？让我们思考一下，如何将矩阵 A 乘以 B 的第一列。我们可以把 B 的第一列称为 b1。那么，A 乘以 b1 就得到了结果矩阵 C 的第一列。这是因为 B 的其他列在计算 C 的第一列时并没有参与。同理，矩阵 A 乘以 B 的第二列就得到了 C 的第二列。

让我来解释一下矩阵乘法的另一种思考方式。我们可以将矩阵乘以向量的操作扩展到矩阵乘法中。想象一下，我们有 p 个列向量并排放置，形成一个矩阵。当我们用矩阵 A 乘以这个由 p 列组成的矩阵时，实际上是在对每一列进行单独的矩阵 - 向量乘法，最终得到结果矩阵 C 的 p 列。

这种思考方式很有帮助，因为它让我们可以逐列理解矩阵乘法。例如，结果矩阵 C 的第一列就是 A 乘以输入矩阵的第一列。

那么，这种理解方式告诉我们关于结果矩阵 C 的列的什么信息呢？我们可以发现，C 的每一列都是 A 的列的线性组合。这是有道理的，因为 A 的列和 C 的列长度都是 m。C 的每一列都可以表示为 A 的列的某种组合，而决定这种组合方式的正是输入矩阵中对应列的元素。

换句话说，在结果矩阵 C 中，我们看到的实际上是 A 的列以不同方式组合的结果。这为我们理解矩阵乘法的本质提供了一个新的视角。

现在，让我们从第三个角度来看这个问题：按行来分析。我们将视角转换到矩阵的行。

在这种情况下，我们可以将矩阵 A 的一行与矩阵 B 的所有行相乘，从而得到结果矩阵的一行。换句话说，结果矩阵中的每一行都是 B 矩阵各行的某种组合。

那么，让我们思考一下：当我们有一个矩阵 B，它有其特定的行，然后我们用矩阵 A 去乘它，会发生什么？实际上，这个过程会将 B 的行进行混合，创造出 B 的行的新组合。

这正是我想要阐明的点。我们可以清楚地看到结果矩阵中的元素是如何产生的：

1. 结果矩阵中的行是 B 矩阵行的组合。
2. 结果矩阵中的列是 A 矩阵列的组合。

通过这种方式，我们可以直观地理解矩阵乘法的本质：它是对原矩阵的行和列进行重新组合的过程。

现在我们已经讨论了三种方法。你可能会问，第四种方法是什么呢？让我们回顾一下：我们有常规方法、列方法和行方法。那么还剩下什么？我想向你介绍的是另一种方法，即列乘以行。

之前我们讨论的是行乘以列，得到的是一个数字。现在，让我们考虑列乘以行会发生什么。如果我们将矩阵 A 的一列乘以矩阵 B 的一行，结果的维度会是什么？这与行乘以列是完全不同的操作。

让我们分析一下：
- A 的一列的维度是什么？是 M×1，也就是说，它有 M 个元素排成一列。
- B 的一行的维度是什么？是 1×P，也就是说，它有 P 个元素排成一行。

那么，当我们将一个列向量（M×1）乘以一个行向量（1×P）时，会得到什么？

答案是：我们会得到一个大矩阵，一个完整尺寸的矩阵。具体来说，结果将是一个 M×P 的矩阵。这是因为列乘以行的运算会产生一个新的矩阵，其行数等于列向量的元素个数，列数等于行向量的元素个数。

这种方法为我们提供了看待矩阵乘法的新视角，展示了矩阵运算的多样性和灵活性。

让我们来讨论一下矩阵乘法。假设我们将一个列向量乘以一个行向量，我们应该用一个具体的例子来说明吗？让我们取一个列向量 [2，3，4]^T 和一个行向量 [1，6]。这就是它们的乘积。我只是在遵循矩阵乘法的基本规则。这些规则看起来很简单，因为这里的行和列都很短，但它们的长度是相同的，都是 1。

那么，结果是什么呢？如果我们计算 [2，3，4]^T 乘以 [1，6]，结果会是什么？让我们来练习一下。结果矩阵的第一行是什么？是 [2，12]。第二行呢？是 [3，18]。第三行呢？是 [4，24]。

实际上，这是一个非常特殊的矩阵。关于这个结果矩阵的列，你能告诉我些什么？它们都是原始列向量的倍数，对吧？这符合我们之前说过的规则：结果矩阵的列是原始列向量的线性组合。但在这个特殊情况下，因为我们只有一个列向量，所以结果就是它的倍数。

那么结果矩阵的行呢？你能说出这三行有什么特点吗？它们都是原始行向量的倍数。它们都是 [1，6] 的倍数，这正是我们所期望的。

尽管我们用了很小的向量进行乘法，但我们得到了一个完整大小的矩阵作为结果。

现在，让我们来介绍矩阵乘法的第四种理解方式。这种方法将 A·B 看作 A 的列与 B 的行的和。

举个例子，假设我们有两个矩阵：

A = [2 7]
  [3 8]
  [4 9]

B = [1 0]
  [6 0]

根据这种方法，我们可以将整个乘法过程看作是 A 的列与 B 的行的组合。具体来说，我们可以将 A 的第一列与 B 的第一行相乘，再加上 A 的第二列与 B 的第二行相乘。

这种方法的优点在于它提供了一种新的视角来理解矩阵乘法。我们可以表示为：

A·B =（A 的第一列 × B 的第一行）+（A 的第二列 × B 的第二行)

在这个具体例子中，第二项（A 的第二列 × B 的第二行）会得到零矩阵，因为 B 的第二行全是零。所以，最终的结果就等于 A 的第一列与 B 的第一行的乘积。

这种方法不仅帮助我们理解矩阵乘法的本质，还可以在某些情况下简化计算过程。




我很高兴能在这里讨论一下矩阵乘法的这些特性，因为这给了我一个机会来介绍一种特殊的矩阵。这是一个非常特殊的矩阵，它的所有行向量都位于同一条直线上，具体来说，是通过点（1，6）的直线。如果我们把这些行向量画出来，你会发现它们都指向同一个方向。同样的，如果我们把这两个列向量画出来，它们也指向同一个方向。

接下来，我会用一些专业术语来描述这个现象，而且这个概念很快就会用到。我们称之为「行空间」（row space），它包含了所有行向量的线性组合。对于这个特殊的矩阵来说，它的行空间就是一条通过向量（1，6）的直线，所有的行向量都位于这条线上。类似地，「列空间」（column space）也是一条直线，所有的列向量都位于通过向量（2，3，4）的直线上。

这个矩阵之所以如此特殊，可以说是一个「最小」的矩阵，主要是因为矩阵中出现了许多 1。这些 1 使得矩阵的秩（rank）降到了最低，导致了行空间和列空间都退化成了一维的直线。

在讨论矩阵乘法的方法时，我们已经介绍了三种方式。现在，我想再介绍一种非常有用的方法：分块乘法（Block multiplication）。

在分块乘法中，我们可以将矩阵切割成若干个块，然后通过这些块进行乘法运算。为了简化说明，让我们以一个简单的例子来解释这个概念。

假设我们有两个相同大小的方阵 A 和 B，我们可以将每个矩阵都划分为四个相等的方形块。例如，矩阵 A 可以被划分为 A1、A2、A3、A4 四个块，矩阵 B 可以被划分为 B1、B2、B3、B4 四个块。

分块乘法的规则如下：结果矩阵中的每个块都是由对应的行块和列块相乘然后相加得到的。具体来说，结果矩阵左上角的块将是 A1B1 + A2B3。

这种方法的优势在于，即使我们处理的是大型矩阵，比如 20x20 的矩阵，我们可以将其划分为 10x10 的子块来进行运算。这种方法不仅可以简化计算过程，还可能提高计算效率。

值得注意的是，虽然这种方法看起来很直观，但即使是像高斯这样的数学天才也不太可能立即理解它的工作原理。然而，如果我们仔细检查，就会发现这种方法实际上与其他矩阵乘法方法（如按列乘法、按行乘法、行列相乘法）在本质上是相同的，只是组织计算的方式不同。

总的来说，分块乘法为我们提供了一种新的视角来理解和执行矩阵乘法，它在某些情况下可能会带来计算上的便利。



OK. So that's the third way. Now, even, yeah, can I, will you take, this is, I want to say one more thing about matrix multiplication while we're on this subject. And it's this. You could also multiply, you could also cut the matrix into blocks and do the multiplication by blocks. Yeah, that's actually so useful that I want to mention it. Block multiplication. So I could take my matrix A and I could chop it up like, maybe just for simplicity, let me chop it into two, into four square blocks. Suppose it's square. Let's just take a nice case. And B, suppose it's square also. Same size. So these sizes don't have to be the same. What they have to do is match properly. Here they certainly will match. So here's the rule for block multiplication. That if this has blocks like A, so maybe A1, A2, A3, A4, are the blocks here, and these blocks are B1, B2, B3, and B4, then the answer, I can find block. I can find that block. And if you tell me what's in that block, then I'm going to be quiet about matrix multiplication for the rest of the day.

What goes into that block? You see, these might be this matrix might be these matrices might be like 20 by 20 with blocks that are 10 by 10 to take the easy case where all the blocks are the same shape. And the point is that I could multiply those by blocks. And what goes in here? What's that block in the answer? A1B1. That's a matrix times a matrix, it's the right size, ten by ten, and any more. Plus, what else goes in there? A2B3, right? It's just like block rows times block columns. Nobody, I think, not even Gauss, could see instantly that it works. But somehow, if we check it through, all five ways we're doing the same multiplications. So this, this familiar multiplication is what we're really doing when we do it by columns, by rows, by columns times rows, and by blocks. Okay. I just have to, like, get the rules straight for matrix multiplication.

Okay. All right. I'm ready for the second topic, which is inverse. Okay, ready for inverse. And let me do it for square matrices first. Okay, so I've got a square matrix A. And it may or may not have an inverse, right? Not all matrices have inverses. In fact, that's the most important question. You can ask about the matrix. If you know it's square, is it invertible or not? If it is invertible, then there is some other matrix, shall I call it A inverse? And what's the, what's if A inverse exists? So this is if, there's a big if here. If this is if, there's a big if here, if this matrix exists, and it'll be really central to figure out when does it exist. And then if it does exist, how would you find it? But what's the equation here that I haven't, that I have to finish now? This matrix, if it exists, multiplies A and produces I. Thanks. The identity.

And actually there's a little more to it. Because normally, I mean, right now that's like a left inverse. It's sitting on the left of A. But an inverse for a square matrix could be on the right. So this is true, too. is true too. That if I have a, in fact, this is not, this is probably the, this is something that's not easy to prove, but it works. That a left, that for square matrices, a left inverse is also a right inverse. If I can find a matrix on the left that gets the identity, then also that matrix on the right will produce that identity. For rectangular matrices, we'll see a left inverse that isn't a right inverse. In fact, the shapes wouldn't allow it. But for square matrices, the shapes allow it, and it happens. If A has an inverse.

Okay. So give me some cases, let's see, I hate to be negative here, but let's talk about the case with no inverse. So this is, these matrices are called invertible or non-singular. Those are the good ones. And we want to be able to identify how, if we're given a matrix, has it got an inverse? Can I talk about the singular case? No inverse. All right. Best to start with an example. Tell me an example. Let's get an example up here, let's make it two by two, of a matrix that has not got an inverse. And let's see why. Let me write one up. No inverse. Let's see why. Let me write up one, three, two, six. Why does that matrix have no input? You can answer that various ways. Give me one reason. Well, if you know about determinants, which you're not supposed to, you could take a determinant and you would get zero. Okay. Now, all right. Okay. Now, all right.

Let me ask you other reasons. Let me ask for other reasons that that matrix isn't invertible. Here, I could use what I'm saying here. Suppose, suppose A times some other matrix gave the identity. Why is that not possible? Because, I'm thinking about columns here. If I multiply this matrix A by some other matrix, then the result, what can you tell me about the column? They're all multiples of those columns, right? If I multiply A by another matrix, the product has columns that come from those columns. So can I get the identity matrix? No way. The columns on the identity matrix, like 1, 0, it's not a combination of those columns. Because those two columns both lie on the same line. Every combination is just going to be on that line, and I can't get 1, 0. So you see that sort of column picture of the matrix not being invertible.

In fact, here's another reason. This is even a more important reason. Well, how can I say more important? All those are important. This is another way to see it. A matrix has no inverse. Yeah, now, this is important. A matrix has no, a square matrix won't have an inverse if there's no inverse because I can solve, I can find an x, a vector x with a times, this a times x giving zero. This is the the reason I like this. That matrix won't have an inverse. Can you, well, let me change I to U. So tell me a vector X that solves Ax equals 0. I mean, this is like the key equation. In mathematics, all the key equations have 0 on the right-hand side. So what's the x? Tell me an x here. So now I'm going to slip in the x that you tell me, and I'm going to get 0. What x would do that job? Three and negative one? Is that the one you picked? Yeah. Or another? Well, if you picked zero and zero, I'm not so excited, right? Because that would always work. So it's really the fact that this vector isn't zero that's important. It's a non-zero vector, and 3, negative 1 would do it. That just says 3 of this column minus 1 of that column is the zero column.

Okay. So now I know that A couldn't be invertible. What's the reasoning? If AX is zero, suppose I multiplied by A inverse. Here's the reason. This is why this spells disaster for an inverse. A matrix can't have an inverse if some combination of the columns gives nothing. Because I could take AX equals zero, I could multiply by A inverse, and what would I discover? Suppose I take that equation and I multiply by, if A inverse existed, which of course I'm going to come to the conclusion it can't. Because if it existed, if there was an A inverse to this dopey matrix, I would multiply that equation by that inverse and I would discover X is 0. If I multiply by A inverse on the left, I get X. If I multiply by A inverse on the left, I get X. If I multiply by A inverse on the right, I get zero. So I would discover X was zero. But X is not zero. X, this guy, wasn't zero. There it is. Three minus one.

So, conclusions. Only take us some time to really work with that conclusion. Our conclusion will be that non-invertible matrices, singular matrices, some combinations of their columns gives a zero column. They take some vector x into zero. And there's no way A inverse can recover. Right? That's what this equation says. This equation says I take this vector x and multiplying by A gives zero. But then when I multiply by A inverse, I can never escape from zero. So there couldn't be an A inverse.

Where here, OK, now fix, all right, now let me take a, all right, back to the positive side. Let's take a matrix that does have an inverse. And why not invert it? OK. Can I, so let me take on this third board a matrix. Shall I fix that up a little? Tell me a matrix that has got an inverse. Well, let me say one, three, two. What shall I put there? Well, don't put six, I guess is right. Do you want any favorites here? One? Eight? I don't care. Seven? Seven, okay. Seven is a lucky. All right, Seven? Seven. Okay. Seven is a lucky number. All right. Seven.

Okay. So, now what's our idea? We believe that this matrix is inverted. Those who like determinants have quickly taken its determinant and found it wasn't zero. Those who like columns, and probably that department is not totally popular yet, but those are like columns. We'll look at those two columns and say, hey, they point in different directions, so I can get anything. Now, let me see, what do I need? How am I going to compute A inverse? So A inverse, here's A inverse, and I have to find it. And what do I get when I do this multiplication? The identity. You know, forgive me for taking two by two, but it's good to keep the computations manageable and let the ideas come out.

Okay, now what's the idea I want? I'm looking for this matrix A inverse. How am I going to find it? Right now, I've got four numbers to find. It's, I've got four numbers to find. I'm going to look at the first column. Let me take this first column, A, B. What's up there? What equation, tell me this, what equation does the first column satisfy? The first column satisfies A times that column is one zero. The first column of the answer. And the second column, CD, satisfies A times that second column is zero one. You see that finding the inverse is like solving two systems. One system when the right-hand side is one, zero. I'm just going to split it into two pieces. I don't even need to rewrite it. I can take A times, so let me put it here. A times column J of A inverse is column J of the identity.

I've got N equations. I've got, well, two in this case. And they have the same matrix A, but they have different right-hand sides. The right-hand sides are just the columns of the identity, this guy and this guy. And these are the two solutions. Do you see what I'm doing? I'm looking at that equation by columns. I'm looking at A times this column giving that guy, and A times that column giving that guy. So essentially, so this is like the Gauss, we're back to Gauss. We're back to solving systems of equations, but we're solving, we've got two right-hand sides instead of one. That's where Jordan comes in. So at the very beginning of the lecture, I mentioned Gauss-Jordan. Let me write it up again. Here's the Gauss-Jordan idea. is solves two equations at once.

Okay, let me show you how the mechanics show. How do I solve a single equation? So the two equations are 1327 multiplying AB gives 10. And the other equation is the same one, three, two, seven, multiplying Cd gives zero, one. Okay. That'll tell me the two columns of the inverse, I'll have the inverse. In other words, if I can solve with this matrix A, if I can solve with that right-hand side and that right-hand side, I'm invertible. I've got it. Okay. And Jordan sort of said to Gauss, solve them together, look at the matrix. If we just solve this one, I would look at 1, 3, 2, 7. And how do I deal with the right-hand side? I stick it on as an extra column, right? You remember that's this augmented matrix. That's the matrix when I'm watching the right-hand side at the same time, doing the same thing to the right side that I do to the left. So I just carry it along as an extra column.

Now I'm going to carry along two extra columns. And I'm going to do whatever Gauss wants, right? I'm going to do elimination. I'm going to get this to be simple, and this thing will turn into the inverse. This is what's coming. I'm going to do elimination steps to make this into the identity, and lo and behold, the inverse will show up here. Let's do it. Okay, so what are the elimination steps? So you see that here's my matrix A, and here is the identity, like stuck on, augmented on.. Yeah.. Did I? Oh, no, they weren't supposed to be switched. So, thanks. OK. Thank you very much. And I've got them right. they weren't, of course, big switches. So, thanks. Thank you very much. And I've got them right. OK. Thanks.

OK. So let's do elimination. All right. It's going to be simple, right? So I take two of this row away from this row. So this row stays the same, and two of those come away from this. That leaves me with a zero and a one, and two of these away from this. Is that what that, is that what you're getting? After one elimination step, I'll, let me sort of separate the, the left half from the right half. So two of that first row got subtracted from the second row. Now this is in upper triangular form. Gauss would quit, but Jordan says keep going. Use elimination upwards. Subtract a multiple of equation two from equation one to get rid of the three.

So let's go the whole way. So now I'm going to, this guy is fine, but I'm going to, what do I do now? What's my final step that produces the inverse? I multiply this by the right number to get up to there, to remove that three. So I guess since this is a one, number to get up to there, to remove that three. So I guess since this is a one, there's the pivot sitting there. I multiply it by three and subtract from that. So what do I get? I'll have one zero. Whoa, yeah, that was my whole point. I'll multiply this by three and subtract from that, which will give me seven. And I multiply this by three and subtract from that which will give me seven. And I multiply this by three and subtract from that which gives me a minus three. And what's my hope, belief? Here I started with A and the identity, and I ended up with the identity and who? That better be A inverse.

That's the Gauss-Jordan idea. Start with this long matrix, double length, AI. Eliminate, eliminate, until this part is down to I, then this one must be, for some reason, and we've got to find the reason, must be A inverse. Shall I just check that it works? Let me just check that, can I multiply this matrix, this part times A, I'll carry A over here, and just do that multiplication. You'll see I'll do it the old-fashioned way. Seven minus six is a 1. 21 minus 21 is a 0. Minus 2 plus 2 is a 0. Minus 6 plus 7 is a 1. Check. So that is the inverse. That's the Gauss-Jordan idea.

So one of the homework problems or more than one for Wednesday will ask you to go through those steps. I think you just got to go through Gauss-Jordan a couple of times. Just to see the mechanics. But the important thing is, what happened? Why did we get A inverse? Let me ask you that. We got... So we do row reduction, we do elimination on this long matrix AI until the first half is up. Then the second half is A inverse. Well, how do I see that? Let me put up here how I see that. So here's my Gauss-Jordan phase, and I'm doing stuff to it. So I'm, well, a whole lot of E's. Remember, those are those elimination matrices. Those are the things that we figured out last time. Yes, that's what an elimination step is. It's in matrix form. I'm multiplying by some E. And the result, well, so I'm multiplying by a whole bunch of E. So I get a, can I call the overall matrix E? That's the elimination matrix, the product of all those little pieces what do I mean by little pieces well there was an elimination matrix that subtracted two of that away from that then there was an elimination matrix that subtracted three of that away from that I guess in this case that was all so there were just two E's in this case one that did this step and one that did this step and together they give me an E that does both steps.

And the net result was to get an I here. And you can tell me what that has to be. This is like the picture of what happens. If E multiplied A, whatever that E is, we never figured it out in this way, but whatever that E is, But whatever that E is, E times A is what's E times A? It's I. That E, whatever the heck it was, multiplied A and produced I. So E must be, EA equaling I tells us what E is. Namely, it is the inverse of A. Great. And therefore, when the second half, when E multiplies I, it's E, but that's A inverse. You see the picture looking that way? E times A is the identity. That tells us what E has to be. It has to be the inverse. And therefore, on the right-hand side, where we just smartly tack on the identity, it's turning in, step by step, it's turning into A inverse. it's turning in, step by step, it's turning into a inverse.

There is the statement of Gauss-Jordan elimination. That's how you find the inverse. We can look at it as elimination, as solving n equations at the same time. And tacking on n columns, solving those equations, and up shows the n columns of a n. Okay, thank you. See you on Wednesday.

让我们继续讨论矩阵乘法的话题。除了之前提到的方法，还有一种非常有用的技巧值得一提，那就是分块矩阵乘法（Block Matrix Multiplication）。

在分块矩阵乘法中，我们可以将矩阵切割成若干个子块，然后对这些子块进行乘法运算。为了简化说明，让我们考虑一个特殊情况：假设我们有两个大小相同的方阵 A 和 B，我们将它们各自划分为 2x2 的四个子块。

例如，矩阵 A 可以被划分为 A1、A2、A3、A4 四个子块，同样地，矩阵 B 可以被划分为 B1、B2、B3、B4 四个子块。需要注意的是，在实际应用中，这些子块的大小不必相同，但它们必须满足矩阵乘法的维度匹配要求。

分块矩阵乘法的规则如下：结果矩阵的每个子块可以通过原矩阵相应子块的乘法和加法得到。具体来说，结果矩阵的左上角子块将是 A1B1 + A2B3，右上角子块是 A1B2 + A2B4，左下角子块是 A3B1 + A4B3，右下角子块是 A3B2 + A4B4。

这种方法不仅可以简化某些计算，还可以在并行计算中发挥重要作用，因为不同的子块乘法可以同时进行。

如果你理解了这个概念，那么我们就可以结束今天关于矩阵乘法的讨论了。

那么，这个块矩阵乘法的结果中，每个块里应该放什么呢？让我们来看看。假设我们有一个 20x20 的大矩阵，它由多个 10x10 的小矩阵块组成。为了简化讨论，我们假设所有的块都是相同大小的。

关键在于，我们可以用块的方式来进行矩阵乘法。那么，结果矩阵中的每个块是什么呢？以左上角的块为例，它是 A1B1。这代表一个矩阵乘以另一个矩阵，大小正好是 10x10。但这还不是全部。这个块中还要加上什么？是的，还要加上 A2B3。这就像是把块行与块列相乘。

说实话，我觉得即使是像高斯这样的数学天才，也不可能一眼就看出这种块矩阵乘法的工作原理。但是，如果我们仔细检查，就会发现所有五种方法（按列乘、按行乘、按列乘行、按块乘，以及传统的矩阵乘法）其实都在进行相同的乘法运算。

所以，这种我们熟悉的矩阵乘法，实际上就是我们在用列法、行法、列乘行法和块法进行计算时所做的事情。好的，我想我们需要好好整理一下矩阵乘法的规则，以便更好地理解这些不同的方法。

现在我们来讨论第二个主题：逆矩阵（inverse matrix）。让我们先从方阵（square matrix）开始。

假设我们有一个方阵 A。这个方阵可能有逆矩阵，也可能没有，对吧？实际上，并非所有矩阵都有逆矩阵。事实上，对于一个方阵来说，"它是否可逆」可能是你能问到的最重要的问题。

如果一个方阵是可逆的，那么就存在另一个矩阵，我们可以称之为 A 的逆矩阵（记作 A^(-1)）。但是这里有一个重要的前提：A 的逆矩阵是否存在？这个问题至关重要，我们需要弄清楚在什么情况下逆矩阵会存在。如果逆矩阵确实存在，我们又该如何求得它呢？

现在，让我们来完成描述逆矩阵的等式。如果逆矩阵存在，它与 A 相乘会得到什么结果呢？答案是单位矩阵（identity matrix），通常记作 I。

所以，逆矩阵的定义等式是：A * A^(-1）= I

实际上，这个问题还有更深层的含义。通常情况下，我们刚才讨论的是左逆矩阵，也就是位于矩阵 A 左侧的逆矩阵。但对于方阵（square matrix）来说，逆矩阵可以出现在左侧，也可以出现在右侧。换句话说，以下两个等式都成立：

左逆：B * A = I
右逆：A * B = I

这里 I 表示单位矩阵（identity matrix）。

事实上，虽然不易证明，但对于方阵来说，一个重要的性质是：左逆矩阵同时也是右逆矩阵。这意味着，如果我们能找到一个矩阵 B，使得 B * A 等于单位矩阵，那么 A * B 也会等于单位矩阵。

然而，对于非方阵的矩形矩阵（rectangular matrix），情况就不同了。我们会发现左逆矩阵通常不是右逆矩阵。实际上，由于矩形矩阵的形状限制，它们可能根本就不存在右逆矩阵。

总之，对于方阵，如果存在逆矩阵，那么这个逆矩阵既是左逆又是右逆。这是方阵独有的一个重要性质。

让我们来讨论一些案例。虽然我不想过于消极，但我们还是来谈谈没有逆矩阵的情况吧。这些有逆矩阵的矩阵被称为可逆矩阵（invertible matrix）或非奇异矩阵（non-singular matrix）。它们是我们通常希望遇到的「好」矩阵。我们需要能够判断，给定一个矩阵时，它是否有逆矩阵。那么，我们能讨论一下奇异（singular）的情况，也就是没有逆矩阵的情况吗？

最好从一个例子开始。让我们举一个二阶方阵的例子，这个矩阵没有逆矩阵。我们来看看为什么。让我写下这个矩阵：

[1 3]
[2 6]

为什么这个矩阵没有逆矩阵呢？你可以用不同的方法来回答这个问题。给我一个理由。嗯，如果你知道行列式（determinant)（虽然你可能还不应该知道这个概念），你可以计算它的行列式，你会得到零。

好的，我们继续讨论这个话题。

让我们来探讨矩阵不可逆的其他原因。我可以用我刚才提到的概念来解释。假设矩阵 A 乘以某个其他矩阵能得到单位矩阵，为什么这是不可能的呢？我们可以从列的角度来思考这个问题。

当我们将矩阵 A 乘以另一个矩阵时，结果矩阵的列会是什么样的？实际上，它们都是 A 的列的线性组合。换句话说，如果我们将 A 乘以另一个矩阵，得到的乘积矩阵的列都来自于 A 的列。

那么，我们能得到单位矩阵吗？显然是不可能的。为什么呢？因为单位矩阵的列，比如（1，0）这样的列向量，不可能是 A 的那两列的线性组合。原因在于 A 的两列都位于同一直线上，它们的任何线性组合也只能落在这条直线上，而我们无法在这条直线上得到像（1，0）这样的向量。

通过这种列的观点，我们可以直观地理解为什么这个矩阵是不可逆的。它的列线性相关，无法生成整个向量空间，因此也就无法通过矩阵乘法得到单位矩阵。

实际上，这里还有另一个原因，甚至可以说是更重要的原因。当然，我们不应该轻易说哪个更重要，因为每个原因都很重要。这只是从另一个角度来看待这个问题。

这个原因是：矩阵没有逆。是的，这一点非常重要。为什么一个方阵（square matrix）可能没有逆矩阵呢？这是因为我们可以找到一个非零向量 x，使得矩阵 A 与 x 的乘积等于零向量。这就是我喜欢这个解释的原因。如果存在这样的向量 x，那么这个矩阵就没有逆。

让我们把方程写成 Ax = 0 的形式。这个方程非常关键。在数学中，所有关键方程的右侧通常都是 0。那么，这里的 x 是什么呢？我们需要找到一个向量 x，当把它代入方程时，结果为 0。

什么样的 x 能完成这个任务呢？是 3 和 -1 吗？这是你选的向量吗？没错。当然，你可能会选择其他向量。如果你选择了 0 和 0，那就没什么特别的了，因为零向量总是满足这个方程。所以，真正重要的是这个向量不是零向量。它是一个非零向量，而（3，-1）就满足这个条件。

这实际上意味着，第一列的 3 倍减去第二列的 1 倍等于零列。这就是为什么矩阵没有逆的原因。

现在我们知道矩阵 A 不可逆，那么原因是什么呢？让我们来分析一下。如果 AX 等于零，假设我们将等式两边都乘以 A 的逆矩阵，会发生什么？这个推理过程揭示了为什么某些情况下逆矩阵不存在。

当矩阵的列向量的某种线性组合得到零向量时，该矩阵就不存在逆矩阵。我们可以通过以下方式来理解这一点：假设 AX = 0，如果 A 的逆矩阵存在（当然，我们最终会得出结论它不存在），我们就可以在等式两边同时左乘 A 的逆矩阵。这样一来，我们会得到 X = 0。换句话说，如果我们在左边乘以 A 的逆矩阵，我们得到 X; 如果我们在右边乘以 A 的逆矩阵，我们得到零。这就意味着 X 必须等于零。但是实际上，我们的 X 并不是零向量。就在那里，三减一。

因此，我们可以得出以下结论（需要一些时间来深入理解)：不可逆矩阵，也称为奇异矩阵（singular matrices），其列向量的某些线性组合会得到零向量。这意味着它们会将某个非零向量 X 映射到零向量。而且，一旦发生这种情况，A 的逆矩阵就无法从零向量中恢复出原始的 X。这就是上述等式所表达的含义：它说明我们取一个向量 X，乘以 A 得到零向量，但是当我们试图乘以 A 的逆矩阵时，我们永远无法摆脱零向量。这就是为什么 A 的逆矩阵不可能存在的原因。

好的，现在让我们回到一个积极的例子。我们来看一个有逆矩阵的矩阵，并尝试求解它的逆。让我在第三块黑板上写一个矩阵。我们应该选择什么样的矩阵呢？告诉我一个肯定有逆矩阵的例子。那么，让我们写下这样一个矩阵：

[1 3]
[2 7]

为什么选 7？嗯，7 是个幸运数字，不是吗？

现在，我们的想法是什么？我们相信这个矩阵是可逆的。那些喜欢用行列式的人可能已经迅速计算出它的行列式不为零，从而确认了这一点。而那些更喜欢从列向量角度思考的人（虽然这种方法可能还不是很流行），会观察这两个列向量并发现它们指向不同的方向。这意味着这两个向量线性无关，因此可以生成整个二维平面上的任何向量。

那么，我们需要做什么来计算 A 的逆矩阵（A^(-1)）呢？我们知道，当我们将 A 与 A^(-1）相乘时，应该得到单位矩阵（I）。用数学符号表示就是：

A * A^(-1）= I

请原谅我使用 2x2 的矩阵，但这样可以使计算保持简单，同时又能充分展示求逆矩阵的基本思想。

那么，我们现在的目标是什么？我们需要找到矩阵 A 的逆矩阵。如何找到它呢？实际上，我们需要确定四个未知数。让我们先关注第一列。假设第一列的元素是 a 和 b。这一列需要满足什么条件呢？它应该满足方程：A 乘以这一列等于 [1，0]^T（即单位矩阵的第一列）。同理，第二列（假设元素为 c 和 d）应该满足方程：A 乘以这一列等于 [0，1]^T（即单位矩阵的第二列）。

从这个过程中，我们可以看出，求逆矩阵实际上相当于解两个线性方程组。一个方程组的右侧是 [1，0]^T，另一个的右侧是 [0，1]^T。我们可以将这个问题分解为两个独立的部分来解决。

概括来说，我们可以得出一个通用公式：A 乘以 A^(-1)（A 的逆矩阵）的第 j 列等于单位矩阵的第 j 列。这个公式为我们提供了一种系统的方法来计算逆矩阵的每一列。

我们现在面对的是一个包含 N 个线性方程的系统。在这个特定的例子中，我们有两个方程。这些方程有一个共同点：它们使用相同的系数矩阵 A，但右侧（常数项）不同。实际上，这些右侧向量恰好是单位矩阵（identity matrix）的列向量。

让我们仔细看看这个过程：我们实际上是在按列分析这个方程组。具体来说，我们考虑矩阵 A 乘以第一个列向量得到第一个结果，然后 A 乘以第二个列向量得到第二个结果。这种方法本质上是回到了高斯（Gauss）消元法的思想，我们再次面对求解方程组的问题。但这次有所不同：我们同时处理两个右侧向量，而不是只有一个。

这就是乔丹（Jordan）方法发挥作用的地方。在讲座开始时，我提到了高斯 - 乔丹（Gauss-Jordan）方法。让我再次强调这个概念：高斯 - 乔丹方法的核心思想是同时求解两个方程。

这种方法的优势在于效率：通过一次计算过程，我们可以得到多个方程的解，而不需要为每个方程单独进行一次完整的求解过程。这在处理大型方程组或需要频繁求解不同右侧向量的情况下特别有用。

让我来向你展示这个解题方法的机制。我们如何解一个单独的方程呢？这里有两个方程：第一个是矩阵 [1 3 2 7] 乘以向量 [A B]^T 等于 [1 0]^T，第二个方程是同样的矩阵 [1 3 2 7] 乘以向量 [C D]^T 等于 [0 1]^T。这两个方程将给出逆矩阵的两列，从而我们就能得到完整的逆矩阵。换句话说，如果我能用矩阵 A 解出这两个不同的右侧向量，那么 A 就是可逆的，我们就找到了它的逆矩阵。

在这个问题中，Jordan 方法实际上建议我们采用类似 Gauss 消元法的思路，同时解这两个方程，并关注整个矩阵的变化。如果我们只解其中一个方程，我们会关注矩阵 [1 3 2 7]。那么，我们如何处理右侧向量呢？我们将它作为额外的一列添加到原矩阵中，对吧？你可能还记得，这就是所谓的增广矩阵（augmented matrix）。这个增广矩阵允许我们在进行矩阵运算时同时观察等式右侧的变化，对右侧向量执行与左侧矩阵相同的操作。因此，我们只需将右侧向量作为额外的一列带入计算过程即可。

现在，我们要在原矩阵旁边增加两个额外的列。我们将按照高斯消元法的步骤进行操作，对吗？我们要进行消元运算。我们的目标是将这个矩阵简化，而这个过程中，逆矩阵将会显现出来。这就是接下来要发生的事情。我们将进行消元步骤，使这个矩阵变成单位矩阵，同时，逆矩阵将在右侧出现。让我们开始吧。

那么，消元步骤是什么呢？你可以看到，这里是我们的矩阵 A，旁边是附加的单位矩阵，形成了增广矩阵。哦，我是不是把它们放错位置了？不，它们的位置是正确的。谢谢提醒。很好，我已经正确排列它们了。

好的，让我们开始进行消元。这个过程其实很简单，对吧？我们从第二行减去第一行的两倍。这样，第一行保持不变，而第二行的第一个元素变成零，第二个元素变成一。这是你们得到的结果吗？在完成一步消元后，让我把左半部分和右半部分分开来看。所以，我们从第二行减去第一行的两倍。现在，左侧矩阵变成了上三角形式。

到这一步，高斯（Gauss）可能就会停止，但乔丹（Jordan）会说继续。我们要使用向上消元。从第一个方程中减去第二个方程的倍数，以消除第一行第二列的那个 3。

让我们继续完成最后的步骤。现在，我们已经处理好了矩阵的左半部分，接下来要做什么呢？最后一步是如何得到逆矩阵的？我们需要将这一行乘以一个适当的数，以消除上面那个 3。由于这里的主元（pivot）是 1，我们只需要将这一行乘以 3，然后从上面那行减去。

这样操作后，我们得到什么呢？第一列变成了 [1，0]。这正是我们想要的结果！继续进行相同的操作，我们将这一行乘以 3 并从上面那行减去，得到 7。对下一个元素也是如此，得到 7。最后一个元素，我们同样乘以 3 并相减，得到 -3。

现在，让我们回顾一下我们做了什么。我们从矩阵 A 和单位矩阵 I 开始，经过一系列操作，最终得到了单位矩阵 I 和另一个矩阵。这个新得到的矩阵，按照我们的推理，应该就是 A 的逆矩阵。

这就是 Gauss-Jordan 方法的核心思想。我们从一个长矩阵开始，它由 [A|I] 组成，其中 A 是我们要求逆的矩阵，I 是单位矩阵。通过一系列的消元操作，我们将左半部分变成单位矩阵 I。神奇的是，此时右半部分就变成了 A 的逆矩阵。

为了验证我们的结果，让我们进行一个简单的检查。我们将得到的这个疑似逆矩阵与原始的 A 相乘，看看是否得到单位矩阵。使用传统的矩阵乘法方法，我们得到：

7 - 6 = 1
21 - 21 = 0
-2 + 2 = 0
-6 + 7 = 1

检查无误！这确实是 A 的逆矩阵。

这就是 Gauss-Jordan 方法的精髓。它不仅能求解线性方程组，还能直接得到矩阵的逆，是一种非常强大和优雅的数学工具。

周三的一个或多个家庭作业题目会要求你们完成这些步骤。我认为你们只需要练习几次高斯 - 乔丹消元法（Gauss-Jordan elimination）就可以了。这主要是为了让你们熟悉这个过程的机制。但更重要的是，我们需要理解发生了什么，为什么我们能得到 A 的逆矩阵？让我来问问你们这个问题。

我们得到的结果是这样的：我们对这个扩展矩阵 [A|I] 进行行约简和消元，直到左半部分变成单位矩阵。然后，右半部分就变成了 A 的逆矩阵。那么，我们如何理解这个过程呢？让我来解释一下我的看法。

在高斯 - 乔丹消元法的过程中，我们实际上是对矩阵进行了一系列的操作。这些操作可以用许多消元矩阵（elimination matrices）来表示，我们用 E 来表示这些矩阵。还记得我们上次讨论过的消元矩阵吗？没错，每一个消元步骤都可以用一个矩阵形式来表示。

我们在进行消元时，实际上是在不断地左乘这些消元矩阵 E。最终的结果是，我们得到了所有这些 E 的乘积。我们可以把这个总的乘积矩阵也叫做 E，它就是整个消元过程的总消元矩阵。

让我们具体看一下这个过程：比如说，第一个消元矩阵 E₁ 可能是从某一行减去另一行的两倍，第二个消元矩阵 E₂ 可能是从某一行减去另一行的三倍。在这个例子中，我们只有两个消元步骤，所以只有两个 E。第一个 E 完成了第一步操作，第二个 E 完成了第二步操作。这两个 E 相乘就给我们一个总的 E，它完成了这两个步骤的全部操作。

通过这个过程，我们就可以理解为什么最终得到的右半部分矩阵是 A 的逆矩阵了。

最终的结果是我们得到了一个 I。让我来解释一下这意味着什么。这就像是整个过程的一幅图画。如果矩阵 E 乘以矩阵 A，虽然我们没有用这种方法确定 E 的具体内容，但无论 E 是什么，E 乘 A 的结果是什么呢？是单位矩阵（identity matrix）I。这个 E，不管它具体是什么，乘以 A 后得到了 I。所以 E 必定是什么呢？EA 等于 I 这个等式告诉我们 E 的身份。它实际上就是 A 的逆矩阵（inverse matrix）。

因此，在等式的后半部分，当 E 乘以 I 时，它就是 E 本身，而这个 E 实际上就是 A 的逆矩阵。你能看出这个过程吗？E 乘 A 等于单位矩阵，这告诉我们 E 必须是什么。它必须是 A 的逆矩阵。因此，在等式的右侧，我们巧妙地附加上单位矩阵，它就逐步转变成了 A 的逆矩阵。

这就是高斯 - 乔丹消元法（Gauss-Jordan elimination）的原理。这就是我们如何找到逆矩阵的方法。我们可以将其视为一种消元过程，同时求解 n 个方程。通过附加 n 列，求解这些方程，最后得到的就是 A 逆矩阵的 n 列。好的，谢谢大家。我们下周三再见。

